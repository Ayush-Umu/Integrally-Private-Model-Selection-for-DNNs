{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SW8gjwq4xxQm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from scipy.stats import ks_2samp\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WqRpV3VQyCr2"
   },
   "outputs": [],
   "source": [
    "def roundR(x, r):\n",
    "    return [np.round(i,r) for i in np.array(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wWQXGIoSytv3"
   },
   "outputs": [],
   "source": [
    "def update_weights(model_weights):\n",
    "  for i in range(len(model_weights)):\n",
    "    for j in range(len(model_weights[i])):\n",
    "      if isinstance(model_weights[i][j], np.ndarray):\n",
    "        for k in range(len(model_weights[i][j])):\n",
    "          #print(\"chalra h\", i, j, k)\n",
    "            if (np.round(model_weights[i][j][k],2)==0):\n",
    "                model_weights[i][j][k]=float(f'{model_weights[i][j][k]:.1g}')\n",
    "            else:\n",
    "                model_weights[i][j][k]=np.round(model_weights[i][j][k],2)\n",
    "      else:\n",
    "        if (np.round(model_weights[i][j],2)==0):\n",
    "          model_weights[i][j]=float(f'{model_weights[i][j]:.1g}')\n",
    "        else:\n",
    "          model_weights[i][j]=np.round(model_weights[i][j],2)\n",
    "  return model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VXccB1Bmyu8O"
   },
   "outputs": [],
   "source": [
    "def node_comparison1(node1, node2, epsilon=0.05):\n",
    "  for x, y in zip(node1,node2):\n",
    "    #print(x,y)\n",
    "    if isinstance(x, list):\n",
    "        if((np.linalg.norm(np.array(x)-np.array(y))/len(x))<=epsilon):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        if(abs(x-y)<=epsilon):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "m0VwL5M3ywjJ"
   },
   "outputs": [],
   "source": [
    "def layer_comparison(layer1, layer2):\n",
    "  for node1 in layer1:\n",
    "    present=False\n",
    "    for node2 in layer2:\n",
    "      if (node_comparison1(node1, node2)):\n",
    "        present=True\n",
    "    if present==False:\n",
    "      return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "my6oyr35yyGW"
   },
   "outputs": [],
   "source": [
    "def check_models(Model_weights1, Model_weights2):\n",
    "  for i in range(0,len(Model_weights1), 2):\n",
    "    layer1=[]\n",
    "    layer2=[]\n",
    "    for j in range(len(Model_weights1[i+1].T)):\n",
    "      Node1=[]\n",
    "      Node2=[]\n",
    "      Node1.append(list(Model_weights1[i].T[j]))\n",
    "      Node1.append(Model_weights1[i+1][j])\n",
    "      if (i+2<len(Model_weights1)):\n",
    "        Node1.append(list(Model_weights1[i+2][j]))\n",
    "      Node2.append(list(Model_weights2[i].T[j]))\n",
    "      Node2.append(Model_weights2[i+1][j])\n",
    "      if (i+2<len(Model_weights2)):\n",
    "        Node2.append(list(Model_weights2[i+2][j]))\n",
    "      layer1.append(Node1)\n",
    "      layer2.append(Node2)\n",
    "    if (layer_comparison(layer1, layer2)):\n",
    "      continue\n",
    "    else:\n",
    "      return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aMLDlzX8yznn"
   },
   "outputs": [],
   "source": [
    "def get_initial_model(inp, out):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(5, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(5, activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_model_2(inp, out):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(1024, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_model_3(inp, out):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(10, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(20, activation='relu'),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_model_4(inp, out):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(5, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(20, activation='relu'),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(5, activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DP_initial_model(inp, out):\n",
    "    model=tf.keras.Sequential([\n",
    "        Dense(5, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(5, activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to average out all the models in the epsilon range\n",
    "#the problem is different here than compared with model comparison. Shape not important.\n",
    "def get_avg_weights(models_weights, inp_shape, out_shape):\n",
    "    avg_sum=get_initial_model(inp_shape, out_shape).get_weights()\n",
    "    #print(avg_sum)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                for k in range(len(avg_sum[i][j])):\n",
    "                    avg_sum[i][j][k]=0\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=0\n",
    "    #print(avg_sum)\n",
    "    print(models_weights[0])\n",
    "    for i in range(len(models_weights)):\n",
    "        for j in range(0, len(avg_sum),2):\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #if(isinstance(avg_sum[j][0], np.ndarray)):\n",
    "            for k in range(len(avg_sum[j])):\n",
    "                avg_sum[j][k]=[avg_sum[j][k][l]+models_weights[i][j][k][l] for l in range(len(avg_sum[j][k]))]\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #else: gayab kr diya\n",
    "            print('andr aara h')\n",
    "            for k in range(len(avg_sum[j+1])):\n",
    "                avg_sum[j+1][k]=avg_sum[j+1][k]+models_weights[i][j+1][k]\n",
    "    print(\"yhn tk\")\n",
    "    mean_size=len(models_weights)\n",
    "    print(mean_size)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                #print(\"yhn tk\")\n",
    "                avg_sum[i][j]=[avg_sum[i][j][k]/mean_size for k in range(len(avg_sum[i][j]))]\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=avg_sum[i+1][j]/mean_size\n",
    "    print(\"Done\")\n",
    "    return avg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to average out all the models in the epsilon range\n",
    "#the problem is different here than compared with model comparison. Shape not important.\n",
    "def get_avg_weights_2(models_weights, inp_shape, out_shape):\n",
    "    avg_sum=get_initial_model_2(inp_shape, out_shape).get_weights()\n",
    "    #print(avg_sum)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                for k in range(len(avg_sum[i][j])):\n",
    "                    avg_sum[i][j][k]=0\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=0\n",
    "    #print(avg_sum)\n",
    "    print(models_weights[0])\n",
    "    for i in range(len(models_weights)):\n",
    "        for j in range(0, len(avg_sum),2):\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #if(isinstance(avg_sum[j][0], np.ndarray)):\n",
    "            for k in range(len(avg_sum[j])):\n",
    "                avg_sum[j][k]=[avg_sum[j][k][l]+models_weights[i][j][k][l] for l in range(len(avg_sum[j][k]))]\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #else: gayab kr diya\n",
    "            print('andr aara h')\n",
    "            for k in range(len(avg_sum[j+1])):\n",
    "                avg_sum[j+1][k]=avg_sum[j+1][k]+models_weights[i][j+1][k]\n",
    "    print(\"yhn tk\")\n",
    "    mean_size=len(models_weights)\n",
    "    print(mean_size)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                #print(\"yhn tk\")\n",
    "                avg_sum[i][j]=[avg_sum[i][j][k]/mean_size for k in range(len(avg_sum[i][j]))]\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=avg_sum[i+1][j]/mean_size\n",
    "    print(\"Done\")\n",
    "    return avg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to average out all the models in the epsilon range\n",
    "#the problem is different here than compared with model comparison. Shape not important.\n",
    "def get_avg_weights_3(models_weights, inp_shape, out_shape):\n",
    "    avg_sum=get_initial_model_3(inp_shape, out_shape).get_weights()\n",
    "    #print(avg_sum)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                for k in range(len(avg_sum[i][j])):\n",
    "                    avg_sum[i][j][k]=0\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=0\n",
    "    #print(avg_sum)\n",
    "    print(models_weights[0])\n",
    "    for i in range(len(models_weights)):\n",
    "        for j in range(0, len(avg_sum),2):\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #if(isinstance(avg_sum[j][0], np.ndarray)):\n",
    "            for k in range(len(avg_sum[j])):\n",
    "                avg_sum[j][k]=[avg_sum[j][k][l]+models_weights[i][j][k][l] for l in range(len(avg_sum[j][k]))]\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #else: gayab kr diya\n",
    "            print('andr aara h')\n",
    "            for k in range(len(avg_sum[j+1])):\n",
    "                avg_sum[j+1][k]=avg_sum[j+1][k]+models_weights[i][j+1][k]\n",
    "    print(\"yhn tk\")\n",
    "    mean_size=len(models_weights)\n",
    "    print(mean_size)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                #print(\"yhn tk\")\n",
    "                avg_sum[i][j]=[avg_sum[i][j][k]/mean_size for k in range(len(avg_sum[i][j]))]\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=avg_sum[i+1][j]/mean_size\n",
    "    print(\"Done\")\n",
    "    return avg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to average out all the models in the epsilon range\n",
    "#the problem is different here than compared with model comparison. Shape not important.\n",
    "def get_avg_weights_4(models_weights, inp_shape, out_shape):\n",
    "    avg_sum=get_initial_model_4(inp_shape, out_shape).get_weights()\n",
    "    #print(avg_sum)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                for k in range(len(avg_sum[i][j])):\n",
    "                    avg_sum[i][j][k]=0\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=0\n",
    "    #print(avg_sum)\n",
    "    print(models_weights[0])\n",
    "    for i in range(len(models_weights)):\n",
    "        for j in range(0, len(avg_sum),2):\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #if(isinstance(avg_sum[j][0], np.ndarray)):\n",
    "            for k in range(len(avg_sum[j])):\n",
    "                avg_sum[j][k]=[avg_sum[j][k][l]+models_weights[i][j][k][l] for l in range(len(avg_sum[j][k]))]\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #else: gayab kr diya\n",
    "            print('andr aara h')\n",
    "            for k in range(len(avg_sum[j+1])):\n",
    "                avg_sum[j+1][k]=avg_sum[j+1][k]+models_weights[i][j+1][k]\n",
    "    print(\"yhn tk\")\n",
    "    mean_size=len(models_weights)\n",
    "    print(mean_size)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                #print(\"yhn tk\")\n",
    "                avg_sum[i][j]=[avg_sum[i][j][k]/mean_size for k in range(len(avg_sum[i][j]))]\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=avg_sum[i+1][j]/mean_size\n",
    "    print(\"Done\")\n",
    "    return avg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BdTIUrUoy1EG"
   },
   "outputs": [],
   "source": [
    "# custom functions for f1, precision and recall\n",
    "\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mwVjaNdSy2-W",
    "outputId": "208589cb-1140-4a4a-dd1f-c1f69de5c5d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Type', 'Air temperature [K]', 'Process temperature [K]',\n",
      "       'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]',\n",
      "       'Machine failure'],\n",
      "      dtype='object')\n",
      "Index(['Air temperature [K]', 'Process temperature [K]',\n",
      "       'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]', 'Type_H',\n",
      "       'Type_L', 'Type_M', 'Machine failure'],\n",
      "      dtype='object')\n",
      "9661\n",
      "339\n"
     ]
    }
   ],
   "source": [
    "#for SKIN_NonSkin dataset\n",
    "df = pd.read_csv(\"ai4i2020.csv\",sep=';')\n",
    "#dataset.round(3)\n",
    "#print(dataset['label'])\n",
    "df_str = df.select_dtypes(include='object')\n",
    "df_int = df.select_dtypes(exclude='object')\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "#handle categorical data\n",
    "df_str = pd.get_dummies(df_str)\n",
    "\n",
    "target_variable=\"Machine failure\"\n",
    "target = df_int[target_variable]\n",
    "x = df_int.drop(columns=target_variable)\n",
    "column_names = x.columns.values\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x_stndrd = scaler.fit_transform(x)\n",
    "type(x_stndrd)\n",
    "\n",
    "x_stndrd = pd.DataFrame(x_stndrd)\n",
    "x_stndrd.columns = column_names\n",
    "\n",
    "dataset = pd.concat([x_stndrd,df_str, target],axis=1)\n",
    "print(dataset.columns)\n",
    "\n",
    "Positive=dataset[dataset[target_variable]==0]\n",
    "Negative=dataset[dataset[target_variable]==1]\n",
    "print(len(Positive))\n",
    "print(len(Negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fqu0KV4VDHjp",
    "outputId": "4250b289-e8c3-4a38-f4be-dd26012f7985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483 17\n",
      "483 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "2022-11-25 17:13:57.779382: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "13/13 [==============================] - 1s 11ms/step - loss: 0.7219 - f1_m: 0.2115 - val_loss: 0.6904 - val_f1_m: 0.7422\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6792 - f1_m: 0.8822 - val_loss: 0.6682 - val_f1_m: 0.9687\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6616 - f1_m: 0.9663 - val_loss: 0.6542 - val_f1_m: 0.9687\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6470 - f1_m: 0.9688 - val_loss: 0.6391 - val_f1_m: 0.9687\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6310 - f1_m: 0.9688 - val_loss: 0.6224 - val_f1_m: 0.9687\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6132 - f1_m: 0.9688 - val_loss: 0.6039 - val_f1_m: 0.9687\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5935 - f1_m: 0.9663 - val_loss: 0.5829 - val_f1_m: 0.9687\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5708 - f1_m: 0.9688 - val_loss: 0.5586 - val_f1_m: 0.9687\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5444 - f1_m: 0.9688 - val_loss: 0.5298 - val_f1_m: 0.9687\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5126 - f1_m: 0.9688 - val_loss: 0.4958 - val_f1_m: 0.9687\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4749 - f1_m: 0.9639 - val_loss: 0.4557 - val_f1_m: 0.9687\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4307 - f1_m: 0.9663 - val_loss: 0.4101 - val_f1_m: 0.9687\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3815 - f1_m: 0.9688 - val_loss: 0.3599 - val_f1_m: 0.9687\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3292 - f1_m: 0.9663 - val_loss: 0.3106 - val_f1_m: 0.9687\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2793 - f1_m: 0.9687 - val_loss: 0.2659 - val_f1_m: 0.9687\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2360 - f1_m: 0.9663 - val_loss: 0.2284 - val_f1_m: 0.9687\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2021 - f1_m: 0.9688 - val_loss: 0.2005 - val_f1_m: 0.9687\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1763 - f1_m: 0.9663 - val_loss: 0.1818 - val_f1_m: 0.9687\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1593 - f1_m: 0.9687 - val_loss: 0.1691 - val_f1_m: 0.9687\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1477 - f1_m: 0.9687 - val_loss: 0.1598 - val_f1_m: 0.9687\n",
      "0.9687499403953552\n",
      "16/16 [==============================] - 0s 573us/step\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 10ms/step - loss: 0.7207 - f1_m: 0.2067 - val_loss: 0.6920 - val_f1_m: 0.7109\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6797 - f1_m: 0.8894 - val_loss: 0.6686 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6621 - f1_m: 0.9663 - val_loss: 0.6536 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6473 - f1_m: 0.9663 - val_loss: 0.6382 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6316 - f1_m: 0.9639 - val_loss: 0.6209 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6139 - f1_m: 0.9663 - val_loss: 0.6022 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5944 - f1_m: 0.9663 - val_loss: 0.5809 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5725 - f1_m: 0.9663 - val_loss: 0.5560 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5468 - f1_m: 0.9663 - val_loss: 0.5268 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5161 - f1_m: 0.9663 - val_loss: 0.4920 - val_f1_m: 0.9766\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4781 - f1_m: 0.9663 - val_loss: 0.4514 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4345 - f1_m: 0.9615 - val_loss: 0.4036 - val_f1_m: 0.9766\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3843 - f1_m: 0.9639 - val_loss: 0.3531 - val_f1_m: 0.9766\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3327 - f1_m: 0.9639 - val_loss: 0.3008 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2819 - f1_m: 0.9639 - val_loss: 0.2535 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2386 - f1_m: 0.9663 - val_loss: 0.2141 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2038 - f1_m: 0.9663 - val_loss: 0.1845 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1788 - f1_m: 0.9663 - val_loss: 0.1648 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1613 - f1_m: 0.9639 - val_loss: 0.1511 - val_f1_m: 0.9766\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1499 - f1_m: 0.9663 - val_loss: 0.1418 - val_f1_m: 0.9766\n",
      "0.9663460850715637\n",
      "16/16 [==============================] - 0s 573us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 0.7200 - f1_m: 0.1971 - val_loss: 0.6921 - val_f1_m: 0.5859\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6785 - f1_m: 0.9014 - val_loss: 0.6705 - val_f1_m: 0.9375\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6607 - f1_m: 0.9784 - val_loss: 0.6576 - val_f1_m: 0.9375\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6456 - f1_m: 0.9760 - val_loss: 0.6437 - val_f1_m: 0.9375\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6289 - f1_m: 0.9784 - val_loss: 0.6285 - val_f1_m: 0.9375\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6105 - f1_m: 0.9784 - val_loss: 0.6115 - val_f1_m: 0.9375\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5899 - f1_m: 0.9760 - val_loss: 0.5923 - val_f1_m: 0.9375\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5660 - f1_m: 0.9760 - val_loss: 0.5704 - val_f1_m: 0.9375\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5380 - f1_m: 0.9784 - val_loss: 0.5444 - val_f1_m: 0.9375\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5044 - f1_m: 0.9784 - val_loss: 0.5132 - val_f1_m: 0.9375\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4640 - f1_m: 0.9784 - val_loss: 0.4770 - val_f1_m: 0.9375\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4168 - f1_m: 0.9784 - val_loss: 0.4370 - val_f1_m: 0.9375\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3656 - f1_m: 0.9784 - val_loss: 0.3956 - val_f1_m: 0.9375\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3120 - f1_m: 0.9784 - val_loss: 0.3561 - val_f1_m: 0.9375\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2625 - f1_m: 0.9784 - val_loss: 0.3225 - val_f1_m: 0.9375\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2195 - f1_m: 0.9784 - val_loss: 0.2965 - val_f1_m: 0.9375\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1841 - f1_m: 0.9784 - val_loss: 0.2790 - val_f1_m: 0.9375\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1576 - f1_m: 0.9760 - val_loss: 0.2686 - val_f1_m: 0.9375\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1383 - f1_m: 0.9784 - val_loss: 0.2638 - val_f1_m: 0.9375\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1248 - f1_m: 0.9760 - val_loss: 0.2630 - val_f1_m: 0.9375\n",
      "0.9759615659713745\n",
      "16/16 [==============================] - 0s 578us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 10ms/step - loss: 0.7208 - f1_m: 0.1971 - val_loss: 0.6917 - val_f1_m: 0.6953\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6795 - f1_m: 0.8918 - val_loss: 0.6687 - val_f1_m: 0.9688\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6616 - f1_m: 0.9663 - val_loss: 0.6545 - val_f1_m: 0.9688\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6471 - f1_m: 0.9688 - val_loss: 0.6394 - val_f1_m: 0.9688\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6310 - f1_m: 0.9688 - val_loss: 0.6228 - val_f1_m: 0.9688\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.6134 - f1_m: 0.9688 - val_loss: 0.6044 - val_f1_m: 0.9688\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5933 - f1_m: 0.9688 - val_loss: 0.5833 - val_f1_m: 0.9688\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5703 - f1_m: 0.9688 - val_loss: 0.5584 - val_f1_m: 0.9688\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5428 - f1_m: 0.9663 - val_loss: 0.5287 - val_f1_m: 0.9688\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.5095 - f1_m: 0.9688 - val_loss: 0.4931 - val_f1_m: 0.9688\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4700 - f1_m: 0.9688 - val_loss: 0.4512 - val_f1_m: 0.9688\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4248 - f1_m: 0.9663 - val_loss: 0.4038 - val_f1_m: 0.9688\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3754 - f1_m: 0.9688 - val_loss: 0.3539 - val_f1_m: 0.9688\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3257 - f1_m: 0.9663 - val_loss: 0.3054 - val_f1_m: 0.9688\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2807 - f1_m: 0.9663 - val_loss: 0.2632 - val_f1_m: 0.9688\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2430 - f1_m: 0.9688 - val_loss: 0.2283 - val_f1_m: 0.9688\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2126 - f1_m: 0.9688 - val_loss: 0.2004 - val_f1_m: 0.9688\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1897 - f1_m: 0.9688 - val_loss: 0.1780 - val_f1_m: 0.9688\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1711 - f1_m: 0.9688 - val_loss: 0.1609 - val_f1_m: 0.9688\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1574 - f1_m: 0.9663 - val_loss: 0.1477 - val_f1_m: 0.9688\n",
      "0.9663461446762085\n",
      "16/16 [==============================] - 0s 543us/step\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 10ms/step - loss: 0.7196 - f1_m: 0.2163 - val_loss: 0.6897 - val_f1_m: 0.7031\n",
      "Epoch 2/20\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6913 - f1_m: 0.6250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9b/5l977gy546s07hjrqj1tqdl80000gp/T/ipykernel_26010/1037765748.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0;31m#print(y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0mann_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf1_m\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# metrics=['accuracy']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mann_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f1_m'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0mann_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1454\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1456\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1457\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1750\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 696\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    719\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 721\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3409\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 3410\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   3411\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3412\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#the below code is for binary classification since MNSIT is a multi-class low-resolution image dataset. I am updating the code in the next cell.\n",
    "N=500\n",
    "#Positive=Pos\n",
    "#Negative=Neg\n",
    "positiveN=int((Positive.shape[0]/dataset.shape[0])*N)\n",
    "negativeN=int(N-positiveN)\n",
    "print(positiveN, negativeN)\n",
    "#target variable\n",
    "#target_variable=\"default.payment.next.month\"\n",
    "df1=Positive.sample(positiveN)\n",
    "Positive.drop(df1.index, inplace=True)\n",
    "df2=Negative.sample(negativeN)\n",
    "Negative.drop(df2.index, inplace=True)\n",
    "test_data=df1.append(df2, ignore_index=True)\n",
    "test_data=test_data.sample(frac = 1) #This is to shuffel the training and testing data\n",
    "test_data=test_data.sample(frac = 1)\n",
    "test_data=test_data.sample(frac = 1)\n",
    "X_test=test_data.drop(columns=[target_variable])\n",
    "y_test=to_categorical(test_data[target_variable])\n",
    "\n",
    "# adding dense layer\n",
    "initial_model= get_initial_model(X_test.shape[1], 2)\n",
    "initial_model.set_weights(update_weights(initial_model.get_weights()))\n",
    "Models=[]\n",
    "val_acc=[]\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "val_loss=[]\n",
    "train_loss=[]\n",
    "add_weights=[]\n",
    "while Positive.empty==False and Negative.empty==False:\n",
    "  print(positiveN, negativeN)\n",
    "  df1=Positive.sample(min(positiveN, len(Positive)))\n",
    "  Positive.drop(df1.index, inplace=True)\n",
    "  df2=Negative.sample(min(negativeN, len(Negative)))\n",
    "  Negative.drop(df2.index, inplace=True)\n",
    "  train_data=df1.append(df2, ignore_index=True)\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "    \n",
    "  #all models have different initialization\n",
    "  # define the sequential model\n",
    "  \"\"\"initial_model = keras.Sequential()\n",
    "\n",
    "    # adding dense layer\n",
    "  initial_model.add(Dense(5, input_dim=X_test.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "  initial_model.add(Dense(10, activation='relu'))\n",
    "  initial_model.add(Dense(5, activation='relu'))\n",
    "\n",
    "    # adding dense layer with softmax activation/output layer\n",
    "  initial_model.add(Dense(2, activation='softmax'))\n",
    "  #initial_model.summary()\"\"\"\n",
    "  ann_model=get_initial_model(X_test.shape[1], 2) #same intial weights\n",
    "  ann_model.set_weights(initial_model.get_weights())\n",
    "  X_train=train_data.drop(columns=[target_variable])\n",
    "  #train_data[target_variable]=train_data[target_variable]-1 #only for skin_nonskin dataset\n",
    "  y_train=to_categorical(train_data[target_variable])\n",
    "  #print(y_train)\n",
    "  ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_m]) # metrics=['accuracy']\n",
    "  history = ann_model.fit(X_train, y_train, epochs=20, validation_split=0.2, verbose=1)\n",
    "  print(history.history['f1_m'][-1])\n",
    "  ann_model.set_weights(update_weights(ann_model.get_weights()))\n",
    "  pred_test=ann_model.predict(X_test)\n",
    "  present=False\n",
    "  for i in range(len(Models)):\n",
    "    if (check_models(Models[i][0], ann_model.get_weights())):\n",
    "      print(\"if any\")\n",
    "      Models[i][1]=Models[i][1]+1\n",
    "      add_weights[i].append(ann_model.get_weights())\n",
    "      val_acc[i].append(history.history['val_f1_m'])\n",
    "      train_acc[i].append(history.history['f1_m'])\n",
    "      test_acc[i].append(f1_m(y_test, pred_test).numpy())\n",
    "      val_loss[i].append(history.history['val_loss'])\n",
    "      train_loss[i].append(history.history['loss'])\n",
    "      present=True\n",
    "      break;\n",
    "  if present==False:\n",
    "    add_weights.append([ann_model.get_weights()])\n",
    "    Models.append([ann_model.get_weights(), 1])\n",
    "    val_acc.append([history.history['val_f1_m']])\n",
    "    train_acc.append([history.history['f1_m']])\n",
    "    test_acc.append([f1_m(y_test, pred_test).numpy()])\n",
    "    val_loss.append([history.history['val_loss']])\n",
    "    train_loss.append([history.history['loss']])\n",
    "for i in range(len(Models)):\n",
    "  print(Models[i][1])\n",
    "print(\"All Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OleKiUkRE-gS",
    "outputId": "7cbd3ced-7e29-4dd6-baab-50939cb6662c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "2\n",
      "2\n",
      "[[0.96599996, 0.96599996, 0.96599996, 0.96599996, 0.96599996, 0.96599996, 0.96599996, 0.96599996, 0.96599996, 0.96599996, 0.96599996, 0.96599996, 0.96599996, 0.96599996, 0.96599996, 0.96599996, 0.96599996], [0.96599996, 0.96599996]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Models)):\n",
    "  print(Models[i][1])\n",
    "print(len(Models))\n",
    "#test_Acc = [i.numpy() for i in test_acc]\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "dipg-81EFDBt",
    "outputId": "860a8888-5d6c-489a-8ab8-9b3049728127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#not needed in this case\n",
    "#this works for getting sorted recurrent models by frequency no\n",
    "A=np.argsort(np.array(Models).T[1])[::-1][:2]\n",
    "print(A)\n",
    "temp=list(np.array(Models)[A])\n",
    "#print(temp[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WhS_T7P0FFQK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \n",
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"\n",
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#reducing the metrics lists to top 5 models only\n",
    "val_acc=list(np.array(val_acc)[A])\n",
    "test_acc=list(np.array(test_acc)[A])\n",
    "train_acc=list(np.array(train_acc)[A])\n",
    "val_loss=list(np.array(val_loss)[A])\n",
    "train_loss=list(np.array(train_loss)[A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.03 , -0.08 ,  0.03 ,  0.004,  0.03 ],\n",
      "       [-0.02 ,  0.07 ,  0.01 , -0.01 ,  0.02 ],\n",
      "       [ 0.01 , -0.02 , -0.03 , -0.01 , -0.07 ],\n",
      "       [-0.14 ,  0.004,  0.04 ,  0.001, -0.04 ],\n",
      "       [-0.06 , -0.02 , -0.07 , -0.003, -0.08 ],\n",
      "       [ 0.26 , -0.02 ,  0.08 , -0.05 ,  0.12 ],\n",
      "       [ 0.27 , -0.06 ,  0.18 ,  0.04 ,  0.27 ],\n",
      "       [ 0.28 , -0.13 ,  0.12 , -0.06 ,  0.21 ]], dtype=float32), array([ 0.26, -0.07,  0.14, -0.08,  0.2 ], dtype=float32), array([[-0.6 ,  0.58,  0.3 , -0.4 ,  0.77,  0.56,  0.91, -0.26, -0.45,\n",
      "        -0.15],\n",
      "       [ 0.11,  0.08, -0.09,  0.09, -0.02, -0.11, -0.36, -0.19, -0.04,\n",
      "        -0.22],\n",
      "       [-0.35,  0.17,  0.31, -0.45,  0.1 , -0.1 , -0.06, -0.58, -0.49,\n",
      "         0.36],\n",
      "       [-0.44, -0.05, -0.32, -0.39,  0.43,  0.26, -0.13,  0.46,  0.02,\n",
      "        -0.17],\n",
      "       [-0.62,  0.49,  0.25, -0.46, -0.19,  0.27,  0.4 , -0.2 ,  0.47,\n",
      "        -0.22]], dtype=float32), array([-0.01,  0.18,  0.21, -0.01,  0.15,  0.18,  0.24, -0.02, -0.02,\n",
      "       -0.03], dtype=float32), array([[-0.6 ,  0.35,  0.43, -0.61, -0.2 ],\n",
      "       [ 0.8 , -0.12,  0.14,  0.54, -0.2 ],\n",
      "       [ 0.9 , -0.16,  0.91, -0.45, -0.19],\n",
      "       [-0.1 , -0.45, -0.02,  0.59,  0.36],\n",
      "       [-0.11, -0.13,  0.62, -0.51, -0.07],\n",
      "       [ 0.71,  0.35,  0.06,  0.17,  0.47],\n",
      "       [ 0.46, -0.47,  0.03, -0.05, -0.11],\n",
      "       [-0.22,  0.39,  0.09, -0.27, -0.04],\n",
      "       [-0.54,  0.26,  0.23,  0.2 , -0.4 ],\n",
      "       [ 0.19, -0.56, -0.45,  0.27,  0.18]], dtype=float32), array([ 0.18, -0.02,  0.18, -0.04, -0.01], dtype=float32), array([[ 0.65, -0.38],\n",
      "       [-0.81,  0.06],\n",
      "       [ 0.11, -0.7 ],\n",
      "       [-0.21,  0.64],\n",
      "       [-0.72, -0.24]], dtype=float32), array([ 0.14, -0.14], dtype=float32)]\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "yhn tk\n",
      "17\n",
      "Done\n",
      "[array([[ 2.0e-02, -8.0e-02, -2.0e-02,  1.0e-03, -3.0e-02],\n",
      "       [-5.0e-02,  7.0e-02, -3.0e-02, -1.0e-02, -4.0e-02],\n",
      "       [ 5.0e-02, -1.0e-02,  1.0e-02, -1.0e-02,  3.0e-03],\n",
      "       [-1.6e-01, -1.0e-02, -3.0e-02,  4.0e-03, -1.2e-01],\n",
      "       [-7.0e-02, -2.0e-02, -8.0e-02, -2.0e-04, -1.1e-01],\n",
      "       [ 3.3e-01, -1.0e-02,  1.9e-01, -5.0e-02,  2.0e-01],\n",
      "       [ 2.6e-01, -6.0e-02,  1.8e-01,  4.0e-02,  2.5e-01],\n",
      "       [ 2.9e-01, -1.4e-01,  1.4e-01, -5.0e-02,  2.1e-01]], dtype=float32), array([ 0.26, -0.07,  0.16, -0.08,  0.2 ], dtype=float32), array([[-0.6 ,  0.59,  0.31, -0.4 ,  0.82,  0.58,  0.93, -0.27, -0.45,\n",
      "        -0.19],\n",
      "       [ 0.11,  0.08, -0.09,  0.08, -0.01, -0.11, -0.37, -0.19, -0.04,\n",
      "        -0.22],\n",
      "       [-0.34,  0.19,  0.32, -0.46,  0.13, -0.09, -0.06, -0.57, -0.49,\n",
      "         0.42],\n",
      "       [-0.43, -0.05, -0.33, -0.4 ,  0.43,  0.26, -0.14,  0.46,  0.02,\n",
      "        -0.17],\n",
      "       [-0.62,  0.5 ,  0.26, -0.46, -0.18,  0.28,  0.41, -0.2 ,  0.47,\n",
      "        -0.2 ]], dtype=float32), array([-0.02,  0.18,  0.2 , -0.02,  0.15,  0.18,  0.23, -0.02, -0.02,\n",
      "        0.03], dtype=float32), array([[-0.58,  0.35,  0.45, -0.61, -0.2 ],\n",
      "       [ 0.8 , -0.11,  0.14,  0.54, -0.2 ],\n",
      "       [ 0.91, -0.16,  0.91, -0.46, -0.18],\n",
      "       [-0.11, -0.45, -0.02,  0.59,  0.36],\n",
      "       [-0.09, -0.13,  0.63, -0.52, -0.07],\n",
      "       [ 0.71,  0.35,  0.07,  0.17,  0.47],\n",
      "       [ 0.47, -0.47,  0.04, -0.05, -0.11],\n",
      "       [-0.24,  0.39,  0.08, -0.27, -0.04],\n",
      "       [-0.55,  0.26,  0.23,  0.2 , -0.4 ],\n",
      "       [ 0.05, -0.57, -0.6 ,  0.27,  0.18]], dtype=float32), array([ 0.17, -0.02,  0.18, -0.04, -0.01], dtype=float32), array([[ 0.66, -0.39],\n",
      "       [-0.81,  0.06],\n",
      "       [ 0.12, -0.71],\n",
      "       [-0.21,  0.64],\n",
      "       [-0.72, -0.24]], dtype=float32), array([ 0.14, -0.14], dtype=float32)]\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "yhn tk\n",
      "2\n",
      "Done\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9b/5l977gy546s07hjrqj1tqdl80000gp/T/ipykernel_77179/1841370904.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmean_model_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmean_model_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_avg_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "mean_model_weights=[]\n",
    "for i in range(5):\n",
    "    mean_model_weights.append(get_avg_weights(add_weights[i],X_test.shape[1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 0s 574us/step - loss: 0.1271 - accuracy: 0.9673\n",
      "94/94 [==============================] - 0s 576us/step - loss: 0.1397 - accuracy: 0.9633\n",
      "219/219 [==============================] - 0s 560us/step - loss: 0.1260 - accuracy: 0.9673\n",
      "94/94 [==============================] - 0s 554us/step - loss: 0.1382 - accuracy: 0.9633\n",
      "Done for model selection\n"
     ]
    }
   ],
   "source": [
    "#mean models\n",
    "from sklearn.model_selection import train_test_split\n",
    "mean_models=[]\n",
    "mean_model_train_metrics=[]\n",
    "mean_model_loss=[]\n",
    "mean_model_acc=[]\n",
    "mean_model_test_metrics=[]\n",
    "mean_model_test_loss=[]\n",
    "mean_model_test_acc=[]\n",
    "y = to_categorical(dataset[target_variable])\n",
    "X = dataset.drop(columns=target_variable)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "for i in range(2):\n",
    "    init_model=get_initial_model(X_test.shape[1], 2)\n",
    "    init_model.set_weights(mean_model_weights[i])\n",
    "    mean_model_train_metrics.append(init_model.evaluate(X_train, y_train))\n",
    "    mean_model_loss.append(mean_model_train_metrics[i][0])\n",
    "    mean_model_acc.append(mean_model_train_metrics[i][1])\n",
    "    mean_model_test_metrics.append(init_model.evaluate(X_test, y_test))\n",
    "    mean_model_test_loss.append(mean_model_test_metrics[i][0])\n",
    "    mean_model_test_acc.append(mean_model_test_metrics[i][1])\n",
    "print(\"Done for model selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12707483768463135, 0.1259629726409912] [0.967285692691803, 0.967285692691803]\n",
      "[0.13974390923976898, 0.13817985355854034] [0.9633333086967468, 0.9633333086967468]\n"
     ]
    }
   ],
   "source": [
    "print(mean_model_loss, mean_model_acc)\n",
    "print(mean_model_test_loss, mean_model_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BkdCnd-kFMVR",
    "outputId": "7dbec5b3-1ebd-4f7d-97d8-b50be6c21426",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.4387 - accuracy: 0.9532 - val_loss: 0.1457 - val_accuracy: 0.9771\n",
      "Epoch 2/20\n",
      "175/175 [==============================] - 0s 813us/step - loss: 0.1310 - accuracy: 0.9668 - val_loss: 0.0997 - val_accuracy: 0.9771\n",
      "Epoch 3/20\n",
      "175/175 [==============================] - 0s 831us/step - loss: 0.1195 - accuracy: 0.9668 - val_loss: 0.0964 - val_accuracy: 0.9771\n",
      "Epoch 4/20\n",
      "175/175 [==============================] - 0s 827us/step - loss: 0.1159 - accuracy: 0.9668 - val_loss: 0.0914 - val_accuracy: 0.9771\n",
      "Epoch 5/20\n",
      "175/175 [==============================] - 0s 816us/step - loss: 0.1129 - accuracy: 0.9668 - val_loss: 0.0891 - val_accuracy: 0.9771\n",
      "Epoch 6/20\n",
      "175/175 [==============================] - 0s 857us/step - loss: 0.1098 - accuracy: 0.9668 - val_loss: 0.0861 - val_accuracy: 0.9771\n",
      "Epoch 7/20\n",
      "175/175 [==============================] - 0s 844us/step - loss: 0.1067 - accuracy: 0.9668 - val_loss: 0.0846 - val_accuracy: 0.9771\n",
      "Epoch 8/20\n",
      "175/175 [==============================] - 0s 844us/step - loss: 0.1039 - accuracy: 0.9668 - val_loss: 0.0798 - val_accuracy: 0.9771\n",
      "Epoch 9/20\n",
      "175/175 [==============================] - 0s 845us/step - loss: 0.1024 - accuracy: 0.9677 - val_loss: 0.0792 - val_accuracy: 0.9786\n",
      "Epoch 10/20\n",
      "175/175 [==============================] - 0s 850us/step - loss: 0.1011 - accuracy: 0.9705 - val_loss: 0.0765 - val_accuracy: 0.9793\n",
      "Epoch 11/20\n",
      "175/175 [==============================] - 0s 842us/step - loss: 0.1000 - accuracy: 0.9705 - val_loss: 0.0740 - val_accuracy: 0.9779\n",
      "Epoch 12/20\n",
      "175/175 [==============================] - 0s 832us/step - loss: 0.0991 - accuracy: 0.9705 - val_loss: 0.0750 - val_accuracy: 0.9786\n",
      "Epoch 13/20\n",
      "175/175 [==============================] - 0s 814us/step - loss: 0.0990 - accuracy: 0.9709 - val_loss: 0.0747 - val_accuracy: 0.9779\n",
      "Epoch 14/20\n",
      "175/175 [==============================] - 0s 805us/step - loss: 0.0984 - accuracy: 0.9700 - val_loss: 0.0771 - val_accuracy: 0.9779\n",
      "Epoch 15/20\n",
      "175/175 [==============================] - 0s 813us/step - loss: 0.0979 - accuracy: 0.9707 - val_loss: 0.0754 - val_accuracy: 0.9771\n",
      "Epoch 16/20\n",
      "175/175 [==============================] - 0s 842us/step - loss: 0.0981 - accuracy: 0.9704 - val_loss: 0.0726 - val_accuracy: 0.9779\n",
      "Epoch 17/20\n",
      "175/175 [==============================] - 0s 800us/step - loss: 0.0975 - accuracy: 0.9702 - val_loss: 0.0752 - val_accuracy: 0.9779\n",
      "Epoch 18/20\n",
      "175/175 [==============================] - 0s 826us/step - loss: 0.0971 - accuracy: 0.9709 - val_loss: 0.0732 - val_accuracy: 0.9771\n",
      "Epoch 19/20\n",
      "175/175 [==============================] - 0s 819us/step - loss: 0.0965 - accuracy: 0.9707 - val_loss: 0.0738 - val_accuracy: 0.9764\n",
      "Epoch 20/20\n",
      "175/175 [==============================] - 0s 814us/step - loss: 0.0968 - accuracy: 0.9707 - val_loss: 0.0721 - val_accuracy: 0.9771\n"
     ]
    }
   ],
   "source": [
    "#benchmark model\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = to_categorical(dataset[target_variable])\n",
    "X = dataset.drop(columns=target_variable)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "benchmark_model=get_initial_model(X_test.shape[1], 2) #same intial weights\n",
    "benchmark_model.set_weights(initial_model.get_weights())\n",
    "history = benchmark_model.fit(X_train, y_train, epochs=20, validation_split=0.2, verbose=1)\n",
    "benchmark_model.set_weights(update_weights(benchmark_model.get_weights()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 5)                 45        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                60        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 12        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 172\n",
      "Trainable params: 172\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 17:43:05.044897: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = to_categorical(dataset[target_variable])\n",
    "X = dataset.drop(columns=target_variable)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "benchmark_model=get_initial_model(X_test.shape[1], 2) #same intial weights\n",
    "benchmark_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ny9CuQeFSGk",
    "outputId": "faf15961-5f98-4428-b2ae-5e798c4792c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 588us/step - loss: 0.1030 - accuracy: 0.9647\n"
     ]
    }
   ],
   "source": [
    "#benchmark metrics\n",
    "benchmark_loss=history.history['loss']\n",
    "benchmark_val_loss=history.history['val_loss']\n",
    "benchmark_acc=history.history['accuracy']\n",
    "benchmark_val_acc=history.history['val_accuracy']\n",
    "benchmark_test_metrics=benchmark_model.evaluate(X_test, y_test)\n",
    "benchmark_test_loss=benchmark_test_metrics[0]\n",
    "benchmark_test_accuracy=benchmark_test_metrics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from here onwards the comparison and computation of DP:\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_privacy\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "l2_norm_clip = 1.5\n",
    "noise_multiplier = 6.9\n",
    "num_microbatches = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 1s 2ms/step - loss: 0.6996 - f1_m: 0.4733 - val_loss: 0.6931 - val_f1_m: 0.6858\n",
      "Epoch 2/20\n",
      "133/175 [=====================>........] - ETA: 0s - loss: 0.6877 - f1_m: 0.7789"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 0s 1ms/step - loss: 0.6852 - f1_m: 0.8073 - val_loss: 0.6759 - val_f1_m: 0.8783\n",
      "Epoch 3/20\n",
      "175/175 [==============================] - 0s 991us/step - loss: 0.6713 - f1_m: 0.8952 - val_loss: 0.6628 - val_f1_m: 0.9164\n",
      "Epoch 4/20\n",
      "175/175 [==============================] - 0s 910us/step - loss: 0.6598 - f1_m: 0.9362 - val_loss: 0.6577 - val_f1_m: 0.9437\n",
      "Epoch 5/20\n",
      "175/175 [==============================] - 0s 934us/step - loss: 0.6548 - f1_m: 0.9613 - val_loss: 0.6552 - val_f1_m: 0.9607\n",
      "Epoch 6/20\n",
      "175/175 [==============================] - 0s 975us/step - loss: 0.6498 - f1_m: 0.9682 - val_loss: 0.6473 - val_f1_m: 0.9614\n",
      "Epoch 7/20\n",
      "175/175 [==============================] - 0s 922us/step - loss: 0.6410 - f1_m: 0.9689 - val_loss: 0.6419 - val_f1_m: 0.9614\n",
      "Epoch 8/20\n",
      "175/175 [==============================] - 0s 935us/step - loss: 0.6377 - f1_m: 0.9689 - val_loss: 0.6294 - val_f1_m: 0.9614\n",
      "Epoch 9/20\n",
      "175/175 [==============================] - 0s 964us/step - loss: 0.6196 - f1_m: 0.9689 - val_loss: 0.6195 - val_f1_m: 0.9614\n",
      "Epoch 10/20\n",
      "175/175 [==============================] - 0s 928us/step - loss: 0.6161 - f1_m: 0.9689 - val_loss: 0.6191 - val_f1_m: 0.9614\n",
      "Epoch 11/20\n",
      "175/175 [==============================] - 0s 926us/step - loss: 0.6064 - f1_m: 0.9689 - val_loss: 0.6005 - val_f1_m: 0.9614\n",
      "Epoch 12/20\n",
      "175/175 [==============================] - 0s 932us/step - loss: 0.5928 - f1_m: 0.9689 - val_loss: 0.5825 - val_f1_m: 0.9614\n",
      "Epoch 13/20\n",
      "175/175 [==============================] - 0s 935us/step - loss: 0.5749 - f1_m: 0.9689 - val_loss: 0.5736 - val_f1_m: 0.9614\n",
      "Epoch 14/20\n",
      "175/175 [==============================] - 0s 967us/step - loss: 0.5663 - f1_m: 0.9689 - val_loss: 0.5584 - val_f1_m: 0.9614\n",
      "Epoch 15/20\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.5548 - f1_m: 0.9689 - val_loss: 0.5573 - val_f1_m: 0.9614\n",
      "Epoch 16/20\n",
      "175/175 [==============================] - 0s 945us/step - loss: 0.5558 - f1_m: 0.9689 - val_loss: 0.5548 - val_f1_m: 0.9614\n",
      "Epoch 17/20\n",
      "175/175 [==============================] - 0s 934us/step - loss: 0.5415 - f1_m: 0.9689 - val_loss: 0.5358 - val_f1_m: 0.9614\n",
      "Epoch 18/20\n",
      "175/175 [==============================] - 0s 944us/step - loss: 0.5327 - f1_m: 0.9689 - val_loss: 0.5344 - val_f1_m: 0.9614\n",
      "Epoch 19/20\n",
      "175/175 [==============================] - 0s 944us/step - loss: 0.5280 - f1_m: 0.9689 - val_loss: 0.5272 - val_f1_m: 0.9614\n",
      "Epoch 20/20\n",
      "175/175 [==============================] - 0s 922us/step - loss: 0.5188 - f1_m: 0.9689 - val_loss: 0.5214 - val_f1_m: 0.9614\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = to_categorical(dataset[target_variable])\n",
    "X = dataset.drop(columns=target_variable)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "DP_model=get_DP_initial_model(X_test.shape[1], 2) #same intial weights\n",
    "optimizer = tensorflow_privacy.DPKerasAdamOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "DP_model.compile(optimizer=optimizer, loss=loss, metrics=[f1_m])\n",
    "history = DP_model.fit(X_train, y_train,  epochs=20, validation_split=0.2, verbose=1)\n",
    "#benchmark_model.set_weights(update_weights(benchmark_model.get_weights()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD with sampling rate = 0.232% and noise_multiplier = 6.9 iterated over 8616 steps satisfies differential privacy with eps = 0.108 and delta = 1e-05.\n",
      "The optimal RDP order is 128.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10807801462543376"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=X_train.shape[0]*0.8,\n",
    "                                              batch_size=13,\n",
    "                                              noise_multiplier=6.9,\n",
    "                                              epochs=20,\n",
    "                                              delta=1e-5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 0s 591us/step - loss: 0.2394 - f1_m: 0.9643\n",
      "94/94 [==============================] - 0s 607us/step - loss: 0.2296 - f1_m: 0.9703\n"
     ]
    }
   ],
   "source": [
    "#for epsilon ≈ 1\n",
    "DP_1_loss=history.history['loss']\n",
    "DP_1_f1=history.history['f1_m']\n",
    "DP_1_f1_train=DP_model.evaluate(X_train,y_train)\n",
    "DP_1_f1_test=DP_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 0s 581us/step - loss: 0.2216 - f1_m: 0.9631\n",
      "94/94 [==============================] - 0s 585us/step - loss: 0.2030 - f1_m: 0.9731\n"
     ]
    }
   ],
   "source": [
    "#for epsilon ≈ 0.5\n",
    "DP_0_5_loss=history.history['loss']\n",
    "DP_0_5_f1=history.history['f1_m']\n",
    "DP_0_5_f1_train=DP_model.evaluate(X_train,y_train)\n",
    "DP_0_5_f1_test=DP_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 0s 590us/step - loss: 0.5186 - f1_m: 0.9674\n",
      "94/94 [==============================] - 0s 584us/step - loss: 0.5206 - f1_m: 0.9630\n"
     ]
    }
   ],
   "source": [
    "#for epsilon ≈ 0.1\n",
    "DP_0_1_loss=history.history['loss']\n",
    "DP_0_1_f1=history.history['f1_m']\n",
    "DP_0_1_f1_train=DP_model.evaluate(X_train,y_train)\n",
    "DP_0_1_f1_test=DP_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "rCFFzLXlFVD7",
    "outputId": "4ff02ad0-e6d0-451d-cd5b-5aca4eead38b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting graphs for loss and accuracies for the top 5 recurrent models:\n",
    "#plotting val_loss and loss for the models generated and the benchmark model.\n",
    "from random import randint\n",
    "import matplotlib.patches as mpatches\n",
    "color = ['red', 'yellow']\n",
    "leg=[]\n",
    "for i in range(len(color)):\n",
    "    leg.append(mpatches.Patch(color=color[i], label=str(len(val_acc[i]))))\n",
    "n = len(val_acc)\n",
    "print(n)\n",
    "x_axis=np.arange(1, 21, 1)\n",
    "print(x_axis)\n",
    "for i in range(n):\n",
    "    for j in range(len(val_acc[i])):\n",
    "        plt.plot(x_axis,val_acc[i][j], color=color[i], alpha=0.2)\n",
    "    plt.plot(x_axis, np.mean(val_acc[i], axis=0), color=color[i])\n",
    "#plt.xlim(-0.5,20.5)\n",
    "plt.xticks([2.5,5.0,7.5,10.0,12.5,15.0,17.5,20.0],[2,5,7,10,12,15,17,20])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"f1 score\")\n",
    "plt.legend(handles=leg, bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", borderaxespad=0, ncol=5)\n",
    "plt.plot(x_axis, benchmark_val_acc, color='black')\n",
    "plt.savefig(\"fig/ai4i2020_F1_Val_20Epochs_10000.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805
    },
    "id": "Z-jymuwCGCj2",
    "outputId": "a089b7cc-b5c3-4d44-8692-e88f32b7dcba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "0 5\n",
      "0 6\n",
      "0 7\n",
      "0 8\n",
      "0 9\n",
      "0 10\n",
      "0 11\n",
      "0 12\n",
      "0 13\n",
      "0 14\n",
      "0 15\n",
      "0 16\n",
      "1 0\n",
      "1 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train accuracy \n",
    "from random import randint\n",
    "color = []\n",
    "#print(train_acc[4][0])\n",
    "color = ['red', 'yellow','blue', 'green']\n",
    "x_axis=np.arange(1, 21, 1)\n",
    "print(x_axis)\n",
    "for i in range(n):\n",
    "    for j in range(len(train_acc[i])):\n",
    "        print(i,j)\n",
    "        plt.plot(x_axis,train_acc[i][j], color=color[i], alpha=0.2)\n",
    "    plt.plot(x_axis,np.mean(train_acc[i], axis=0), color=color[i])\n",
    "plt.plot(x_axis,benchmark_acc, color='black')\n",
    "plt.xticks([2.5,5.0,7.5,10.0,12.5,15.0,17.5,20.0],[2,5,7,10,12,15,17,20])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"f1 score\")\n",
    "plt.plot(x_axis, DP_1_f1, 'o', color='black')\n",
    "plt.plot(x_axis, DP_0_5_f1, '-.', color='black')\n",
    "plt.plot(x_axis, DP_0_1_f1, 'd', color='black')\n",
    "plt.legend(handles=leg, bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", borderaxespad=0, ncol=5)\n",
    "plt.savefig(\"fig/AI4I2020_F1_train_20Epochs_10000.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 631
    },
    "id": "aXOPc0LzHLpq",
    "outputId": "d37b5a26-991c-4ca3-c1d0-5d398c87d2f6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#training loss\n",
    "from random import randint\n",
    "color = []\n",
    "n = len(train_loss)\n",
    "color = ['red', 'yellow','blue', 'green','cyan']\n",
    "for i in range(n):\n",
    "    for j in range(len(train_loss[i])):\n",
    "        plt.plot(x_axis,train_loss[i][j], color=color[i], alpha=0.2)\n",
    "    plt.plot(x_axis,np.mean(train_loss[i], axis=0), color=color[i])\n",
    "plt.plot(x_axis,benchmark_loss, color='black')\n",
    "plt.xticks([2.5,5.0,7.5,10.0,12.5,15.0,17.5,20.0],[2,5,7,10,12,15,17,20])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(x_axis, DP_1_loss, 'o', color='black')\n",
    "plt.plot(x_axis, DP_0_5_loss, '-.', color='black')\n",
    "plt.plot(x_axis, DP_0_1_loss, 'd', color='black')\n",
    "plt.legend(handles=leg, bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", borderaxespad=0, ncol=5)\n",
    "plt.savefig(\"fig/AI4i2020_loss_train_20Epochs_10000.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "QY_BSAqhHTYk",
    "outputId": "fe4f5819-4003-44cb-cb9c-b08763a30a14"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#validation loss loss\n",
    "from random import randint\n",
    "color = []\n",
    "n = len(val_acc)\n",
    "color = ['red', 'yellow','blue', 'green','cyan']\n",
    "for i in range(n):\n",
    "    for j in range(len(train_loss[i])):\n",
    "        plt.plot(x_axis,val_loss[i][j], color=color[i], alpha=0.2)\n",
    "    plt.plot(x_axis,np.mean(val_loss[i], axis=0), color=color[i])\n",
    "plt.plot(x_axis,benchmark_val_loss, color='black')\n",
    "plt.xticks([2.5,5.0,7.5,10.0,12.5,15.0,17.5,20.0],[2,5,7,10,12,15,17,20])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(handles=leg, bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", borderaxespad=0, ncol=5)\n",
    "plt.savefig(\"fig/ai4i2020_loss_val_20Epochs_10000.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FruJTtqzHZt6",
    "outputId": "9700121f-687d-4ef2-da91-bd43cfca4fc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 626us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 614us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 601us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 605us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 617us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 607us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 604us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 607us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 601us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 624us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 587us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 599us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 591us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 602us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 592us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 592us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 666us/step - loss: 0.1457 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 678us/step - loss: 0.1352 - accuracy: 0.9630\n",
      "94/94 [==============================] - 0s 618us/step - loss: 0.1352 - accuracy: 0.9630\n",
      "[[0.1456855833530426, 0.1456855833530426, 0.1456855833530426, 0.1456855833530426, 0.1456855833530426, 0.1456855833530426, 0.1456855833530426, 0.1456855833530426, 0.1456855833530426, 0.1456855833530426, 0.1456855833530426, 0.1456855833530426, 0.1456855833530426, 0.1456855833530426, 0.1456855833530426, 0.1456855833530426, 0.1456855833530426], [0.13523511588573456, 0.13523511588573456]] [[0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628], [0.9629999995231628, 0.9629999995231628]]\n"
     ]
    }
   ],
   "source": [
    "#lets compute the accuracy for bigger test set 30% benchmark test data\n",
    "high_test_loss=[]\n",
    "high_test_f1=[]\n",
    "for i in range(len(add_weights)):\n",
    "    for j in range(len(add_weights[i])):\n",
    "        high_test_model=get_initial_model(X_test.shape[1], 2) #same intial weights\n",
    "        high_test_model.set_weights(Models[i][0])\n",
    "        high_test_metrics=high_test_model.evaluate(X_test,y_test)\n",
    "        if j == 0:\n",
    "            high_test_loss.append([high_test_metrics[0]])\n",
    "            high_test_f1.append([high_test_metrics[1]])\n",
    "        else:\n",
    "            high_test_loss[i].append(high_test_metrics[0])\n",
    "            high_test_f1[i].append(high_test_metrics[1])\n",
    "print(high_test_loss, high_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xtc3FnwMHlLC",
    "outputId": "07c12805-4318-4bf9-ffee-44228fd600a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628, 0.9629999995231628], [0.9629999995231628, 0.9629999995231628]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "high_test_accs=list(np.array(high_test_f1)[A])\n",
    "print(high_test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "id": "PqawvsbSHoWx",
    "outputId": "767657c6-9674-41ea-b32e-f92a382c4c45"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for test accuracy plot\n",
    "#plot test accuracy of smaller test set\n",
    "benchmark_test_accs=[benchmark_test_accuracy]*n\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0.9, 1.0])\n",
    "for i in range(n):\n",
    "    plt.scatter([i]*len(test_acc[i]),test_acc[i], color=color[i], alpha=0.2)\n",
    "    plt.scatter(i,high_test_accs[i][0], color=color[i])\n",
    "plt.plot(benchmark_test_accs, color='black')\n",
    "plt.legend(handles=leg, bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", borderaxespad=0, ncol=5)\n",
    "plt.savefig(\"fig/AI4i2020_test_acc_20Epochs_1000.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "beHYUTasHqwk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting graphs for loss and accuracies for the top 5 recurrent models:\n",
    "#plotting val_loss and loss for the models generated and the benchmark model.\n",
    "from random import randint\n",
    "import matplotlib.patches as mpatches\n",
    "color = ['red', 'yellow']\n",
    "leg=[]\n",
    "for i in range(len(color)):\n",
    "    leg.append(mpatches.Patch(color=color[i], label=str(len(val_acc[i]))))\n",
    "n = len(val_acc)\n",
    "x_axis=np.arange(1, 21, 1)\n",
    "print(x_axis)\n",
    "for i in range(n):\n",
    "    for j in range(len(val_acc[i])):\n",
    "        plt.plot(x_axis,val_acc[i][j], color=color[i], alpha=0.2)\n",
    "    plt.plot(x_axis, np.mean(val_acc[i], axis=0), color=color[i])\n",
    "#plt.xlim(-0.5,20.5)\n",
    "plt.xticks([2.5,5.0,7.5,10.0,12.5,15.0,17.5,20.0],[2,5,7,10,12,15,17,20])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"f1 score\")\n",
    "plt.legend(handles=leg, bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", borderaxespad=0, ncol=5)\n",
    "plt.plot(x_axis, benchmark_val_acc, color='black')\n",
    "plt.savefig(\"fig/ai4i2020_F1_Val_20Epochs_10000.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552], [0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375], [0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625], [0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625], [0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625], [0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552, 0.9531249403953552], [0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625], [0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552], [0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552, 0.9609374403953552], [0.9140625, 0.9140625, 0.9140625, 0.9140625, 0.9140625, 0.9140625, 0.9140625, 0.9140625, 0.9140625, 0.9140625, 0.9140625, 0.9140625, 0.9140625, 0.9140625, 0.9140625, 0.9140625, 0.9140625, 0.9140625, 0.9140625, 0.9140625], [0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552, 0.9453124403953552], [0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875], [0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875], [0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625], [0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625]], [[0.90625, 0.90625, 0.90625, 0.90625, 0.90625, 0.90625, 0.90625, 0.90625, 0.90625, 0.90625, 0.90625, 0.90625, 0.90625, 0.90625, 0.90625, 0.90625, 0.90625, 0.90625, 0.90625, 0.90625], [0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625, 0.9765625]]]\n"
     ]
    }
   ],
   "source": [
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot for mean model accuracy and benchmark loss:\n",
    "n=2\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0.8, 1.1])\n",
    "for i in range(n):\n",
    "    plt.plot(i, mean_model_acc[i],'o',color=color[i])\n",
    "    plt.plot(i, mean_model_test_acc[i],'o', mfc='none',color=color[i])\n",
    "plt.plot([benchmark_acc[-1]]*n,color='black')\n",
    "plt.plot([benchmark_test_accuracy]*n,'-.',color='black')\n",
    "plt.legend(handles=leg, bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", borderaxespad=0, ncol=5)\n",
    "plt.savefig(\"fig/ai4i_mean_model_results.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483 17\n",
      "483 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.3919 - f1_m: 0.9327 - val_loss: 0.1631 - val_f1_m: 0.9844\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1809 - f1_m: 0.9639 - val_loss: 0.1042 - val_f1_m: 0.9844\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1592 - f1_m: 0.9615 - val_loss: 0.0906 - val_f1_m: 0.9844\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1412 - f1_m: 0.9615 - val_loss: 0.0861 - val_f1_m: 0.9844\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1269 - f1_m: 0.9615 - val_loss: 0.0864 - val_f1_m: 0.9844\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1169 - f1_m: 0.9639 - val_loss: 0.0859 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1077 - f1_m: 0.9663 - val_loss: 0.0798 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0974 - f1_m: 0.9639 - val_loss: 0.0739 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0907 - f1_m: 0.9639 - val_loss: 0.0740 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0832 - f1_m: 0.9736 - val_loss: 0.0711 - val_f1_m: 0.9766\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0765 - f1_m: 0.9784 - val_loss: 0.0696 - val_f1_m: 0.9844\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0707 - f1_m: 0.9760 - val_loss: 0.0660 - val_f1_m: 0.9844\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0675 - f1_m: 0.9688 - val_loss: 0.0633 - val_f1_m: 0.9844\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0635 - f1_m: 0.9808 - val_loss: 0.0655 - val_f1_m: 0.9844\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0585 - f1_m: 0.9856 - val_loss: 0.0604 - val_f1_m: 0.9844\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0550 - f1_m: 0.9856 - val_loss: 0.0555 - val_f1_m: 0.9844\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0523 - f1_m: 0.9880 - val_loss: 0.0583 - val_f1_m: 0.9844\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0500 - f1_m: 0.9856 - val_loss: 0.0570 - val_f1_m: 0.9844\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0475 - f1_m: 0.9856 - val_loss: 0.0539 - val_f1_m: 0.9844\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0465 - f1_m: 0.9880 - val_loss: 0.0517 - val_f1_m: 0.9844\n",
      "0.9879807829856873\n",
      "16/16 [==============================] - 0s 696us/step\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 0.3719 - f1_m: 0.9375 - val_loss: 0.2673 - val_f1_m: 0.9609\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1482 - f1_m: 0.9712 - val_loss: 0.2634 - val_f1_m: 0.9609\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1216 - f1_m: 0.9712 - val_loss: 0.2696 - val_f1_m: 0.9609\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1041 - f1_m: 0.9712 - val_loss: 0.2715 - val_f1_m: 0.9609\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0945 - f1_m: 0.9712 - val_loss: 0.2727 - val_f1_m: 0.9609\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0878 - f1_m: 0.9663 - val_loss: 0.2653 - val_f1_m: 0.9609\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0816 - f1_m: 0.9760 - val_loss: 0.2633 - val_f1_m: 0.9609\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0785 - f1_m: 0.9736 - val_loss: 0.2645 - val_f1_m: 0.9609\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0744 - f1_m: 0.9736 - val_loss: 0.2582 - val_f1_m: 0.9609\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0715 - f1_m: 0.9760 - val_loss: 0.2513 - val_f1_m: 0.9609\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0690 - f1_m: 0.9784 - val_loss: 0.2500 - val_f1_m: 0.9609\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0662 - f1_m: 0.9808 - val_loss: 0.2519 - val_f1_m: 0.9609\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0645 - f1_m: 0.9760 - val_loss: 0.2547 - val_f1_m: 0.9609\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0623 - f1_m: 0.9832 - val_loss: 0.2479 - val_f1_m: 0.9688\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0605 - f1_m: 0.9832 - val_loss: 0.2428 - val_f1_m: 0.9609\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0592 - f1_m: 0.9832 - val_loss: 0.2460 - val_f1_m: 0.9609\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0576 - f1_m: 0.9832 - val_loss: 0.2410 - val_f1_m: 0.9609\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0565 - f1_m: 0.9856 - val_loss: 0.2448 - val_f1_m: 0.9609\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0552 - f1_m: 0.9856 - val_loss: 0.2444 - val_f1_m: 0.9609\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0558 - f1_m: 0.9784 - val_loss: 0.2428 - val_f1_m: 0.9688\n",
      "0.9783653616905212\n",
      "16/16 [==============================] - 0s 619us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 0.4116 - f1_m: 0.9207 - val_loss: 0.1651 - val_f1_m: 0.9922\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2141 - f1_m: 0.9615 - val_loss: 0.1003 - val_f1_m: 0.9922\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1849 - f1_m: 0.9591 - val_loss: 0.0904 - val_f1_m: 0.9922\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1627 - f1_m: 0.9615 - val_loss: 0.0956 - val_f1_m: 0.9922\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1431 - f1_m: 0.9615 - val_loss: 0.0925 - val_f1_m: 0.9922\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1277 - f1_m: 0.9567 - val_loss: 0.0810 - val_f1_m: 0.9922\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1160 - f1_m: 0.9639 - val_loss: 0.0733 - val_f1_m: 0.9922\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1067 - f1_m: 0.9639 - val_loss: 0.0654 - val_f1_m: 0.9922\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1022 - f1_m: 0.9688 - val_loss: 0.0747 - val_f1_m: 0.9844\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0922 - f1_m: 0.9687 - val_loss: 0.0644 - val_f1_m: 0.9922\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0914 - f1_m: 0.9712 - val_loss: 0.0605 - val_f1_m: 0.9922\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0865 - f1_m: 0.9712 - val_loss: 0.0571 - val_f1_m: 0.9922\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0817 - f1_m: 0.9688 - val_loss: 0.0666 - val_f1_m: 0.9844\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0805 - f1_m: 0.9712 - val_loss: 0.0640 - val_f1_m: 0.9844\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0803 - f1_m: 0.9736 - val_loss: 0.0573 - val_f1_m: 0.9844\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0758 - f1_m: 0.9784 - val_loss: 0.0629 - val_f1_m: 0.9844\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0749 - f1_m: 0.9784 - val_loss: 0.0627 - val_f1_m: 0.9844\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0722 - f1_m: 0.9808 - val_loss: 0.0621 - val_f1_m: 0.9844\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0739 - f1_m: 0.9712 - val_loss: 0.0524 - val_f1_m: 0.9844\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0725 - f1_m: 0.9784 - val_loss: 0.0690 - val_f1_m: 0.9844\n",
      "0.9783653616905212\n",
      "16/16 [==============================] - 0s 657us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 10ms/step - loss: 0.3772 - f1_m: 0.9327 - val_loss: 0.1990 - val_f1_m: 0.9766\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1732 - f1_m: 0.9663 - val_loss: 0.1635 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1465 - f1_m: 0.9663 - val_loss: 0.1515 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1336 - f1_m: 0.9663 - val_loss: 0.1337 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1204 - f1_m: 0.9663 - val_loss: 0.1233 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1133 - f1_m: 0.9663 - val_loss: 0.1193 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1079 - f1_m: 0.9639 - val_loss: 0.1135 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1047 - f1_m: 0.9639 - val_loss: 0.1051 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0991 - f1_m: 0.9687 - val_loss: 0.1035 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0952 - f1_m: 0.9663 - val_loss: 0.1021 - val_f1_m: 0.9766\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0917 - f1_m: 0.9663 - val_loss: 0.0980 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0883 - f1_m: 0.9688 - val_loss: 0.0948 - val_f1_m: 0.9844\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0859 - f1_m: 0.9688 - val_loss: 0.0930 - val_f1_m: 0.9766\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0845 - f1_m: 0.9688 - val_loss: 0.0913 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0821 - f1_m: 0.9663 - val_loss: 0.0897 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0797 - f1_m: 0.9712 - val_loss: 0.0837 - val_f1_m: 0.9844\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0773 - f1_m: 0.9736 - val_loss: 0.0840 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0765 - f1_m: 0.9736 - val_loss: 0.0831 - val_f1_m: 0.9844\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0746 - f1_m: 0.9736 - val_loss: 0.0822 - val_f1_m: 0.9844\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0753 - f1_m: 0.9639 - val_loss: 0.0829 - val_f1_m: 0.9844\n",
      "0.963942289352417\n",
      "16/16 [==============================] - 0s 709us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 0.4045 - f1_m: 0.9327 - val_loss: 0.3166 - val_f1_m: 0.9531\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1947 - f1_m: 0.9736 - val_loss: 0.3202 - val_f1_m: 0.9531\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1633 - f1_m: 0.9736 - val_loss: 0.3002 - val_f1_m: 0.9531\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1440 - f1_m: 0.9736 - val_loss: 0.2751 - val_f1_m: 0.9531\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1261 - f1_m: 0.9736 - val_loss: 0.2379 - val_f1_m: 0.9531\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1089 - f1_m: 0.9688 - val_loss: 0.2153 - val_f1_m: 0.9609\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0951 - f1_m: 0.9784 - val_loss: 0.1941 - val_f1_m: 0.9609\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0862 - f1_m: 0.9784 - val_loss: 0.1894 - val_f1_m: 0.9609\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0799 - f1_m: 0.9760 - val_loss: 0.1796 - val_f1_m: 0.9609\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0739 - f1_m: 0.9832 - val_loss: 0.1690 - val_f1_m: 0.9609\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0688 - f1_m: 0.9856 - val_loss: 0.1617 - val_f1_m: 0.9609\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0665 - f1_m: 0.9856 - val_loss: 0.1638 - val_f1_m: 0.9609\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0628 - f1_m: 0.9832 - val_loss: 0.1680 - val_f1_m: 0.9609\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0590 - f1_m: 0.9880 - val_loss: 0.1636 - val_f1_m: 0.9609\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0572 - f1_m: 0.9880 - val_loss: 0.1608 - val_f1_m: 0.9609\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0560 - f1_m: 0.9856 - val_loss: 0.1629 - val_f1_m: 0.9609\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0528 - f1_m: 0.9880 - val_loss: 0.1584 - val_f1_m: 0.9688\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0510 - f1_m: 0.9880 - val_loss: 0.1603 - val_f1_m: 0.9609\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0491 - f1_m: 0.9880 - val_loss: 0.1565 - val_f1_m: 0.9688\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0468 - f1_m: 0.9880 - val_loss: 0.1663 - val_f1_m: 0.9609\n",
      "0.9879807829856873\n",
      "16/16 [==============================] - 0s 566us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 0.3913 - f1_m: 0.9207 - val_loss: 0.1187 - val_f1_m: 1.0000\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2087 - f1_m: 0.9591 - val_loss: 0.0429 - val_f1_m: 1.0000\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1894 - f1_m: 0.9591 - val_loss: 0.0380 - val_f1_m: 1.0000\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1727 - f1_m: 0.9591 - val_loss: 0.0485 - val_f1_m: 1.0000\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1584 - f1_m: 0.9567 - val_loss: 0.0532 - val_f1_m: 1.0000\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1480 - f1_m: 0.9567 - val_loss: 0.0489 - val_f1_m: 1.0000\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1390 - f1_m: 0.9591 - val_loss: 0.0491 - val_f1_m: 1.0000\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1299 - f1_m: 0.9567 - val_loss: 0.0491 - val_f1_m: 1.0000\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1220 - f1_m: 0.9567 - val_loss: 0.0516 - val_f1_m: 0.9922\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1154 - f1_m: 0.9639 - val_loss: 0.0494 - val_f1_m: 0.9922\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1096 - f1_m: 0.9663 - val_loss: 0.0472 - val_f1_m: 0.9922\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1044 - f1_m: 0.9663 - val_loss: 0.0481 - val_f1_m: 0.9922\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1018 - f1_m: 0.9688 - val_loss: 0.0391 - val_f1_m: 0.9922\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0984 - f1_m: 0.9688 - val_loss: 0.0491 - val_f1_m: 0.9922\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0964 - f1_m: 0.9688 - val_loss: 0.0353 - val_f1_m: 0.9922\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0927 - f1_m: 0.9688 - val_loss: 0.0468 - val_f1_m: 0.9922\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0912 - f1_m: 0.9688 - val_loss: 0.0439 - val_f1_m: 0.9922\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0877 - f1_m: 0.9663 - val_loss: 0.0374 - val_f1_m: 0.9922\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0843 - f1_m: 0.9736 - val_loss: 0.0474 - val_f1_m: 0.9922\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0829 - f1_m: 0.9712 - val_loss: 0.0390 - val_f1_m: 0.9922\n",
      "0.9711538553237915\n",
      "16/16 [==============================] - 0s 678us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 0.4049 - f1_m: 0.9255 - val_loss: 0.1368 - val_f1_m: 0.9922\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1866 - f1_m: 0.9615 - val_loss: 0.0730 - val_f1_m: 0.9922\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1655 - f1_m: 0.9591 - val_loss: 0.0613 - val_f1_m: 0.9922\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1513 - f1_m: 0.9591 - val_loss: 0.0650 - val_f1_m: 0.9922\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1405 - f1_m: 0.9615 - val_loss: 0.0676 - val_f1_m: 0.9922\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1289 - f1_m: 0.9615 - val_loss: 0.0574 - val_f1_m: 0.9922\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1252 - f1_m: 0.9615 - val_loss: 0.0603 - val_f1_m: 0.9922\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1173 - f1_m: 0.9639 - val_loss: 0.0580 - val_f1_m: 0.9922\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1127 - f1_m: 0.9639 - val_loss: 0.0554 - val_f1_m: 0.9922\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1084 - f1_m: 0.9688 - val_loss: 0.0611 - val_f1_m: 0.9844\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1035 - f1_m: 0.9688 - val_loss: 0.0574 - val_f1_m: 0.9922\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1014 - f1_m: 0.9712 - val_loss: 0.0597 - val_f1_m: 0.9844\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0979 - f1_m: 0.9760 - val_loss: 0.0641 - val_f1_m: 0.9844\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0932 - f1_m: 0.9760 - val_loss: 0.0590 - val_f1_m: 0.9922\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0909 - f1_m: 0.9712 - val_loss: 0.0576 - val_f1_m: 0.9922\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0881 - f1_m: 0.9712 - val_loss: 0.0666 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0850 - f1_m: 0.9760 - val_loss: 0.0576 - val_f1_m: 0.9844\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0843 - f1_m: 0.9760 - val_loss: 0.0579 - val_f1_m: 0.9844\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0800 - f1_m: 0.9760 - val_loss: 0.0652 - val_f1_m: 0.9766\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0803 - f1_m: 0.9760 - val_loss: 0.0627 - val_f1_m: 0.9844\n",
      "0.9759615659713745\n",
      "16/16 [==============================] - 0s 633us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 0.4158 - f1_m: 0.9159 - val_loss: 0.1527 - val_f1_m: 0.9922\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2148 - f1_m: 0.9591 - val_loss: 0.0924 - val_f1_m: 0.9922\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1841 - f1_m: 0.9615 - val_loss: 0.0911 - val_f1_m: 0.9922\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1649 - f1_m: 0.9591 - val_loss: 0.0951 - val_f1_m: 0.9922\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1506 - f1_m: 0.9591 - val_loss: 0.0927 - val_f1_m: 0.9922\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1357 - f1_m: 0.9615 - val_loss: 0.0858 - val_f1_m: 0.9922\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1239 - f1_m: 0.9591 - val_loss: 0.0807 - val_f1_m: 0.9922\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1120 - f1_m: 0.9615 - val_loss: 0.0831 - val_f1_m: 0.9922\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1055 - f1_m: 0.9688 - val_loss: 0.0830 - val_f1_m: 0.9922\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0962 - f1_m: 0.9688 - val_loss: 0.0768 - val_f1_m: 0.9922\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0919 - f1_m: 0.9688 - val_loss: 0.0755 - val_f1_m: 0.9922\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0863 - f1_m: 0.9688 - val_loss: 0.0706 - val_f1_m: 0.9922\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0828 - f1_m: 0.9712 - val_loss: 0.0748 - val_f1_m: 0.9922\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0802 - f1_m: 0.9712 - val_loss: 0.0685 - val_f1_m: 0.9922\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0756 - f1_m: 0.9760 - val_loss: 0.0680 - val_f1_m: 0.9922\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0748 - f1_m: 0.9760 - val_loss: 0.0680 - val_f1_m: 0.9922\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0728 - f1_m: 0.9760 - val_loss: 0.0671 - val_f1_m: 0.9922\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0693 - f1_m: 0.9784 - val_loss: 0.0710 - val_f1_m: 0.9844\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0688 - f1_m: 0.9784 - val_loss: 0.0682 - val_f1_m: 0.9922\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0678 - f1_m: 0.9736 - val_loss: 0.0659 - val_f1_m: 0.9922\n",
      "0.973557710647583\n",
      "16/16 [==============================] - 0s 568us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 10ms/step - loss: 0.4032 - f1_m: 0.9087 - val_loss: 0.1451 - val_f1_m: 0.9922\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2035 - f1_m: 0.9591 - val_loss: 0.0814 - val_f1_m: 0.9922\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1871 - f1_m: 0.9615 - val_loss: 0.0777 - val_f1_m: 0.9922\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1718 - f1_m: 0.9615 - val_loss: 0.0799 - val_f1_m: 0.9922\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1592 - f1_m: 0.9615 - val_loss: 0.0861 - val_f1_m: 0.9922\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1529 - f1_m: 0.9615 - val_loss: 0.0872 - val_f1_m: 0.9922\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1409 - f1_m: 0.9615 - val_loss: 0.0774 - val_f1_m: 0.9922\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1359 - f1_m: 0.9591 - val_loss: 0.0741 - val_f1_m: 0.9922\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1311 - f1_m: 0.9615 - val_loss: 0.0772 - val_f1_m: 0.9922\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1255 - f1_m: 0.9639 - val_loss: 0.0707 - val_f1_m: 0.9922\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1237 - f1_m: 0.9615 - val_loss: 0.0654 - val_f1_m: 0.9922\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1192 - f1_m: 0.9615 - val_loss: 0.0631 - val_f1_m: 0.9922\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1162 - f1_m: 0.9615 - val_loss: 0.0613 - val_f1_m: 0.9922\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1131 - f1_m: 0.9639 - val_loss: 0.0653 - val_f1_m: 0.9922\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1128 - f1_m: 0.9639 - val_loss: 0.0622 - val_f1_m: 0.9922\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1086 - f1_m: 0.9639 - val_loss: 0.0611 - val_f1_m: 0.9922\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1091 - f1_m: 0.9615 - val_loss: 0.0550 - val_f1_m: 0.9922\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1057 - f1_m: 0.9688 - val_loss: 0.0608 - val_f1_m: 0.9922\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1046 - f1_m: 0.9615 - val_loss: 0.0542 - val_f1_m: 0.9922\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1002 - f1_m: 0.9688 - val_loss: 0.0599 - val_f1_m: 0.9922\n",
      "0.96875\n",
      "16/16 [==============================] - 0s 674us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 0.4003 - f1_m: 0.9111 - val_loss: 0.1792 - val_f1_m: 0.9844\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2039 - f1_m: 0.9639 - val_loss: 0.1253 - val_f1_m: 0.9844\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1789 - f1_m: 0.9615 - val_loss: 0.1193 - val_f1_m: 0.9844\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1601 - f1_m: 0.9639 - val_loss: 0.1178 - val_f1_m: 0.9844\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1402 - f1_m: 0.9615 - val_loss: 0.1165 - val_f1_m: 0.9844\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1254 - f1_m: 0.9615 - val_loss: 0.1116 - val_f1_m: 0.9844\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1141 - f1_m: 0.9663 - val_loss: 0.1053 - val_f1_m: 0.9844\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0993 - f1_m: 0.9663 - val_loss: 0.1048 - val_f1_m: 0.9844\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0903 - f1_m: 0.9663 - val_loss: 0.0968 - val_f1_m: 0.9844\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0822 - f1_m: 0.9712 - val_loss: 0.1023 - val_f1_m: 0.9844\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0790 - f1_m: 0.9760 - val_loss: 0.1048 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0704 - f1_m: 0.9760 - val_loss: 0.1016 - val_f1_m: 0.9844\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0695 - f1_m: 0.9784 - val_loss: 0.1052 - val_f1_m: 0.9844\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0646 - f1_m: 0.9784 - val_loss: 0.1104 - val_f1_m: 0.9688\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0611 - f1_m: 0.9832 - val_loss: 0.1023 - val_f1_m: 0.9844\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0629 - f1_m: 0.9808 - val_loss: 0.1097 - val_f1_m: 0.9844\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0563 - f1_m: 0.9808 - val_loss: 0.1099 - val_f1_m: 0.9688\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0553 - f1_m: 0.9856 - val_loss: 0.1080 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0538 - f1_m: 0.9832 - val_loss: 0.1081 - val_f1_m: 0.9844\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0515 - f1_m: 0.9760 - val_loss: 0.1142 - val_f1_m: 0.9688\n",
      "0.9759615659713745\n",
      "16/16 [==============================] - 0s 655us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 0.4102 - f1_m: 0.9183 - val_loss: 0.1732 - val_f1_m: 0.9844\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2147 - f1_m: 0.9591 - val_loss: 0.1250 - val_f1_m: 0.9844\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1831 - f1_m: 0.9639 - val_loss: 0.1099 - val_f1_m: 0.9844\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1600 - f1_m: 0.9639 - val_loss: 0.0957 - val_f1_m: 0.9844\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1378 - f1_m: 0.9639 - val_loss: 0.0842 - val_f1_m: 0.9844\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1215 - f1_m: 0.9663 - val_loss: 0.0723 - val_f1_m: 0.9844\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1109 - f1_m: 0.9663 - val_loss: 0.0598 - val_f1_m: 0.9844\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1003 - f1_m: 0.9639 - val_loss: 0.0504 - val_f1_m: 0.9844\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0926 - f1_m: 0.9663 - val_loss: 0.0457 - val_f1_m: 1.0000\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0870 - f1_m: 0.9784 - val_loss: 0.0498 - val_f1_m: 1.0000\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0845 - f1_m: 0.9736 - val_loss: 0.0361 - val_f1_m: 1.0000\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0779 - f1_m: 0.9736 - val_loss: 0.0400 - val_f1_m: 1.0000\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0743 - f1_m: 0.9832 - val_loss: 0.0404 - val_f1_m: 1.0000\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0710 - f1_m: 0.9808 - val_loss: 0.0323 - val_f1_m: 1.0000\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0683 - f1_m: 0.9832 - val_loss: 0.0357 - val_f1_m: 1.0000\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0675 - f1_m: 0.9808 - val_loss: 0.0307 - val_f1_m: 1.0000\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0672 - f1_m: 0.9832 - val_loss: 0.0379 - val_f1_m: 1.0000\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0635 - f1_m: 0.9832 - val_loss: 0.0267 - val_f1_m: 1.0000\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0604 - f1_m: 0.9832 - val_loss: 0.0309 - val_f1_m: 1.0000\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0595 - f1_m: 0.9856 - val_loss: 0.0354 - val_f1_m: 1.0000\n",
      "0.9855769276618958\n",
      "16/16 [==============================] - 0s 584us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 0.3898 - f1_m: 0.9279 - val_loss: 0.2408 - val_f1_m: 0.9687\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1724 - f1_m: 0.9663 - val_loss: 0.2219 - val_f1_m: 0.9687\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1400 - f1_m: 0.9663 - val_loss: 0.2167 - val_f1_m: 0.9687\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1217 - f1_m: 0.9688 - val_loss: 0.2067 - val_f1_m: 0.9687\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1122 - f1_m: 0.9688 - val_loss: 0.2004 - val_f1_m: 0.9687\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1055 - f1_m: 0.9663 - val_loss: 0.1980 - val_f1_m: 0.9687\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0992 - f1_m: 0.9663 - val_loss: 0.1913 - val_f1_m: 0.9687\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0952 - f1_m: 0.9712 - val_loss: 0.1860 - val_f1_m: 0.9687\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0931 - f1_m: 0.9712 - val_loss: 0.1860 - val_f1_m: 0.9687\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0897 - f1_m: 0.9712 - val_loss: 0.1798 - val_f1_m: 0.9687\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0869 - f1_m: 0.9736 - val_loss: 0.1765 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0846 - f1_m: 0.9736 - val_loss: 0.1767 - val_f1_m: 0.9766\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0829 - f1_m: 0.9736 - val_loss: 0.1768 - val_f1_m: 0.9766\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0811 - f1_m: 0.9760 - val_loss: 0.1705 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0798 - f1_m: 0.9712 - val_loss: 0.1712 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0773 - f1_m: 0.9784 - val_loss: 0.1711 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0755 - f1_m: 0.9736 - val_loss: 0.1705 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0741 - f1_m: 0.9784 - val_loss: 0.1729 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0737 - f1_m: 0.9760 - val_loss: 0.1729 - val_f1_m: 0.9766\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0718 - f1_m: 0.9760 - val_loss: 0.1715 - val_f1_m: 0.9766\n",
      "0.9759615659713745\n",
      "16/16 [==============================] - 0s 611us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 0.3957 - f1_m: 0.9351 - val_loss: 0.1890 - val_f1_m: 0.9219\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1970 - f1_m: 0.9663 - val_loss: 0.1624 - val_f1_m: 0.9219\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1776 - f1_m: 0.9663 - val_loss: 0.1537 - val_f1_m: 0.9219\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1576 - f1_m: 0.9663 - val_loss: 0.1436 - val_f1_m: 0.9219\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1389 - f1_m: 0.9663 - val_loss: 0.1358 - val_f1_m: 0.9219\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1235 - f1_m: 0.9688 - val_loss: 0.1199 - val_f1_m: 0.9219\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1129 - f1_m: 0.9663 - val_loss: 0.1086 - val_f1_m: 0.9219\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1018 - f1_m: 0.9688 - val_loss: 0.1039 - val_f1_m: 0.9219\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0941 - f1_m: 0.9712 - val_loss: 0.0897 - val_f1_m: 0.9297\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0876 - f1_m: 0.9712 - val_loss: 0.0871 - val_f1_m: 0.9219\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0824 - f1_m: 0.9736 - val_loss: 0.0834 - val_f1_m: 0.9219\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0776 - f1_m: 0.9712 - val_loss: 0.0831 - val_f1_m: 0.9219\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0761 - f1_m: 0.9760 - val_loss: 0.0806 - val_f1_m: 0.9219\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0720 - f1_m: 0.9736 - val_loss: 0.0801 - val_f1_m: 0.9141\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0694 - f1_m: 0.9736 - val_loss: 0.0759 - val_f1_m: 0.9141\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0682 - f1_m: 0.9736 - val_loss: 0.0777 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0687 - f1_m: 0.9808 - val_loss: 0.0754 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0629 - f1_m: 0.9784 - val_loss: 0.0691 - val_f1_m: 0.9141\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0626 - f1_m: 0.9760 - val_loss: 0.0719 - val_f1_m: 0.9141\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0610 - f1_m: 0.9784 - val_loss: 0.0668 - val_f1_m: 0.9219\n",
      "0.9783653616905212\n",
      "16/16 [==============================] - 0s 600us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 0.3845 - f1_m: 0.9375 - val_loss: 0.2043 - val_f1_m: 0.9766\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1827 - f1_m: 0.9663 - val_loss: 0.1482 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1633 - f1_m: 0.9663 - val_loss: 0.1235 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1459 - f1_m: 0.9663 - val_loss: 0.1110 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1327 - f1_m: 0.9663 - val_loss: 0.0966 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1230 - f1_m: 0.9663 - val_loss: 0.0928 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1130 - f1_m: 0.9663 - val_loss: 0.0859 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1070 - f1_m: 0.9663 - val_loss: 0.0819 - val_f1_m: 0.9688\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1003 - f1_m: 0.9663 - val_loss: 0.0811 - val_f1_m: 0.9688\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0935 - f1_m: 0.9663 - val_loss: 0.0750 - val_f1_m: 0.9688\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0875 - f1_m: 0.9712 - val_loss: 0.0736 - val_f1_m: 0.9688\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0833 - f1_m: 0.9688 - val_loss: 0.0715 - val_f1_m: 0.9688\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0766 - f1_m: 0.9688 - val_loss: 0.0687 - val_f1_m: 0.9688\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0757 - f1_m: 0.9688 - val_loss: 0.0665 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0716 - f1_m: 0.9736 - val_loss: 0.0610 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0695 - f1_m: 0.9736 - val_loss: 0.0678 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0669 - f1_m: 0.9712 - val_loss: 0.0627 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0630 - f1_m: 0.9760 - val_loss: 0.0606 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0625 - f1_m: 0.9760 - val_loss: 0.0599 - val_f1_m: 0.9766\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0603 - f1_m: 0.9760 - val_loss: 0.0644 - val_f1_m: 0.9844\n",
      "0.9759615659713745\n",
      "16/16 [==============================] - 0s 583us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 10ms/step - loss: 0.3696 - f1_m: 0.9471 - val_loss: 0.2971 - val_f1_m: 0.8984\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1601 - f1_m: 0.9736 - val_loss: 0.2983 - val_f1_m: 0.8984\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1374 - f1_m: 0.9712 - val_loss: 0.2888 - val_f1_m: 0.8984\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1186 - f1_m: 0.9712 - val_loss: 0.2503 - val_f1_m: 0.8984\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1035 - f1_m: 0.9712 - val_loss: 0.2326 - val_f1_m: 0.8984\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0954 - f1_m: 0.9736 - val_loss: 0.2150 - val_f1_m: 0.8984\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0870 - f1_m: 0.9712 - val_loss: 0.2110 - val_f1_m: 0.8984\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0803 - f1_m: 0.9784 - val_loss: 0.1921 - val_f1_m: 0.8984\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0729 - f1_m: 0.9736 - val_loss: 0.1795 - val_f1_m: 0.8984\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0686 - f1_m: 0.9760 - val_loss: 0.1847 - val_f1_m: 0.8984\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0631 - f1_m: 0.9712 - val_loss: 0.1581 - val_f1_m: 0.8984\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0579 - f1_m: 0.9808 - val_loss: 0.1504 - val_f1_m: 0.8984\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0538 - f1_m: 0.9832 - val_loss: 0.1526 - val_f1_m: 0.8984\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0520 - f1_m: 0.9808 - val_loss: 0.1488 - val_f1_m: 0.8984\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0509 - f1_m: 0.9808 - val_loss: 0.1477 - val_f1_m: 0.8984\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0481 - f1_m: 0.9856 - val_loss: 0.1372 - val_f1_m: 0.8984\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0476 - f1_m: 0.9856 - val_loss: 0.1466 - val_f1_m: 0.8984\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0454 - f1_m: 0.9856 - val_loss: 0.1370 - val_f1_m: 0.8984\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0450 - f1_m: 0.9856 - val_loss: 0.1357 - val_f1_m: 0.8984\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0429 - f1_m: 0.9856 - val_loss: 0.1420 - val_f1_m: 0.8984\n",
      "0.9855769276618958\n",
      "16/16 [==============================] - 0s 636us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 0.3862 - f1_m: 0.9351 - val_loss: 0.1880 - val_f1_m: 0.9766\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1888 - f1_m: 0.9639 - val_loss: 0.1403 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1711 - f1_m: 0.9663 - val_loss: 0.1252 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1593 - f1_m: 0.9663 - val_loss: 0.1188 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1496 - f1_m: 0.9663 - val_loss: 0.1176 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1425 - f1_m: 0.9663 - val_loss: 0.1147 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1357 - f1_m: 0.9663 - val_loss: 0.1106 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1293 - f1_m: 0.9663 - val_loss: 0.1095 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1249 - f1_m: 0.9639 - val_loss: 0.1088 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1184 - f1_m: 0.9663 - val_loss: 0.1101 - val_f1_m: 0.9766\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1147 - f1_m: 0.9663 - val_loss: 0.1063 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1097 - f1_m: 0.9663 - val_loss: 0.1080 - val_f1_m: 0.9766\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1049 - f1_m: 0.9639 - val_loss: 0.1074 - val_f1_m: 0.9766\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1044 - f1_m: 0.9663 - val_loss: 0.1081 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0990 - f1_m: 0.9639 - val_loss: 0.1077 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0953 - f1_m: 0.9663 - val_loss: 0.1063 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0959 - f1_m: 0.9712 - val_loss: 0.1006 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0939 - f1_m: 0.9688 - val_loss: 0.1057 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0903 - f1_m: 0.9688 - val_loss: 0.1140 - val_f1_m: 0.9766\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0898 - f1_m: 0.9688 - val_loss: 0.1076 - val_f1_m: 0.9766\n",
      "0.96875\n",
      "16/16 [==============================] - 0s 573us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 10ms/step - loss: 0.3867 - f1_m: 0.9375 - val_loss: 0.2140 - val_f1_m: 0.9141\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1565 - f1_m: 0.9663 - val_loss: 0.1654 - val_f1_m: 0.9141\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1284 - f1_m: 0.9688 - val_loss: 0.1564 - val_f1_m: 0.9141\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1155 - f1_m: 0.9687 - val_loss: 0.1512 - val_f1_m: 0.9141\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1058 - f1_m: 0.9663 - val_loss: 0.1489 - val_f1_m: 0.9141\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0993 - f1_m: 0.9687 - val_loss: 0.1490 - val_f1_m: 0.9141\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0938 - f1_m: 0.9736 - val_loss: 0.1483 - val_f1_m: 0.9141\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0938 - f1_m: 0.9663 - val_loss: 0.1492 - val_f1_m: 0.9141\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0861 - f1_m: 0.9712 - val_loss: 0.1504 - val_f1_m: 0.9141\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0834 - f1_m: 0.9736 - val_loss: 0.1526 - val_f1_m: 0.9141\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0816 - f1_m: 0.9712 - val_loss: 0.1529 - val_f1_m: 0.9141\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0776 - f1_m: 0.9760 - val_loss: 0.1539 - val_f1_m: 0.9219\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0736 - f1_m: 0.9760 - val_loss: 0.1548 - val_f1_m: 0.9219\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0733 - f1_m: 0.9688 - val_loss: 0.1557 - val_f1_m: 0.9141\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0704 - f1_m: 0.9784 - val_loss: 0.1587 - val_f1_m: 0.9219\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0693 - f1_m: 0.9760 - val_loss: 0.1591 - val_f1_m: 0.9141\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0653 - f1_m: 0.9760 - val_loss: 0.1623 - val_f1_m: 0.9219\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0629 - f1_m: 0.9760 - val_loss: 0.1639 - val_f1_m: 0.9219\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0628 - f1_m: 0.9784 - val_loss: 0.1657 - val_f1_m: 0.9219\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0595 - f1_m: 0.9760 - val_loss: 0.1674 - val_f1_m: 0.9219\n",
      "0.9759615659713745\n",
      "16/16 [==============================] - 0s 569us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 0.3869 - f1_m: 0.9327 - val_loss: 0.2182 - val_f1_m: 0.9766\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1859 - f1_m: 0.9639 - val_loss: 0.1905 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1605 - f1_m: 0.9615 - val_loss: 0.1819 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1443 - f1_m: 0.9663 - val_loss: 0.1744 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1306 - f1_m: 0.9639 - val_loss: 0.1637 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1223 - f1_m: 0.9663 - val_loss: 0.1608 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1105 - f1_m: 0.9663 - val_loss: 0.1490 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1045 - f1_m: 0.9663 - val_loss: 0.1384 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0968 - f1_m: 0.9688 - val_loss: 0.1180 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0897 - f1_m: 0.9663 - val_loss: 0.1096 - val_f1_m: 0.9766\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0840 - f1_m: 0.9736 - val_loss: 0.0942 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0785 - f1_m: 0.9736 - val_loss: 0.0886 - val_f1_m: 0.9766\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0751 - f1_m: 0.9736 - val_loss: 0.0815 - val_f1_m: 0.9766\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0705 - f1_m: 0.9760 - val_loss: 0.0786 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0704 - f1_m: 0.9736 - val_loss: 0.0726 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0673 - f1_m: 0.9784 - val_loss: 0.0728 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0646 - f1_m: 0.9808 - val_loss: 0.0691 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0625 - f1_m: 0.9784 - val_loss: 0.0673 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0636 - f1_m: 0.9832 - val_loss: 0.0698 - val_f1_m: 0.9687\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0583 - f1_m: 0.9856 - val_loss: 0.0675 - val_f1_m: 0.9766\n",
      "0.9855769276618958\n",
      "16/16 [==============================] - 0s 572us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 0.3808 - f1_m: 0.9303 - val_loss: 0.1794 - val_f1_m: 0.9766\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1655 - f1_m: 0.9688 - val_loss: 0.1424 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1402 - f1_m: 0.9633 - val_loss: 0.1499 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1256 - f1_m: 0.9688 - val_loss: 0.1522 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1135 - f1_m: 0.9660 - val_loss: 0.1594 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1073 - f1_m: 0.9688 - val_loss: 0.1656 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1013 - f1_m: 0.9688 - val_loss: 0.1697 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0969 - f1_m: 0.9712 - val_loss: 0.1744 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0935 - f1_m: 0.9684 - val_loss: 0.1759 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0908 - f1_m: 0.9712 - val_loss: 0.1882 - val_f1_m: 0.9687\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0872 - f1_m: 0.9684 - val_loss: 0.1894 - val_f1_m: 0.9687\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0844 - f1_m: 0.9684 - val_loss: 0.1880 - val_f1_m: 0.9766\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0820 - f1_m: 0.9712 - val_loss: 0.1922 - val_f1_m: 0.9687\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0803 - f1_m: 0.9736 - val_loss: 0.1952 - val_f1_m: 0.9687\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0777 - f1_m: 0.9736 - val_loss: 0.1949 - val_f1_m: 0.9687\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0765 - f1_m: 0.9712 - val_loss: 0.1957 - val_f1_m: 0.9687\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0747 - f1_m: 0.9708 - val_loss: 0.1997 - val_f1_m: 0.9687\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0718 - f1_m: 0.9760 - val_loss: 0.2063 - val_f1_m: 0.9687\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0708 - f1_m: 0.9784 - val_loss: 0.2040 - val_f1_m: 0.9687\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0693 - f1_m: 0.9736 - val_loss: 0.2070 - val_f1_m: 0.9687\n",
      "0.973557710647583\n",
      "16/16 [==============================] - 0s 628us/step\n",
      "if any\n",
      "19\n",
      "All Done\n"
     ]
    }
   ],
   "source": [
    "#now trying for aNN-2 get-initial_model2\n",
    "#the below code is for binary classification since MNSIT is a multi-class low-resolution image dataset. I am updating the code in the next cell.\n",
    "N=500\n",
    "#Positive=Pos\n",
    "#Negative=Neg\n",
    "positiveN=int((Positive.shape[0]/dataset.shape[0])*N)\n",
    "negativeN=int(N-positiveN)\n",
    "print(positiveN, negativeN)\n",
    "#target variable\n",
    "#target_variable=\"default.payment.next.month\"\n",
    "df1=Positive.sample(positiveN)\n",
    "Positive.drop(df1.index, inplace=True)\n",
    "df2=Negative.sample(negativeN)\n",
    "Negative.drop(df2.index, inplace=True)\n",
    "test_data=df1.append(df2, ignore_index=True)\n",
    "test_data=test_data.sample(frac = 1) #This is to shuffel the training and testing data\n",
    "test_data=test_data.sample(frac = 1)\n",
    "test_data=test_data.sample(frac = 1)\n",
    "X_test=test_data.drop(columns=[target_variable])\n",
    "y_test=to_categorical(test_data[target_variable])\n",
    "\n",
    "# adding dense layer\n",
    "initial_model= get_initial_model_2(X_test.shape[1], 2)\n",
    "initial_model.set_weights(update_weights(initial_model.get_weights()))\n",
    "Models=[]\n",
    "val_acc=[]\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "val_loss=[]\n",
    "train_loss=[]\n",
    "add_weights=[]\n",
    "while Positive.empty==False and Negative.empty==False:\n",
    "  print(positiveN, negativeN)\n",
    "  df1=Positive.sample(min(positiveN, len(Positive)))\n",
    "  Positive.drop(df1.index, inplace=True)\n",
    "  df2=Negative.sample(min(negativeN, len(Negative)))\n",
    "  Negative.drop(df2.index, inplace=True)\n",
    "  train_data=df1.append(df2, ignore_index=True)\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "    \n",
    "  #all models have different initialization\n",
    "  # define the sequential model\n",
    "  \"\"\"initial_model = keras.Sequential()\n",
    "\n",
    "    # adding dense layer\n",
    "  initial_model.add(Dense(5, input_dim=X_test.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "  initial_model.add(Dense(10, activation='relu'))\n",
    "  initial_model.add(Dense(5, activation='relu'))\n",
    "\n",
    "    # adding dense layer with softmax activation/output layer\n",
    "  initial_model.add(Dense(2, activation='softmax'))\n",
    "  #initial_model.summary()\"\"\"\n",
    "  ann_model=get_initial_model_2(X_test.shape[1], 2) #same intial weights\n",
    "  ann_model.set_weights(initial_model.get_weights())\n",
    "  X_train=train_data.drop(columns=[target_variable])\n",
    "  #train_data[target_variable]=train_data[target_variable]-1 #only for skin_nonskin dataset\n",
    "  y_train=to_categorical(train_data[target_variable])\n",
    "  #print(y_train)\n",
    "  ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_m]) # metrics=['accuracy']\n",
    "  history = ann_model.fit(X_train, y_train, epochs=20, validation_split=0.2, verbose=1)\n",
    "  print(history.history['f1_m'][-1])\n",
    "  ann_model.set_weights(update_weights(ann_model.get_weights()))\n",
    "  pred_test=ann_model.predict(X_test)\n",
    "  present=False\n",
    "  for i in range(len(Models)):\n",
    "    if (check_models(Models[i][0], ann_model.get_weights())):\n",
    "      print(\"if any\")\n",
    "      Models[i][1]=Models[i][1]+1\n",
    "      add_weights[i].append(ann_model.get_weights())\n",
    "      val_acc[i].append(history.history['val_f1_m'])\n",
    "      train_acc[i].append(history.history['f1_m'])\n",
    "      test_acc[i].append(f1_m(y_test, pred_test).numpy())\n",
    "      val_loss[i].append(history.history['val_loss'])\n",
    "      train_loss[i].append(history.history['loss'])\n",
    "      present=True\n",
    "      break;\n",
    "  if present==False:\n",
    "    add_weights.append([ann_model.get_weights()])\n",
    "    Models.append([ann_model.get_weights(), 1])\n",
    "    val_acc.append([history.history['val_f1_m']])\n",
    "    train_acc.append([history.history['f1_m']])\n",
    "    test_acc.append([f1_m(y_test, pred_test).numpy()])\n",
    "    val_loss.append([history.history['val_loss']])\n",
    "    train_loss.append([history.history['loss']])\n",
    "for i in range(len(Models)):\n",
    "  print(Models[i][1])\n",
    "print(\"All Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "1\n",
      "[[0.96799994, 0.96799994, 0.97199994, 0.96999997, 0.96599996, 0.9779999, 0.96799994, 0.96799994, 0.96199995, 0.96999997, 0.96599996, 0.96599996, 0.96799994, 0.9739999, 0.97199994, 0.96799994, 0.97199994, 0.96999997, 0.96799994]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Models)):\n",
    "    print(Models[i][1])\n",
    "print(len(Models))\n",
    "#test_Acc = [i.numpy() for i in test_acc]\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#not needed in this case\n",
    "#this works for getting sorted recurrent models by frequency no\n",
    "A=np.argsort(np.array(Models).T[1])[::-1][:2]\n",
    "print(A)\n",
    "temp=list(np.array(Models)[A])\n",
    "#print(temp[2][1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(len(add_weights[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.07  , -0.03  , -0.04  , ..., -0.02  ,  0.05  , -0.09  ],\n",
      "       [ 0.06  , -0.03  ,  0.0005, ...,  0.05  ,  0.004 ,  0.01  ],\n",
      "       [-0.03  , -0.03  , -0.06  , ..., -0.13  , -0.09  ,  0.03  ],\n",
      "       ...,\n",
      "       [-0.01  ,  0.01  ,  0.1   , ..., -0.03  ,  0.1   ,  0.06  ],\n",
      "       [ 0.07  ,  0.12  , -0.13  , ..., -0.07  , -0.02  , -0.06  ],\n",
      "       [-0.02  ,  0.1   ,  0.1   , ..., -0.03  , -0.19  ,  0.07  ]],\n",
      "      dtype=float32), array([ 0.07,  0.05,  0.01, ..., -0.02, -0.05,  0.03], dtype=float32), array([[ 0.1 , -0.11],\n",
      "       [ 0.06, -0.04],\n",
      "       [ 0.08, -0.01],\n",
      "       ...,\n",
      "       [-0.11,  0.05],\n",
      "       [-0.07,  0.05],\n",
      "       [ 0.06, -0.01]], dtype=float32), array([ 0.03, -0.03], dtype=float32)]\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "yhn tk\n",
      "19\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "mean_model_weights=[]\n",
    "for i in range(1):\n",
    "    mean_model_weights.append(get_avg_weights_2(add_weights[i],X_test.shape[1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 868us/step - loss: 0.1042 - accuracy: 0.9723\n",
      "[0.10420272499322891, 0.9723333120346069]\n"
     ]
    }
   ],
   "source": [
    "#mean models\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = to_categorical(dataset[target_variable])\n",
    "X = dataset.drop(columns=target_variable)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "init_model=get_initial_model_2(X_test.shape[1], 2)\n",
    "init_model.set_weights(mean_model_weights[0])\n",
    "\n",
    "print(init_model.evaluate(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Type', 'Air temperature [K]', 'Process temperature [K]',\n",
      "       'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]',\n",
      "       'Machine failure'],\n",
      "      dtype='object')\n",
      "Index(['Air temperature [K]', 'Process temperature [K]',\n",
      "       'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]', 'Type_H',\n",
      "       'Type_L', 'Type_M', 'Machine failure'],\n",
      "      dtype='object')\n",
      "9661\n",
      "339\n"
     ]
    }
   ],
   "source": [
    "#for SKIN_NonSkin dataset\n",
    "df = pd.read_csv(\"ai4i2020.csv\",sep=';')\n",
    "#dataset.round(3)\n",
    "#print(dataset['label'])\n",
    "df_str = df.select_dtypes(include='object')\n",
    "df_int = df.select_dtypes(exclude='object')\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "#handle categorical data\n",
    "df_str = pd.get_dummies(df_str)\n",
    "\n",
    "target_variable=\"Machine failure\"\n",
    "target = df_int[target_variable]\n",
    "x = df_int.drop(columns=target_variable)\n",
    "column_names = x.columns.values\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x_stndrd = scaler.fit_transform(x)\n",
    "type(x_stndrd)\n",
    "\n",
    "x_stndrd = pd.DataFrame(x_stndrd)\n",
    "x_stndrd.columns = column_names\n",
    "\n",
    "dataset = pd.concat([x_stndrd,df_str, target],axis=1)\n",
    "print(dataset.columns)\n",
    "\n",
    "Positive=dataset[dataset[target_variable]==0]\n",
    "Negative=dataset[dataset[target_variable]==1]\n",
    "print(len(Positive))\n",
    "print(len(Negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483 17\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5735 - f1_m: 0.9736 - val_loss: 0.5280 - val_f1_m: 0.8437\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4577 - f1_m: 0.9712 - val_loss: 0.4255 - val_f1_m: 0.8437\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3416 - f1_m: 0.9736 - val_loss: 0.3374 - val_f1_m: 0.8437\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2475 - f1_m: 0.9712 - val_loss: 0.2830 - val_f1_m: 0.8437\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1887 - f1_m: 0.9712 - val_loss: 0.2613 - val_f1_m: 0.8437\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1591 - f1_m: 0.9736 - val_loss: 0.2560 - val_f1_m: 0.8437\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1449 - f1_m: 0.9712 - val_loss: 0.2568 - val_f1_m: 0.8437\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1375 - f1_m: 0.9712 - val_loss: 0.2546 - val_f1_m: 0.8437\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1325 - f1_m: 0.9736 - val_loss: 0.2515 - val_f1_m: 0.8437\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1283 - f1_m: 0.9736 - val_loss: 0.2493 - val_f1_m: 0.8437\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1253 - f1_m: 0.9736 - val_loss: 0.2482 - val_f1_m: 0.8437\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1218 - f1_m: 0.9736 - val_loss: 0.2446 - val_f1_m: 0.8437\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1190 - f1_m: 0.9736 - val_loss: 0.2400 - val_f1_m: 0.8437\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1160 - f1_m: 0.9736 - val_loss: 0.2373 - val_f1_m: 0.8437\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1145 - f1_m: 0.9712 - val_loss: 0.2338 - val_f1_m: 0.8437\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1117 - f1_m: 0.9712 - val_loss: 0.2343 - val_f1_m: 0.8437\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1096 - f1_m: 0.9712 - val_loss: 0.2324 - val_f1_m: 0.8437\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1079 - f1_m: 0.9736 - val_loss: 0.2321 - val_f1_m: 0.8437\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1059 - f1_m: 0.9736 - val_loss: 0.2319 - val_f1_m: 0.8437\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1042 - f1_m: 0.9712 - val_loss: 0.2324 - val_f1_m: 0.8437\n",
      "0.9711538553237915\n",
      "16/16 [==============================] - 0s 937us/step\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5785 - f1_m: 0.9663 - val_loss: 0.5084 - val_f1_m: 0.9766\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4617 - f1_m: 0.9663 - val_loss: 0.3889 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3472 - f1_m: 0.9663 - val_loss: 0.2775 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2553 - f1_m: 0.9639 - val_loss: 0.2004 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1955 - f1_m: 0.9663 - val_loss: 0.1633 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1655 - f1_m: 0.9639 - val_loss: 0.1479 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1529 - f1_m: 0.9639 - val_loss: 0.1424 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1455 - f1_m: 0.9639 - val_loss: 0.1402 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1409 - f1_m: 0.9663 - val_loss: 0.1407 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1370 - f1_m: 0.9663 - val_loss: 0.1406 - val_f1_m: 0.9766\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1336 - f1_m: 0.9615 - val_loss: 0.1409 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1312 - f1_m: 0.9663 - val_loss: 0.1415 - val_f1_m: 0.9766\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1283 - f1_m: 0.9639 - val_loss: 0.1411 - val_f1_m: 0.9766\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1262 - f1_m: 0.9639 - val_loss: 0.1416 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1237 - f1_m: 0.9663 - val_loss: 0.1414 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1220 - f1_m: 0.9663 - val_loss: 0.1413 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1200 - f1_m: 0.9663 - val_loss: 0.1418 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1185 - f1_m: 0.9615 - val_loss: 0.1426 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1162 - f1_m: 0.9663 - val_loss: 0.1415 - val_f1_m: 0.9766\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1145 - f1_m: 0.9639 - val_loss: 0.1421 - val_f1_m: 0.9766\n",
      "0.963942289352417\n",
      "16/16 [==============================] - 0s 895us/step\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5794 - f1_m: 0.9639 - val_loss: 0.5112 - val_f1_m: 0.9766\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4675 - f1_m: 0.9663 - val_loss: 0.3938 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3552 - f1_m: 0.9663 - val_loss: 0.2838 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2606 - f1_m: 0.9663 - val_loss: 0.2050 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2013 - f1_m: 0.9639 - val_loss: 0.1623 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1716 - f1_m: 0.9639 - val_loss: 0.1438 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1568 - f1_m: 0.9591 - val_loss: 0.1349 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1489 - f1_m: 0.9663 - val_loss: 0.1294 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1442 - f1_m: 0.9639 - val_loss: 0.1253 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1398 - f1_m: 0.9663 - val_loss: 0.1215 - val_f1_m: 0.9766\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1351 - f1_m: 0.9663 - val_loss: 0.1187 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1318 - f1_m: 0.9615 - val_loss: 0.1168 - val_f1_m: 0.9766\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1289 - f1_m: 0.9663 - val_loss: 0.1144 - val_f1_m: 0.9766\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1257 - f1_m: 0.9663 - val_loss: 0.1132 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1234 - f1_m: 0.9615 - val_loss: 0.1125 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1215 - f1_m: 0.9663 - val_loss: 0.1111 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1194 - f1_m: 0.9639 - val_loss: 0.1109 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1164 - f1_m: 0.9663 - val_loss: 0.1104 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1146 - f1_m: 0.9663 - val_loss: 0.1097 - val_f1_m: 0.9766\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1138 - f1_m: 0.9639 - val_loss: 0.1093 - val_f1_m: 0.9766\n",
      "0.963942289352417\n",
      "16/16 [==============================] - 0s 752us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5753 - f1_m: 0.9736 - val_loss: 0.5395 - val_f1_m: 0.9453\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4595 - f1_m: 0.9760 - val_loss: 0.4381 - val_f1_m: 0.9453\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3383 - f1_m: 0.9760 - val_loss: 0.3507 - val_f1_m: 0.9453\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2382 - f1_m: 0.9736 - val_loss: 0.2980 - val_f1_m: 0.9453\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1727 - f1_m: 0.9736 - val_loss: 0.2803 - val_f1_m: 0.9453\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1428 - f1_m: 0.9760 - val_loss: 0.2798 - val_f1_m: 0.9453\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1278 - f1_m: 0.9736 - val_loss: 0.2815 - val_f1_m: 0.9453\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1208 - f1_m: 0.9736 - val_loss: 0.2806 - val_f1_m: 0.9453\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1172 - f1_m: 0.9736 - val_loss: 0.2760 - val_f1_m: 0.9453\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1144 - f1_m: 0.9760 - val_loss: 0.2693 - val_f1_m: 0.9453\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1120 - f1_m: 0.9760 - val_loss: 0.2683 - val_f1_m: 0.9453\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1096 - f1_m: 0.9760 - val_loss: 0.2644 - val_f1_m: 0.9453\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1076 - f1_m: 0.9760 - val_loss: 0.2638 - val_f1_m: 0.9453\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1064 - f1_m: 0.9760 - val_loss: 0.2652 - val_f1_m: 0.9453\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1047 - f1_m: 0.9760 - val_loss: 0.2588 - val_f1_m: 0.9453\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1029 - f1_m: 0.9760 - val_loss: 0.2577 - val_f1_m: 0.9453\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1018 - f1_m: 0.9760 - val_loss: 0.2522 - val_f1_m: 0.9453\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1000 - f1_m: 0.9760 - val_loss: 0.2530 - val_f1_m: 0.9453\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0985 - f1_m: 0.9760 - val_loss: 0.2511 - val_f1_m: 0.9453\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0972 - f1_m: 0.9760 - val_loss: 0.2490 - val_f1_m: 0.9453\n",
      "0.9759615659713745\n",
      "16/16 [==============================] - 0s 973us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5783 - f1_m: 0.9615 - val_loss: 0.4982 - val_f1_m: 0.9922\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4672 - f1_m: 0.9591 - val_loss: 0.3756 - val_f1_m: 0.9922\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3561 - f1_m: 0.9591 - val_loss: 0.2605 - val_f1_m: 0.9922\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2643 - f1_m: 0.9591 - val_loss: 0.1739 - val_f1_m: 0.9922\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2056 - f1_m: 0.9615 - val_loss: 0.1210 - val_f1_m: 0.9922\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1724 - f1_m: 0.9615 - val_loss: 0.0940 - val_f1_m: 0.9922\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1567 - f1_m: 0.9591 - val_loss: 0.0799 - val_f1_m: 0.9922\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1475 - f1_m: 0.9591 - val_loss: 0.0720 - val_f1_m: 0.9922\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1398 - f1_m: 0.9591 - val_loss: 0.0701 - val_f1_m: 0.9922\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1343 - f1_m: 0.9615 - val_loss: 0.0696 - val_f1_m: 0.9922\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1295 - f1_m: 0.9591 - val_loss: 0.0655 - val_f1_m: 0.9922\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1250 - f1_m: 0.9567 - val_loss: 0.0646 - val_f1_m: 0.9922\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1220 - f1_m: 0.9615 - val_loss: 0.0680 - val_f1_m: 0.9922\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1185 - f1_m: 0.9567 - val_loss: 0.0644 - val_f1_m: 0.9922\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1165 - f1_m: 0.9615 - val_loss: 0.0663 - val_f1_m: 0.9922\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1133 - f1_m: 0.9615 - val_loss: 0.0639 - val_f1_m: 0.9922\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1111 - f1_m: 0.9615 - val_loss: 0.0608 - val_f1_m: 0.9922\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1095 - f1_m: 0.9591 - val_loss: 0.0594 - val_f1_m: 0.9922\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1075 - f1_m: 0.9591 - val_loss: 0.0597 - val_f1_m: 0.9922\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1062 - f1_m: 0.9591 - val_loss: 0.0614 - val_f1_m: 0.9922\n",
      "0.9591345191001892\n",
      "16/16 [==============================] - 0s 857us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5782 - f1_m: 0.9663 - val_loss: 0.5302 - val_f1_m: 0.9687\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4627 - f1_m: 0.9688 - val_loss: 0.4193 - val_f1_m: 0.9687\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3495 - f1_m: 0.9688 - val_loss: 0.3137 - val_f1_m: 0.9687\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2509 - f1_m: 0.9688 - val_loss: 0.2383 - val_f1_m: 0.9687\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1909 - f1_m: 0.9663 - val_loss: 0.1983 - val_f1_m: 0.9687\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1613 - f1_m: 0.9663 - val_loss: 0.1832 - val_f1_m: 0.9687\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1486 - f1_m: 0.9663 - val_loss: 0.1771 - val_f1_m: 0.9687\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1422 - f1_m: 0.9663 - val_loss: 0.1724 - val_f1_m: 0.9687\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1380 - f1_m: 0.9688 - val_loss: 0.1705 - val_f1_m: 0.9687\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1354 - f1_m: 0.9615 - val_loss: 0.1673 - val_f1_m: 0.9687\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1325 - f1_m: 0.9639 - val_loss: 0.1654 - val_f1_m: 0.9687\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1308 - f1_m: 0.9663 - val_loss: 0.1655 - val_f1_m: 0.9687\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1286 - f1_m: 0.9663 - val_loss: 0.1634 - val_f1_m: 0.9687\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1271 - f1_m: 0.9688 - val_loss: 0.1617 - val_f1_m: 0.9687\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1255 - f1_m: 0.9688 - val_loss: 0.1594 - val_f1_m: 0.9687\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1244 - f1_m: 0.9663 - val_loss: 0.1597 - val_f1_m: 0.9687\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1227 - f1_m: 0.9687 - val_loss: 0.1566 - val_f1_m: 0.9687\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1211 - f1_m: 0.9663 - val_loss: 0.1552 - val_f1_m: 0.9687\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1200 - f1_m: 0.9663 - val_loss: 0.1553 - val_f1_m: 0.9687\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1184 - f1_m: 0.9663 - val_loss: 0.1530 - val_f1_m: 0.9687\n",
      "0.9663461446762085\n",
      "16/16 [==============================] - 0s 956us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5836 - f1_m: 0.9663 - val_loss: 0.5170 - val_f1_m: 0.9766\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4715 - f1_m: 0.9639 - val_loss: 0.4027 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3576 - f1_m: 0.9663 - val_loss: 0.2965 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2660 - f1_m: 0.9663 - val_loss: 0.2180 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2061 - f1_m: 0.9663 - val_loss: 0.1770 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1777 - f1_m: 0.9639 - val_loss: 0.1581 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1632 - f1_m: 0.9663 - val_loss: 0.1475 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1553 - f1_m: 0.9663 - val_loss: 0.1429 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1497 - f1_m: 0.9615 - val_loss: 0.1402 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1454 - f1_m: 0.9639 - val_loss: 0.1373 - val_f1_m: 0.9766\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1409 - f1_m: 0.9639 - val_loss: 0.1348 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1381 - f1_m: 0.9663 - val_loss: 0.1306 - val_f1_m: 0.9766\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1348 - f1_m: 0.9663 - val_loss: 0.1286 - val_f1_m: 0.9766\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1323 - f1_m: 0.9639 - val_loss: 0.1272 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1295 - f1_m: 0.9663 - val_loss: 0.1233 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1266 - f1_m: 0.9663 - val_loss: 0.1198 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1232 - f1_m: 0.9639 - val_loss: 0.1176 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1208 - f1_m: 0.9663 - val_loss: 0.1124 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1173 - f1_m: 0.9639 - val_loss: 0.1105 - val_f1_m: 0.9766\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1144 - f1_m: 0.9639 - val_loss: 0.1075 - val_f1_m: 0.9766\n",
      "0.963942289352417\n",
      "16/16 [==============================] - 0s 780us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5820 - f1_m: 0.9639 - val_loss: 0.5111 - val_f1_m: 0.9844\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4727 - f1_m: 0.9639 - val_loss: 0.3911 - val_f1_m: 0.9844\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3596 - f1_m: 0.9591 - val_loss: 0.2796 - val_f1_m: 0.9844\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2652 - f1_m: 0.9615 - val_loss: 0.2010 - val_f1_m: 0.9844\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2014 - f1_m: 0.9615 - val_loss: 0.1606 - val_f1_m: 0.9844\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1688 - f1_m: 0.9639 - val_loss: 0.1444 - val_f1_m: 0.9844\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1548 - f1_m: 0.9591 - val_loss: 0.1350 - val_f1_m: 0.9844\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1452 - f1_m: 0.9591 - val_loss: 0.1342 - val_f1_m: 0.9844\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1398 - f1_m: 0.9615 - val_loss: 0.1359 - val_f1_m: 0.9844\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1361 - f1_m: 0.9567 - val_loss: 0.1391 - val_f1_m: 0.9844\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1331 - f1_m: 0.9639 - val_loss: 0.1363 - val_f1_m: 0.9844\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1295 - f1_m: 0.9639 - val_loss: 0.1375 - val_f1_m: 0.9844\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1268 - f1_m: 0.9639 - val_loss: 0.1374 - val_f1_m: 0.9844\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1253 - f1_m: 0.9639 - val_loss: 0.1405 - val_f1_m: 0.9844\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1234 - f1_m: 0.9615 - val_loss: 0.1367 - val_f1_m: 0.9844\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1213 - f1_m: 0.9639 - val_loss: 0.1372 - val_f1_m: 0.9844\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1199 - f1_m: 0.9615 - val_loss: 0.1375 - val_f1_m: 0.9844\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1178 - f1_m: 0.9639 - val_loss: 0.1368 - val_f1_m: 0.9844\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1160 - f1_m: 0.9639 - val_loss: 0.1349 - val_f1_m: 0.9844\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1141 - f1_m: 0.9639 - val_loss: 0.1352 - val_f1_m: 0.9844\n",
      "0.9639422297477722\n",
      "16/16 [==============================] - 0s 880us/step\n",
      "if any\n",
      "483 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5783 - f1_m: 0.9639 - val_loss: 0.5084 - val_f1_m: 0.9844\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4634 - f1_m: 0.9639 - val_loss: 0.3860 - val_f1_m: 0.9844\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3505 - f1_m: 0.9567 - val_loss: 0.2686 - val_f1_m: 0.9844\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2549 - f1_m: 0.9639 - val_loss: 0.1875 - val_f1_m: 0.9844\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2011 - f1_m: 0.9639 - val_loss: 0.1386 - val_f1_m: 0.9844\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1726 - f1_m: 0.9639 - val_loss: 0.1170 - val_f1_m: 0.9844\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1628 - f1_m: 0.9591 - val_loss: 0.1074 - val_f1_m: 0.9844\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1555 - f1_m: 0.9615 - val_loss: 0.1047 - val_f1_m: 0.9844\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1515 - f1_m: 0.9591 - val_loss: 0.1030 - val_f1_m: 0.9844\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1483 - f1_m: 0.9615 - val_loss: 0.1016 - val_f1_m: 0.9844\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1462 - f1_m: 0.9639 - val_loss: 0.1007 - val_f1_m: 0.9844\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1433 - f1_m: 0.9639 - val_loss: 0.0995 - val_f1_m: 0.9844\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1421 - f1_m: 0.9615 - val_loss: 0.0993 - val_f1_m: 0.9844\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1405 - f1_m: 0.9639 - val_loss: 0.0986 - val_f1_m: 0.9844\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1388 - f1_m: 0.9639 - val_loss: 0.0982 - val_f1_m: 0.9844\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1374 - f1_m: 0.9639 - val_loss: 0.0981 - val_f1_m: 0.9844\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1365 - f1_m: 0.9615 - val_loss: 0.0973 - val_f1_m: 0.9844\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1352 - f1_m: 0.9639 - val_loss: 0.0972 - val_f1_m: 0.9844\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1343 - f1_m: 0.9591 - val_loss: 0.0961 - val_f1_m: 0.9844\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1341 - f1_m: 0.9615 - val_loss: 0.0960 - val_f1_m: 0.9844\n",
      "0.9615383744239807\n",
      "16/16 [==============================] - 0s 832us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5840 - f1_m: 0.9663 - val_loss: 0.5152 - val_f1_m: 0.9766\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4697 - f1_m: 0.9663 - val_loss: 0.3967 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3548 - f1_m: 0.9663 - val_loss: 0.2840 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2610 - f1_m: 0.9639 - val_loss: 0.2020 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2002 - f1_m: 0.9663 - val_loss: 0.1582 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1687 - f1_m: 0.9639 - val_loss: 0.1365 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1533 - f1_m: 0.9663 - val_loss: 0.1239 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1425 - f1_m: 0.9639 - val_loss: 0.1161 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1346 - f1_m: 0.9663 - val_loss: 0.1101 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1279 - f1_m: 0.9663 - val_loss: 0.1058 - val_f1_m: 0.9766\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1230 - f1_m: 0.9663 - val_loss: 0.1028 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1190 - f1_m: 0.9663 - val_loss: 0.1001 - val_f1_m: 0.9766\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1163 - f1_m: 0.9615 - val_loss: 0.0975 - val_f1_m: 0.9766\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1129 - f1_m: 0.9663 - val_loss: 0.0966 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1096 - f1_m: 0.9663 - val_loss: 0.0948 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1069 - f1_m: 0.9663 - val_loss: 0.0931 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1056 - f1_m: 0.9639 - val_loss: 0.0919 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1030 - f1_m: 0.9663 - val_loss: 0.0916 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1001 - f1_m: 0.9663 - val_loss: 0.0911 - val_f1_m: 0.9766\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0984 - f1_m: 0.9639 - val_loss: 0.0903 - val_f1_m: 0.9766\n",
      "0.963942289352417\n",
      "16/16 [==============================] - 0s 748us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5768 - f1_m: 0.9712 - val_loss: 0.5197 - val_f1_m: 0.9609\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4619 - f1_m: 0.9688 - val_loss: 0.4084 - val_f1_m: 0.9609\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3416 - f1_m: 0.9687 - val_loss: 0.3096 - val_f1_m: 0.9609\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2409 - f1_m: 0.9712 - val_loss: 0.2416 - val_f1_m: 0.9609\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1757 - f1_m: 0.9688 - val_loss: 0.2105 - val_f1_m: 0.9609\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1405 - f1_m: 0.9688 - val_loss: 0.1998 - val_f1_m: 0.9609\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1245 - f1_m: 0.9712 - val_loss: 0.1977 - val_f1_m: 0.9609\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1143 - f1_m: 0.9688 - val_loss: 0.1967 - val_f1_m: 0.9609\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1074 - f1_m: 0.9688 - val_loss: 0.1954 - val_f1_m: 0.9609\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1025 - f1_m: 0.9688 - val_loss: 0.1948 - val_f1_m: 0.9609\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0980 - f1_m: 0.9688 - val_loss: 0.1955 - val_f1_m: 0.9609\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0946 - f1_m: 0.9688 - val_loss: 0.1955 - val_f1_m: 0.9609\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0918 - f1_m: 0.9712 - val_loss: 0.1961 - val_f1_m: 0.9609\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0886 - f1_m: 0.9712 - val_loss: 0.1999 - val_f1_m: 0.9609\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0860 - f1_m: 0.9712 - val_loss: 0.2012 - val_f1_m: 0.9609\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0835 - f1_m: 0.9712 - val_loss: 0.2040 - val_f1_m: 0.9609\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0815 - f1_m: 0.9688 - val_loss: 0.2043 - val_f1_m: 0.9609\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0791 - f1_m: 0.9712 - val_loss: 0.2056 - val_f1_m: 0.9609\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0776 - f1_m: 0.9712 - val_loss: 0.2069 - val_f1_m: 0.9609\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0756 - f1_m: 0.9712 - val_loss: 0.2107 - val_f1_m: 0.9609\n",
      "0.9711538553237915\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5761 - f1_m: 0.9663 - val_loss: 0.5068 - val_f1_m: 0.9766\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4607 - f1_m: 0.9663 - val_loss: 0.3886 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3466 - f1_m: 0.9639 - val_loss: 0.2821 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2560 - f1_m: 0.9639 - val_loss: 0.2091 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1973 - f1_m: 0.9639 - val_loss: 0.1703 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1690 - f1_m: 0.9663 - val_loss: 0.1511 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1556 - f1_m: 0.9639 - val_loss: 0.1409 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1479 - f1_m: 0.9663 - val_loss: 0.1349 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1427 - f1_m: 0.9663 - val_loss: 0.1302 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1388 - f1_m: 0.9663 - val_loss: 0.1263 - val_f1_m: 0.9766\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1347 - f1_m: 0.9663 - val_loss: 0.1230 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1311 - f1_m: 0.9663 - val_loss: 0.1208 - val_f1_m: 0.9766\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1282 - f1_m: 0.9639 - val_loss: 0.1197 - val_f1_m: 0.9766\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1251 - f1_m: 0.9639 - val_loss: 0.1184 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1226 - f1_m: 0.9663 - val_loss: 0.1171 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1196 - f1_m: 0.9663 - val_loss: 0.1159 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1169 - f1_m: 0.9639 - val_loss: 0.1159 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1143 - f1_m: 0.9663 - val_loss: 0.1142 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1126 - f1_m: 0.9663 - val_loss: 0.1137 - val_f1_m: 0.9766\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1093 - f1_m: 0.9663 - val_loss: 0.1127 - val_f1_m: 0.9766\n",
      "0.9663460850715637\n",
      "16/16 [==============================] - 0s 837us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5755 - f1_m: 0.9688 - val_loss: 0.5230 - val_f1_m: 0.9688\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4596 - f1_m: 0.9688 - val_loss: 0.4113 - val_f1_m: 0.9688\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3422 - f1_m: 0.9687 - val_loss: 0.3094 - val_f1_m: 0.9688\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2485 - f1_m: 0.9663 - val_loss: 0.2361 - val_f1_m: 0.9688\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1906 - f1_m: 0.9663 - val_loss: 0.2022 - val_f1_m: 0.9688\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1634 - f1_m: 0.9663 - val_loss: 0.1874 - val_f1_m: 0.9688\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1528 - f1_m: 0.9639 - val_loss: 0.1787 - val_f1_m: 0.9688\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1475 - f1_m: 0.9591 - val_loss: 0.1732 - val_f1_m: 0.9688\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1447 - f1_m: 0.9688 - val_loss: 0.1658 - val_f1_m: 0.9688\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1420 - f1_m: 0.9663 - val_loss: 0.1619 - val_f1_m: 0.9688\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1396 - f1_m: 0.9688 - val_loss: 0.1610 - val_f1_m: 0.9688\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1383 - f1_m: 0.9639 - val_loss: 0.1602 - val_f1_m: 0.9688\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1367 - f1_m: 0.9663 - val_loss: 0.1577 - val_f1_m: 0.9688\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1352 - f1_m: 0.9663 - val_loss: 0.1557 - val_f1_m: 0.9688\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1340 - f1_m: 0.9688 - val_loss: 0.1528 - val_f1_m: 0.9688\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1331 - f1_m: 0.9688 - val_loss: 0.1507 - val_f1_m: 0.9688\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1317 - f1_m: 0.9663 - val_loss: 0.1497 - val_f1_m: 0.9688\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1307 - f1_m: 0.9639 - val_loss: 0.1492 - val_f1_m: 0.9688\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1294 - f1_m: 0.9688 - val_loss: 0.1475 - val_f1_m: 0.9688\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1287 - f1_m: 0.9663 - val_loss: 0.1464 - val_f1_m: 0.9688\n",
      "0.9663461446762085\n",
      "16/16 [==============================] - 0s 966us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 14ms/step - loss: 0.5823 - f1_m: 0.9663 - val_loss: 0.5214 - val_f1_m: 0.9766\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4700 - f1_m: 0.9639 - val_loss: 0.4079 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3568 - f1_m: 0.9663 - val_loss: 0.3013 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2637 - f1_m: 0.9615 - val_loss: 0.2234 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2056 - f1_m: 0.9639 - val_loss: 0.1820 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1779 - f1_m: 0.9567 - val_loss: 0.1626 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1636 - f1_m: 0.9639 - val_loss: 0.1541 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1562 - f1_m: 0.9639 - val_loss: 0.1471 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1516 - f1_m: 0.9615 - val_loss: 0.1442 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1458 - f1_m: 0.9663 - val_loss: 0.1403 - val_f1_m: 0.9766\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1422 - f1_m: 0.9663 - val_loss: 0.1361 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1389 - f1_m: 0.9639 - val_loss: 0.1332 - val_f1_m: 0.9766\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1356 - f1_m: 0.9615 - val_loss: 0.1323 - val_f1_m: 0.9766\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1324 - f1_m: 0.9663 - val_loss: 0.1291 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1295 - f1_m: 0.9639 - val_loss: 0.1257 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1265 - f1_m: 0.9663 - val_loss: 0.1227 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1233 - f1_m: 0.9663 - val_loss: 0.1195 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1206 - f1_m: 0.9639 - val_loss: 0.1170 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1176 - f1_m: 0.9639 - val_loss: 0.1154 - val_f1_m: 0.9766\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1141 - f1_m: 0.9663 - val_loss: 0.1148 - val_f1_m: 0.9766\n",
      "0.9663461446762085\n",
      "16/16 [==============================] - 0s 886us/step\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5815 - f1_m: 0.9663 - val_loss: 0.5235 - val_f1_m: 0.9688\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4649 - f1_m: 0.9687 - val_loss: 0.4132 - val_f1_m: 0.9688\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3504 - f1_m: 0.9688 - val_loss: 0.3080 - val_f1_m: 0.9688\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2572 - f1_m: 0.9639 - val_loss: 0.2345 - val_f1_m: 0.9688\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1985 - f1_m: 0.9663 - val_loss: 0.2007 - val_f1_m: 0.9688\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1677 - f1_m: 0.9663 - val_loss: 0.1915 - val_f1_m: 0.9688\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1539 - f1_m: 0.9688 - val_loss: 0.1912 - val_f1_m: 0.9688\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1452 - f1_m: 0.9688 - val_loss: 0.1936 - val_f1_m: 0.9688\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1392 - f1_m: 0.9663 - val_loss: 0.1955 - val_f1_m: 0.9688\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1344 - f1_m: 0.9663 - val_loss: 0.1974 - val_f1_m: 0.9688\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1302 - f1_m: 0.9688 - val_loss: 0.2009 - val_f1_m: 0.9688\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1266 - f1_m: 0.9688 - val_loss: 0.2024 - val_f1_m: 0.9688\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1231 - f1_m: 0.9688 - val_loss: 0.2031 - val_f1_m: 0.9688\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1206 - f1_m: 0.9688 - val_loss: 0.2050 - val_f1_m: 0.9688\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1182 - f1_m: 0.9688 - val_loss: 0.2066 - val_f1_m: 0.9688\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1152 - f1_m: 0.9688 - val_loss: 0.2080 - val_f1_m: 0.9688\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1125 - f1_m: 0.9639 - val_loss: 0.2078 - val_f1_m: 0.9688\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1101 - f1_m: 0.9688 - val_loss: 0.2075 - val_f1_m: 0.9688\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1083 - f1_m: 0.9663 - val_loss: 0.2078 - val_f1_m: 0.9688\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1067 - f1_m: 0.9688 - val_loss: 0.2069 - val_f1_m: 0.9688\n",
      "0.96875\n",
      "16/16 [==============================] - 0s 870us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5843 - f1_m: 0.9567 - val_loss: 0.4958 - val_f1_m: 1.0000\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4740 - f1_m: 0.9567 - val_loss: 0.3662 - val_f1_m: 1.0000\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3645 - f1_m: 0.9567 - val_loss: 0.2426 - val_f1_m: 1.0000\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2733 - f1_m: 0.9543 - val_loss: 0.1486 - val_f1_m: 1.0000\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2160 - f1_m: 0.9591 - val_loss: 0.0933 - val_f1_m: 1.0000\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1887 - f1_m: 0.9591 - val_loss: 0.0639 - val_f1_m: 1.0000\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1746 - f1_m: 0.9591 - val_loss: 0.0522 - val_f1_m: 1.0000\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1671 - f1_m: 0.9567 - val_loss: 0.0445 - val_f1_m: 1.0000\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1622 - f1_m: 0.9591 - val_loss: 0.0428 - val_f1_m: 1.0000\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1578 - f1_m: 0.9567 - val_loss: 0.0410 - val_f1_m: 1.0000\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1537 - f1_m: 0.9591 - val_loss: 0.0417 - val_f1_m: 1.0000\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1502 - f1_m: 0.9567 - val_loss: 0.0394 - val_f1_m: 1.0000\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1473 - f1_m: 0.9567 - val_loss: 0.0396 - val_f1_m: 1.0000\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1442 - f1_m: 0.9591 - val_loss: 0.0407 - val_f1_m: 1.0000\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1422 - f1_m: 0.9567 - val_loss: 0.0396 - val_f1_m: 1.0000\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1404 - f1_m: 0.9543 - val_loss: 0.0406 - val_f1_m: 1.0000\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1379 - f1_m: 0.9567 - val_loss: 0.0412 - val_f1_m: 1.0000\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1364 - f1_m: 0.9591 - val_loss: 0.0423 - val_f1_m: 1.0000\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1348 - f1_m: 0.9543 - val_loss: 0.0384 - val_f1_m: 1.0000\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1324 - f1_m: 0.9567 - val_loss: 0.0414 - val_f1_m: 1.0000\n",
      "0.9567307233810425\n",
      "16/16 [==============================] - 0s 980us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 12ms/step - loss: 0.5813 - f1_m: 0.9639 - val_loss: 0.5087 - val_f1_m: 0.9844\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4692 - f1_m: 0.9639 - val_loss: 0.3880 - val_f1_m: 0.9844\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3600 - f1_m: 0.9615 - val_loss: 0.2760 - val_f1_m: 0.9844\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2700 - f1_m: 0.9591 - val_loss: 0.1970 - val_f1_m: 0.9844\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2147 - f1_m: 0.9615 - val_loss: 0.1535 - val_f1_m: 0.9844\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1859 - f1_m: 0.9615 - val_loss: 0.1298 - val_f1_m: 0.9844\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1727 - f1_m: 0.9639 - val_loss: 0.1193 - val_f1_m: 0.9844\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1662 - f1_m: 0.9639 - val_loss: 0.1112 - val_f1_m: 0.9844\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1608 - f1_m: 0.9615 - val_loss: 0.1062 - val_f1_m: 0.9844\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1573 - f1_m: 0.9591 - val_loss: 0.1034 - val_f1_m: 0.9844\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1541 - f1_m: 0.9615 - val_loss: 0.1018 - val_f1_m: 0.9844\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1512 - f1_m: 0.9615 - val_loss: 0.1000 - val_f1_m: 0.9844\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1488 - f1_m: 0.9615 - val_loss: 0.0977 - val_f1_m: 0.9844\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1460 - f1_m: 0.9591 - val_loss: 0.0973 - val_f1_m: 0.9844\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1445 - f1_m: 0.9639 - val_loss: 0.0964 - val_f1_m: 0.9844\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1420 - f1_m: 0.9615 - val_loss: 0.0936 - val_f1_m: 0.9844\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1395 - f1_m: 0.9639 - val_loss: 0.0918 - val_f1_m: 0.9844\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1379 - f1_m: 0.9615 - val_loss: 0.0905 - val_f1_m: 0.9844\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1363 - f1_m: 0.9639 - val_loss: 0.0881 - val_f1_m: 0.9844\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1346 - f1_m: 0.9639 - val_loss: 0.0881 - val_f1_m: 0.9844\n",
      "0.9639422297477722\n",
      "16/16 [==============================] - 0s 740us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 12ms/step - loss: 0.5812 - f1_m: 0.9688 - val_loss: 0.5200 - val_f1_m: 0.9688\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4661 - f1_m: 0.9687 - val_loss: 0.4051 - val_f1_m: 0.9688\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3548 - f1_m: 0.9663 - val_loss: 0.3004 - val_f1_m: 0.9688\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2592 - f1_m: 0.9688 - val_loss: 0.2284 - val_f1_m: 0.9688\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2013 - f1_m: 0.9688 - val_loss: 0.1906 - val_f1_m: 0.9688\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1713 - f1_m: 0.9688 - val_loss: 0.1735 - val_f1_m: 0.9688\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1573 - f1_m: 0.9663 - val_loss: 0.1632 - val_f1_m: 0.9688\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1496 - f1_m: 0.9663 - val_loss: 0.1585 - val_f1_m: 0.9688\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1451 - f1_m: 0.9688 - val_loss: 0.1533 - val_f1_m: 0.9688\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1420 - f1_m: 0.9639 - val_loss: 0.1509 - val_f1_m: 0.9688\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1389 - f1_m: 0.9639 - val_loss: 0.1480 - val_f1_m: 0.9688\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1358 - f1_m: 0.9639 - val_loss: 0.1469 - val_f1_m: 0.9688\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1342 - f1_m: 0.9663 - val_loss: 0.1462 - val_f1_m: 0.9688\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1326 - f1_m: 0.9663 - val_loss: 0.1454 - val_f1_m: 0.9688\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1305 - f1_m: 0.9687 - val_loss: 0.1433 - val_f1_m: 0.9688\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1285 - f1_m: 0.9639 - val_loss: 0.1429 - val_f1_m: 0.9688\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1265 - f1_m: 0.9688 - val_loss: 0.1437 - val_f1_m: 0.9688\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1247 - f1_m: 0.9688 - val_loss: 0.1443 - val_f1_m: 0.9688\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1234 - f1_m: 0.9688 - val_loss: 0.1444 - val_f1_m: 0.9688\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1212 - f1_m: 0.9663 - val_loss: 0.1432 - val_f1_m: 0.9688\n",
      "0.9663461446762085\n",
      "16/16 [==============================] - 0s 784us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 13ms/step - loss: 0.5827 - f1_m: 0.9612 - val_loss: 0.5025 - val_f1_m: 0.9922\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4755 - f1_m: 0.9612 - val_loss: 0.3813 - val_f1_m: 0.9922\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3637 - f1_m: 0.9639 - val_loss: 0.2670 - val_f1_m: 0.9922\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2694 - f1_m: 0.9612 - val_loss: 0.1773 - val_f1_m: 0.9922\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2113 - f1_m: 0.9612 - val_loss: 0.1236 - val_f1_m: 0.9922\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1787 - f1_m: 0.9612 - val_loss: 0.1004 - val_f1_m: 0.9922\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1626 - f1_m: 0.9639 - val_loss: 0.0921 - val_f1_m: 0.9922\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1530 - f1_m: 0.9639 - val_loss: 0.0889 - val_f1_m: 0.9922\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1466 - f1_m: 0.9639 - val_loss: 0.0860 - val_f1_m: 0.9922\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1408 - f1_m: 0.9639 - val_loss: 0.0860 - val_f1_m: 0.9922\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1361 - f1_m: 0.9585 - val_loss: 0.0857 - val_f1_m: 0.9922\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1323 - f1_m: 0.9585 - val_loss: 0.0866 - val_f1_m: 0.9922\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1271 - f1_m: 0.9639 - val_loss: 0.0862 - val_f1_m: 0.9922\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1238 - f1_m: 0.9639 - val_loss: 0.0857 - val_f1_m: 0.9922\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1209 - f1_m: 0.9639 - val_loss: 0.0851 - val_f1_m: 0.9922\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1180 - f1_m: 0.9639 - val_loss: 0.0845 - val_f1_m: 0.9922\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1153 - f1_m: 0.9612 - val_loss: 0.0849 - val_f1_m: 0.9922\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1129 - f1_m: 0.9639 - val_loss: 0.0854 - val_f1_m: 0.9922\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1097 - f1_m: 0.9639 - val_loss: 0.0856 - val_f1_m: 0.9922\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1068 - f1_m: 0.9639 - val_loss: 0.0843 - val_f1_m: 0.9922\n",
      "0.9639422297477722\n",
      "16/16 [==============================] - 0s 815us/step\n",
      "if any\n",
      "12\n",
      "4\n",
      "3\n",
      "All Done\n"
     ]
    }
   ],
   "source": [
    "#now for ANN-3\n",
    "#now trying for aNN-2 get-initial_model2\n",
    "#the below code is for binary classification since MNSIT is a multi-class low-resolution image dataset. I am updating the code in the next cell.\n",
    "N=500\n",
    "#Positive=Pos\n",
    "#Negative=Neg\n",
    "positiveN=int((Positive.shape[0]/dataset.shape[0])*N)\n",
    "negativeN=int(N-positiveN)\n",
    "print(positiveN, negativeN)\n",
    "#target variable\n",
    "#target_variable=\"default.payment.next.month\"\n",
    "df1=Positive.sample(positiveN)\n",
    "Positive.drop(df1.index, inplace=True)\n",
    "df2=Negative.sample(negativeN)\n",
    "Negative.drop(df2.index, inplace=True)\n",
    "test_data=df1.append(df2, ignore_index=True)\n",
    "test_data=test_data.sample(frac = 1) #This is to shuffel the training and testing data\n",
    "test_data=test_data.sample(frac = 1)\n",
    "test_data=test_data.sample(frac = 1)\n",
    "X_test=test_data.drop(columns=[target_variable])\n",
    "y_test=to_categorical(test_data[target_variable])\n",
    "\n",
    "# adding dense layer\n",
    "initial_model= get_initial_model_3(X_test.shape[1], 2)\n",
    "initial_model.set_weights(update_weights(initial_model.get_weights()))\n",
    "Models=[]\n",
    "val_acc=[]\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "val_loss=[]\n",
    "train_loss=[]\n",
    "add_weights=[]\n",
    "while Positive.empty==False and Negative.empty==False:\n",
    "  print(positiveN, negativeN)\n",
    "  df1=Positive.sample(min(positiveN, len(Positive)))\n",
    "  Positive.drop(df1.index, inplace=True)\n",
    "  df2=Negative.sample(min(negativeN, len(Negative)))\n",
    "  Negative.drop(df2.index, inplace=True)\n",
    "  train_data=df1.append(df2, ignore_index=True)\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "    \n",
    "  #all models have different initialization\n",
    "  # define the sequential model\n",
    "  \"\"\"initial_model = keras.Sequential()\n",
    "\n",
    "    # adding dense layer\n",
    "  initial_model.add(Dense(5, input_dim=X_test.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "  initial_model.add(Dense(10, activation='relu'))\n",
    "  initial_model.add(Dense(5, activation='relu'))\n",
    "\n",
    "    # adding dense layer with softmax activation/output layer\n",
    "  initial_model.add(Dense(2, activation='softmax'))\n",
    "  #initial_model.summary()\"\"\"\n",
    "  ann_model=get_initial_model_3(X_test.shape[1], 2) #same intial weights\n",
    "  ann_model.set_weights(initial_model.get_weights())\n",
    "  X_train=train_data.drop(columns=[target_variable])\n",
    "  #train_data[target_variable]=train_data[target_variable]-1 #only for skin_nonskin dataset\n",
    "  y_train=to_categorical(train_data[target_variable])\n",
    "  #print(y_train)\n",
    "  ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_m]) # metrics=['accuracy']\n",
    "  history = ann_model.fit(X_train, y_train, epochs=20, validation_split=0.2, verbose=1)\n",
    "  print(history.history['f1_m'][-1])\n",
    "  ann_model.set_weights(update_weights(ann_model.get_weights()))\n",
    "  pred_test=ann_model.predict(X_test)\n",
    "  present=False\n",
    "  for i in range(len(Models)):\n",
    "    if (check_models(Models[i][0], ann_model.get_weights())):\n",
    "      print(\"if any\")\n",
    "      Models[i][1]=Models[i][1]+1\n",
    "      add_weights[i].append(ann_model.get_weights())\n",
    "      val_acc[i].append(history.history['val_f1_m'])\n",
    "      train_acc[i].append(history.history['f1_m'])\n",
    "      test_acc[i].append(f1_m(y_test, pred_test).numpy())\n",
    "      val_loss[i].append(history.history['val_loss'])\n",
    "      train_loss[i].append(history.history['loss'])\n",
    "      present=True\n",
    "      break;\n",
    "  if present==False:\n",
    "    add_weights.append([ann_model.get_weights()])\n",
    "    Models.append([ann_model.get_weights(), 1])\n",
    "    val_acc.append([history.history['val_f1_m']])\n",
    "    train_acc.append([history.history['f1_m']])\n",
    "    test_acc.append([f1_m(y_test, pred_test).numpy()])\n",
    "    val_loss.append([history.history['val_loss']])\n",
    "    train_loss.append([history.history['loss']])\n",
    "for i in range(len(Models)):\n",
    "  print(Models[i][1])\n",
    "print(\"All Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(add_weights[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.05 ,  0.1  ,  0.18 ,  0.004, -0.09 ,  0.03 , -0.22 ,  0.02 ,\n",
      "         0.03 , -0.05 ],\n",
      "       [-0.01 ,  0.11 , -0.04 ,  0.01 , -0.12 ,  0.03 , -0.13 ,  0.1  ,\n",
      "        -0.02 , -0.24 ],\n",
      "       [ 0.02 , -0.01 , -0.08 , -0.09 ,  0.02 , -0.12 ,  0.02 , -0.06 ,\n",
      "         0.001, -0.07 ],\n",
      "       [-0.12 ,  0.08 , -0.06 ,  0.12 ,  0.03 ,  0.05 , -0.1  , -0.05 ,\n",
      "        -0.02 , -0.03 ],\n",
      "       [ 0.08 ,  0.14 , -0.12 , -0.04 , -0.06 , -0.01 ,  0.004, -0.12 ,\n",
      "         0.04 ,  0.003],\n",
      "       [ 0.08 ,  0.1  , -0.01 ,  0.04 , -0.03 , -0.24 ,  0.03 ,  0.05 ,\n",
      "        -0.005, -0.03 ],\n",
      "       [ 0.1  ,  0.12 ,  0.12 , -0.06 ,  0.1  , -0.01 ,  0.17 ,  0.11 ,\n",
      "         0.07 ,  0.15 ],\n",
      "       [ 0.04 , -0.04 ,  0.19 , -0.08 ,  0.09 , -0.11 ,  0.05 ,  0.15 ,\n",
      "         0.003,  0.06 ]], dtype=float32), array([ 0.1  ,  0.07 ,  0.07 , -0.002,  0.12 , -0.02 ,  0.11 ,  0.08 ,\n",
      "        0.09 ,  0.1  ], dtype=float32), array([[ 4.0e-01, -1.5e-01,  3.8e-01, -4.0e-05, -1.8e-01,  1.0e-03,\n",
      "         4.3e-01,  1.6e-01,  2.6e-01,  2.6e-01,  1.3e-01,  1.3e-01,\n",
      "         2.6e-01,  1.3e-01,  3.9e-01,  6.0e-02,  3.4e-01,  3.0e-01,\n",
      "        -1.6e-01,  1.2e-01],\n",
      "       [-7.0e-02,  4.2e-01,  1.0e-01, -4.6e-01, -3.2e-01, -2.3e-01,\n",
      "         1.2e-01,  1.8e-01,  2.2e-01,  4.0e-01, -2.3e-01, -3.0e-02,\n",
      "         1.5e-01,  4.7e-01,  3.0e-01, -1.8e-01,  4.7e-01,  3.3e-01,\n",
      "        -7.0e-02,  4.0e-01],\n",
      "       [ 2.5e-01, -3.9e-01, -4.1e-01,  5.0e-02,  2.4e-01,  1.1e-01,\n",
      "        -3.2e-01, -2.7e-01,  3.6e-01, -2.6e-01,  1.0e-02,  2.5e-01,\n",
      "         4.6e-01,  2.8e-01,  3.9e-01, -4.4e-01, -3.5e-01, -1.0e-01,\n",
      "         2.9e-01, -3.9e-01],\n",
      "       [ 1.8e-01,  2.3e-01, -1.4e-01,  4.0e-02,  1.9e-01, -4.2e-01,\n",
      "        -2.0e-01,  4.0e-02,  3.3e-01, -3.6e-01,  9.0e-02, -1.9e-01,\n",
      "        -5.0e-02, -3.2e-01,  2.1e-01, -5.0e-03,  5.0e-02,  3.0e-01,\n",
      "        -4.6e-01, -4.1e-01],\n",
      "       [ 2.1e-01, -2.8e-01, -2.0e-01, -5.4e-01,  2.1e-01,  1.6e-01,\n",
      "         2.1e-01,  5.0e-02,  3.5e-01,  2.3e-01,  4.3e-01,  4.6e-01,\n",
      "        -3.5e-01, -1.0e-02,  3.7e-01, -1.0e-01, -2.4e-01, -8.0e-02,\n",
      "         2.6e-01, -1.6e-01],\n",
      "       [-3.4e-01,  5.1e-01, -1.1e-01,  3.0e-03, -3.5e-01, -4.2e-01,\n",
      "        -5.0e-02, -2.8e-01,  1.4e-01, -4.3e-01, -4.5e-01, -2.4e-01,\n",
      "        -9.0e-02,  2.5e-01, -4.9e-01,  2.0e-02, -2.7e-01, -1.5e-01,\n",
      "         5.0e-02,  3.8e-01],\n",
      "       [-9.0e-02, -1.7e-01,  4.1e-01, -2.1e-01,  5.0e-01,  1.7e-01,\n",
      "        -2.2e-01,  3.0e-01,  1.6e-01,  4.5e-01,  1.4e-01, -1.1e-01,\n",
      "        -2.4e-01, -1.0e-01,  2.8e-01,  2.2e-01,  2.2e-01,  8.0e-02,\n",
      "         1.2e-01,  3.0e-02],\n",
      "       [ 1.0e-01,  2.2e-01, -1.1e-01,  5.0e-02, -2.0e-01,  5.3e-01,\n",
      "        -4.3e-01, -1.4e-01,  4.4e-01,  4.0e-03,  4.3e-01, -4.0e-03,\n",
      "         5.0e-01, -2.8e-01,  1.9e-01,  8.0e-02,  3.5e-01, -4.4e-01,\n",
      "         1.1e-01, -3.6e-01],\n",
      "       [-4.1e-01, -3.4e-01, -1.0e-02,  5.1e-01, -6.0e-02, -5.0e-02,\n",
      "         5.0e-02, -2.7e-01,  4.2e-01,  2.4e-01, -3.3e-01, -2.7e-01,\n",
      "         3.9e-01,  1.1e-01,  1.6e-01, -4.7e-01, -1.7e-01,  5.0e-02,\n",
      "         2.6e-01, -3.2e-01],\n",
      "       [-2.7e-01,  2.7e-01,  1.6e-01, -1.7e-01, -8.0e-02,  1.1e-01,\n",
      "         3.8e-01, -2.6e-01, -2.4e-01,  4.1e-01,  2.6e-01,  1.7e-01,\n",
      "         3.4e-01,  4.6e-01,  4.0e-01, -3.5e-01,  6.0e-02,  3.7e-01,\n",
      "         3.6e-01,  2.4e-01]], dtype=float32), array([-0.06 , -0.01 , -0.02 ,  0.02 ,  0.11 ,  0.08 , -0.03 ,  0.07 ,\n",
      "        0.07 ,  0.09 ,  0.07 ,  0.08 ,  0.07 ,  0.08 ,  0.07 , -0.03 ,\n",
      "        0.07 ,  0.002,  0.08 , -0.04 ], dtype=float32), array([[-5.0e-02, -1.9e-01, -1.7e-01,  2.6e-01, -3.8e-01, -2.8e-01,\n",
      "        -1.2e-01, -2.2e-01, -4.0e-02,  3.5e-01],\n",
      "       [-2.0e-02,  4.0e-03, -3.1e-01, -2.5e-01, -8.0e-02, -3.4e-01,\n",
      "        -3.0e-01,  1.7e-01,  4.2e-01, -2.1e-01],\n",
      "       [ 1.1e-01,  1.7e-01,  5.0e-02,  2.0e-01, -3.6e-01,  4.1e-01,\n",
      "        -7.0e-02, -3.0e-01, -4.4e-01, -2.1e-01],\n",
      "       [ 8.0e-02, -6.0e-02, -4.5e-01, -2.0e-02,  2.5e-01,  2.6e-01,\n",
      "        -2.0e-01, -4.3e-01, -3.4e-01,  3.4e-01],\n",
      "       [ 2.5e-01,  2.7e-01,  2.6e-01,  2.8e-01,  1.4e-01, -6.0e-02,\n",
      "        -2.4e-01,  3.8e-01,  2.6e-01,  3.5e-01],\n",
      "       [ 4.0e-02,  4.7e-01,  1.2e-01,  2.8e-01,  2.9e-01,  3.3e-01,\n",
      "        -3.0e-01,  5.3e-01,  8.0e-02, -1.3e-01],\n",
      "       [ 4.0e-02, -1.2e-01, -2.5e-01,  4.3e-01, -1.5e-01,  9.0e-02,\n",
      "        -3.0e-01,  1.7e-01,  3.3e-01,  3.7e-01],\n",
      "       [-2.5e-01,  2.9e-01, -1.3e-01,  2.2e-01, -1.4e-01,  5.1e-01,\n",
      "        -6.0e-02,  3.6e-01, -6.0e-02, -2.4e-01],\n",
      "       [-4.4e-01,  2.1e-01,  1.0e-01,  1.0e-02, -2.7e-01,  2.5e-01,\n",
      "        -4.0e-03,  2.6e-01, -4.4e-01, -4.6e-01],\n",
      "       [ 3.5e-01,  2.7e-01, -8.0e-02,  1.4e-01, -4.5e-01,  3.2e-01,\n",
      "         2.5e-01,  3.8e-01,  2.5e-01,  2.6e-01],\n",
      "       [-2.1e-01, -2.9e-01, -1.4e-01, -1.6e-01,  2.0e-01,  4.7e-01,\n",
      "         1.3e-01,  4.2e-01, -3.0e-02, -4.8e-01],\n",
      "       [-1.3e-01, -9.0e-02,  5.4e-01, -5.0e-04, -3.6e-01, -1.6e-01,\n",
      "         1.0e-01,  2.3e-01,  2.6e-01, -3.8e-01],\n",
      "       [ 1.8e-01, -2.6e-01,  4.7e-01,  7.0e-02,  3.6e-01, -1.2e-01,\n",
      "        -3.8e-01,  3.5e-01,  3.9e-01, -3.2e-01],\n",
      "       [-2.4e-01,  8.0e-02,  1.0e-01, -8.0e-02, -2.8e-01,  1.2e-01,\n",
      "         1.0e-01,  4.9e-01,  3.4e-01, -4.0e-02],\n",
      "       [ 4.0e-02,  3.2e-01,  2.7e-01,  5.4e-01, -1.5e-01,  2.7e-01,\n",
      "        -3.0e-02,  5.2e-01, -4.9e-01,  1.1e-01],\n",
      "       [-1.9e-01, -1.0e-02,  2.2e-01,  2.9e-01, -4.4e-01, -4.0e-02,\n",
      "         4.0e-02, -8.0e-02, -1.3e-01,  2.5e-01],\n",
      "       [-2.6e-01,  1.2e-01,  4.0e-03,  1.7e-01, -3.8e-01,  2.9e-01,\n",
      "        -2.4e-01,  2.6e-01, -3.9e-01, -1.8e-01],\n",
      "       [-4.8e-01,  5.0e-02,  5.0e-02, -1.8e-01,  4.0e-02, -2.7e-01,\n",
      "         2.1e-01,  2.4e-01,  2.6e-01,  3.9e-01],\n",
      "       [ 2.6e-01,  4.2e-01,  2.5e-01,  4.9e-01,  7.0e-02,  3.4e-01,\n",
      "        -1.3e-01,  2.7e-01,  1.9e-01, -1.9e-01],\n",
      "       [-4.1e-01, -1.4e-01, -3.4e-01,  8.0e-02,  4.3e-01, -1.8e-01,\n",
      "        -4.0e-03, -1.6e-01, -3.0e-01, -3.0e-02]], dtype=float32), array([-0.05,  0.07,  0.08,  0.09, -0.03,  0.07, -0.03,  0.06, -0.05,\n",
      "       -0.03], dtype=float32), array([[-0.5 ,  0.09],\n",
      "       [ 0.27, -0.8 ],\n",
      "       [-0.1 , -0.64],\n",
      "       [-0.22, -0.44],\n",
      "       [-0.35,  0.23],\n",
      "       [ 0.43, -0.28],\n",
      "       [-0.3 ,  0.43],\n",
      "       [ 0.74, -0.77],\n",
      "       [-0.23,  0.39],\n",
      "       [-0.31,  0.5 ]], dtype=float32), array([ 0.06, -0.06], dtype=float32)]\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "andr aara h\n",
      "yhn tk\n",
      "12\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "mean_model_weights=[]\n",
    "for i in range(1):\n",
    "    mean_model_weights.append(get_avg_weights_3(add_weights[i],X_test.shape[1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 928us/step - loss: 0.1287 - accuracy: 0.9683\n",
      "[0.12874867022037506, 0.9683333039283752]\n"
     ]
    }
   ],
   "source": [
    "#mean models\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = to_categorical(dataset[target_variable])\n",
    "X = dataset.drop(columns=target_variable)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "init_model=get_initial_model_3(X_test.shape[1], 2)\n",
    "init_model.set_weights(mean_model_weights[0])\n",
    "\n",
    "print(init_model.evaluate(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.01508333,  0.075     ,  0.14916666,  0.04366667, -0.10750001,\n",
      "         0.05      , -0.20250003, -0.03058334,  0.04166666, -0.08333334],\n",
      "       [ 0.07416666,  0.13499999,  0.00875   ,  0.00118333, -0.04916666,\n",
      "        -0.00466667, -0.09      ,  0.13000001,  0.02783334, -0.17583333],\n",
      "       [ 0.05166666,  0.031     , -0.00058333, -0.07333333,  0.01975   ,\n",
      "        -0.09666666, -0.001525  , -0.022     ,  0.04925   , -0.08250001],\n",
      "       [-0.1425    ,  0.04416667, -0.10666668,  0.0775    ,  0.02708333,\n",
      "         0.0125    , -0.08583334, -0.0575    , -0.06      , -0.02566667],\n",
      "       [ 0.07125   ,  0.12083334, -0.11000001, -0.02933333, -0.10833333,\n",
      "         0.00166667, -0.03133333, -0.12333333,  0.04333333, -0.02641667],\n",
      "       [ 0.15583335,  0.14916666,  0.09833333, -0.02832917,  0.06083333,\n",
      "        -0.1975    ,  0.10833333,  0.12666667,  0.0725    ,  0.05166667],\n",
      "       [ 0.07666667,  0.08666667,  0.10583334, -0.07333333,  0.06416667,\n",
      "        -0.01633333,  0.14333333,  0.11166667,  0.03833333,  0.1425    ],\n",
      "       [ 0.08750001, -0.03583333,  0.19416668, -0.0425    ,  0.07333333,\n",
      "        -0.07833333,  0.07416667,  0.15749998,  0.03941666,  0.02958333]],\n",
      "      dtype=float32), array([ 0.11500001,  0.0625    ,  0.0825    , -0.01046667,  0.11083334,\n",
      "       -0.01875   ,  0.11416667,  0.09416667,  0.085     ,  0.0975    ],\n",
      "      dtype=float32), array([[ 0.37916663, -0.21      ,  0.39249995, -0.08917   , -0.24083333,\n",
      "         0.02241667,  0.415     ,  0.12666667,  0.28583333,  0.29083332,\n",
      "         0.14666666,  0.185     ,  0.28583333,  0.16833334,  0.41916668,\n",
      "         0.06416667,  0.36833334,  0.32666662, -0.12249999,  0.09166667],\n",
      "       [-0.06916666,  0.43999997,  0.03624583, -0.3366666 , -0.3158333 ,\n",
      "        -0.25      ,  0.16583334,  0.17083335,  0.2041667 ,  0.3891667 ,\n",
      "        -0.2708333 , -0.04333333,  0.1425    ,  0.46249998,  0.2925    ,\n",
      "        -0.18250002,  0.45583335,  0.36833334, -0.07583333,  0.3841667 ],\n",
      "       [ 0.26      , -0.37416664, -0.40833333,  0.01616667,  0.20666666,\n",
      "         0.10166667, -0.3158333 , -0.32999998,  0.35166666, -0.2575    ,\n",
      "        -0.00145833,  0.24416667,  0.45749998,  0.26833332,  0.38333336,\n",
      "        -0.4416667 , -0.34749997, -0.06416667,  0.2758333 , -0.41      ],\n",
      "       [ 0.15666667,  0.26416665, -0.2525    ,  0.04583333,  0.19666666,\n",
      "        -0.40916666, -0.15916665,  0.05016666,  0.32666662, -0.35416666,\n",
      "         0.10083333, -0.16833334, -0.035075  , -0.30916664,  0.21083336,\n",
      "        -0.01458333,  0.05333333,  0.35750005, -0.4483333 , -0.39333332],\n",
      "       [ 0.175     , -0.28083333, -0.26916665, -0.46749997,  0.23166668,\n",
      "         0.17      ,  0.19500004,  0.05858334,  0.36166665,  0.25833333,\n",
      "         0.4458333 ,  0.51916665, -0.30416664,  0.02083333,  0.3833333 ,\n",
      "        -0.09750002, -0.21750002, -0.0375    ,  0.28666663, -0.13083334],\n",
      "       [-0.36166668,  0.47833338, -0.14083333,  0.00475   , -0.34333327,\n",
      "        -0.4091667 , -0.0375    , -0.29416665,  0.16083334, -0.39166668,\n",
      "        -0.43916667, -0.21416666, -0.075     ,  0.27416667, -0.46749997,\n",
      "         0.025     , -0.23      , -0.0725    ,  0.0885    ,  0.36249998],\n",
      "       [-0.0625    , -0.16166668,  0.4091667 , -0.245     ,  0.4725    ,\n",
      "         0.15916665, -0.25083333,  0.28416666,  0.14666666,  0.41833332,\n",
      "         0.12833333, -0.16333333, -0.29749998, -0.12833333,  0.26333335,\n",
      "         0.22000001,  0.2041667 ,  0.05333334,  0.09166668,  0.04583333],\n",
      "       [ 0.09666667,  0.205     , -0.08333334,  0.001095  , -0.1875    ,\n",
      "         0.54499996, -0.4466667 , -0.11833333,  0.45166668,  0.03891667,\n",
      "         0.44333336,  0.02166667,  0.515     , -0.25916666,  0.20166667,\n",
      "         0.07916666,  0.37249997, -0.37333333,  0.1275    , -0.41833332],\n",
      "       [-0.42833337, -0.34916666, -0.0085    ,  0.42416665, -0.16666667,\n",
      "        -0.044     ,  0.06416667, -0.31166664,  0.4375    ,  0.26583332,\n",
      "        -0.33916664, -0.21583335,  0.41500005,  0.13333334,  0.18166667,\n",
      "        -0.46499994, -0.15166666,  0.07166667,  0.285     , -0.34916666],\n",
      "       [-0.2708333 ,  0.29916665,  0.105     , -0.13083334, -0.10666668,\n",
      "         0.095     ,  0.38750002, -0.28916666, -0.26333332,  0.39      ,\n",
      "         0.24666667,  0.14833334,  0.29916665,  0.44666663,  0.38583335,\n",
      "        -0.3458333 ,  0.05083333,  0.39166665,  0.3425    ,  0.2733333 ]],\n",
      "      dtype=float32), array([-0.055     ,  0.01783333, -0.02666667, -0.00666667,  0.09      ,\n",
      "        0.08166666, -0.02583333,  0.06583334,  0.06166667,  0.08416667,\n",
      "        0.06416667,  0.07916667,  0.06666666,  0.07666666,  0.06583334,\n",
      "       -0.0275    ,  0.06333334,  0.0175    ,  0.08083332, -0.04666667],\n",
      "      dtype=float32), array([[-0.05083334, -0.16583331, -0.15333332,  0.29083332, -0.37833336,\n",
      "        -0.26333332, -0.12249999, -0.20750003, -0.03583333,  0.35166666],\n",
      "       [-0.03583333, -0.00641667, -0.3108333 , -0.2525    , -0.07583333,\n",
      "        -0.34249997, -0.29416665,  0.16083333,  0.45166668, -0.18916667],\n",
      "       [ 0.11000001,  0.15416665,  0.02833334,  0.17166667, -0.36333337,\n",
      "         0.395     , -0.07416666, -0.31666663, -0.44583333, -0.20833336],\n",
      "       [ 0.08083332,  0.04      , -0.3741666 ,  0.06416667,  0.26166666,\n",
      "         0.33500004, -0.20000003, -0.36416665, -0.34166667,  0.33916664],\n",
      "       [ 0.25583333,  0.26      ,  0.26      ,  0.27416667,  0.17833334,\n",
      "        -0.06666667, -0.23083334,  0.3766667 ,  0.27999997,  0.34666666],\n",
      "       [ 0.04083333,  0.4708333 ,  0.12666667,  0.2875    ,  0.29749998,\n",
      "         0.3366666 , -0.29166666,  0.53166664,  0.07333333, -0.13166666],\n",
      "       [ 0.04249999, -0.13916665, -0.2833333 ,  0.39666668, -0.15499999,\n",
      "         0.06416667, -0.28333333,  0.14999999,  0.36416665,  0.38500002],\n",
      "       [-0.2475    ,  0.285     , -0.13416666,  0.21583335, -0.14      ,\n",
      "         0.50666666, -0.07250001,  0.35416672, -0.055     , -0.24000001],\n",
      "       [-0.43666664,  0.22083335,  0.10916667,  0.02416667, -0.26      ,\n",
      "         0.26      ,  0.01908333,  0.27249998, -0.4366667 , -0.45083335],\n",
      "       [ 0.3516666 ,  0.26      , -0.08416667,  0.13166668, -0.45249996,\n",
      "         0.3133333 ,  0.2575    ,  0.37833333,  0.25583333,  0.27833334],\n",
      "       [-0.20833336, -0.27916667, -0.12249999, -0.1475    ,  0.20833336,\n",
      "         0.48666668,  0.14499998,  0.43250003, -0.02333667, -0.47666666],\n",
      "       [-0.12583333, -0.07916667,  0.5541667 ,  0.01045833, -0.35750005,\n",
      "        -0.14999999,  0.13666666,  0.23833334,  0.28499997, -0.35916665],\n",
      "       [ 0.18166669, -0.25583333,  0.47749996,  0.08333334,  0.36999997,\n",
      "        -0.1125    , -0.38166666,  0.35333332,  0.3983333 , -0.30249998],\n",
      "       [-0.2425    ,  0.06250001,  0.07833333, -0.09916667, -0.28      ,\n",
      "         0.10583334,  0.11666667,  0.47166666,  0.3633333 , -0.02333334],\n",
      "       [ 0.04083333,  0.31666663,  0.27      ,  0.5416667 , -0.14166667,\n",
      "         0.27416667, -0.02083333,  0.5174999 , -0.47916666,  0.1275    ],\n",
      "       [-0.18833335, -0.02083333,  0.20250003,  0.27166665, -0.44333336,\n",
      "        -0.05083333,  0.04166666, -0.09083334, -0.13333333,  0.25083333],\n",
      "       [-0.26833332,  0.1225    ,  0.01366667,  0.17666668, -0.37083328,\n",
      "         0.29666665, -0.24666665,  0.26999998, -0.39166665, -0.16583334],\n",
      "       [-0.47833332,  0.03166667,  0.02916667, -0.2025    ,  0.04333333,\n",
      "        -0.2875    ,  0.22166668,  0.23      ,  0.28749996,  0.405     ],\n",
      "       [ 0.26166666,  0.40166673,  0.23166668,  0.4716666 ,  0.07666666,\n",
      "         0.32166666, -0.11583334,  0.25666666,  0.205     , -0.17833334],\n",
      "       [-0.41750002, -0.16      , -0.36083338,  0.06083333,  0.42499998,\n",
      "        -0.2       , -0.000558  , -0.17916667, -0.26833335, -0.01841667]],\n",
      "      dtype=float32), array([-0.04916668,  0.06833334,  0.07666666,  0.08416667, -0.025     ,\n",
      "        0.06500001, -0.02833333,  0.06      , -0.0425    , -0.02583333],\n",
      "      dtype=float32), array([[-0.5       ,  0.09      ],\n",
      "       [ 0.25666666, -0.7866666 ],\n",
      "       [-0.08916666, -0.6508333 ],\n",
      "       [-0.22250001, -0.4375    ],\n",
      "       [-0.36000004,  0.24      ],\n",
      "       [ 0.4275    , -0.2775    ],\n",
      "       [-0.3208333 ,  0.45083332],\n",
      "       [ 0.7383333 , -0.76833344],\n",
      "       [-0.2733333 ,  0.43333337],\n",
      "       [-0.31916663,  0.50916666]], dtype=float32), array([ 0.06, -0.06], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(mean_model_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_206 (Dense)           (None, 10)                90        \n",
      "                                                                 \n",
      " dense_207 (Dense)           (None, 20)                220       \n",
      "                                                                 \n",
      " dense_208 (Dense)           (None, 10)                210       \n",
      "                                                                 \n",
      " dense_209 (Dense)           (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 542\n",
      "Trainable params: 542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(init_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Type', 'Air temperature [K]', 'Process temperature [K]',\n",
      "       'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]',\n",
      "       'Machine failure'],\n",
      "      dtype='object')\n",
      "Index(['Air temperature [K]', 'Process temperature [K]',\n",
      "       'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]', 'Type_H',\n",
      "       'Type_L', 'Type_M', 'Machine failure'],\n",
      "      dtype='object')\n",
      "9661\n",
      "339\n"
     ]
    }
   ],
   "source": [
    "#for SKIN_NonSkin dataset\n",
    "df = pd.read_csv(\"ai4i2020.csv\",sep=';')\n",
    "#dataset.round(3)\n",
    "#print(dataset['label'])\n",
    "df_str = df.select_dtypes(include='object')\n",
    "df_int = df.select_dtypes(exclude='object')\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "#handle categorical data\n",
    "df_str = pd.get_dummies(df_str)\n",
    "\n",
    "target_variable=\"Machine failure\"\n",
    "target = df_int[target_variable]\n",
    "x = df_int.drop(columns=target_variable)\n",
    "column_names = x.columns.values\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x_stndrd = scaler.fit_transform(x)\n",
    "type(x_stndrd)\n",
    "\n",
    "x_stndrd = pd.DataFrame(x_stndrd)\n",
    "x_stndrd.columns = column_names\n",
    "\n",
    "dataset = pd.concat([x_stndrd,df_str, target],axis=1)\n",
    "print(dataset.columns)\n",
    "\n",
    "Positive=dataset[dataset[target_variable]==0]\n",
    "Negative=dataset[dataset[target_variable]==1]\n",
    "print(len(Positive))\n",
    "print(len(Negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483 17\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 14ms/step - loss: 0.6835 - f1_m: 0.7692 - val_loss: 0.6673 - val_f1_m: 0.8984\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6517 - f1_m: 0.9736 - val_loss: 0.6336 - val_f1_m: 0.8984\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6077 - f1_m: 0.9736 - val_loss: 0.5820 - val_f1_m: 0.8984\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5417 - f1_m: 0.9736 - val_loss: 0.5062 - val_f1_m: 0.8984\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4462 - f1_m: 0.9736 - val_loss: 0.4086 - val_f1_m: 0.8984\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3313 - f1_m: 0.9712 - val_loss: 0.3131 - val_f1_m: 0.8984\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2226 - f1_m: 0.9712 - val_loss: 0.2574 - val_f1_m: 0.8984\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1603 - f1_m: 0.9736 - val_loss: 0.2487 - val_f1_m: 0.8984\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1366 - f1_m: 0.9736 - val_loss: 0.2507 - val_f1_m: 0.8984\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1305 - f1_m: 0.9712 - val_loss: 0.2516 - val_f1_m: 0.8984\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1269 - f1_m: 0.9736 - val_loss: 0.2439 - val_f1_m: 0.8984\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1243 - f1_m: 0.9712 - val_loss: 0.2398 - val_f1_m: 0.8984\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1225 - f1_m: 0.9736 - val_loss: 0.2320 - val_f1_m: 0.8984\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1205 - f1_m: 0.9736 - val_loss: 0.2294 - val_f1_m: 0.8984\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1194 - f1_m: 0.9736 - val_loss: 0.2256 - val_f1_m: 0.8984\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1176 - f1_m: 0.9736 - val_loss: 0.2253 - val_f1_m: 0.8984\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1171 - f1_m: 0.9736 - val_loss: 0.2270 - val_f1_m: 0.8984\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1156 - f1_m: 0.9712 - val_loss: 0.2222 - val_f1_m: 0.8984\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1155 - f1_m: 0.9736 - val_loss: 0.2188 - val_f1_m: 0.8984\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1136 - f1_m: 0.9736 - val_loss: 0.2158 - val_f1_m: 0.8984\n",
      "0.973557710647583\n",
      "16/16 [==============================] - 0s 928us/step\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 14ms/step - loss: 0.6828 - f1_m: 0.8006 - val_loss: 0.6649 - val_f1_m: 0.9766\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6507 - f1_m: 0.9663 - val_loss: 0.6283 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6077 - f1_m: 0.9663 - val_loss: 0.5735 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5428 - f1_m: 0.9639 - val_loss: 0.4913 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4491 - f1_m: 0.9663 - val_loss: 0.3801 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3318 - f1_m: 0.9663 - val_loss: 0.2592 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2209 - f1_m: 0.9663 - val_loss: 0.1718 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1566 - f1_m: 0.9615 - val_loss: 0.1356 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1330 - f1_m: 0.9663 - val_loss: 0.1278 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1267 - f1_m: 0.9639 - val_loss: 0.1259 - val_f1_m: 0.9766\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1235 - f1_m: 0.9639 - val_loss: 0.1244 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1209 - f1_m: 0.9663 - val_loss: 0.1233 - val_f1_m: 0.9766\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1199 - f1_m: 0.9663 - val_loss: 0.1223 - val_f1_m: 0.9766\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1185 - f1_m: 0.9639 - val_loss: 0.1219 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1172 - f1_m: 0.9639 - val_loss: 0.1217 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1160 - f1_m: 0.9639 - val_loss: 0.1213 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1148 - f1_m: 0.9663 - val_loss: 0.1209 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1138 - f1_m: 0.9639 - val_loss: 0.1207 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1125 - f1_m: 0.9639 - val_loss: 0.1207 - val_f1_m: 0.9766\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1119 - f1_m: 0.9663 - val_loss: 0.1212 - val_f1_m: 0.9766\n",
      "0.9663461446762085\n",
      "16/16 [==============================] - 0s 816us/step\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6828 - f1_m: 0.8077 - val_loss: 0.6660 - val_f1_m: 0.9688\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6513 - f1_m: 0.9663 - val_loss: 0.6298 - val_f1_m: 0.9688\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6078 - f1_m: 0.9663 - val_loss: 0.5756 - val_f1_m: 0.9688\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5438 - f1_m: 0.9688 - val_loss: 0.4954 - val_f1_m: 0.9688\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4520 - f1_m: 0.9639 - val_loss: 0.3870 - val_f1_m: 0.9688\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3351 - f1_m: 0.9687 - val_loss: 0.2734 - val_f1_m: 0.9688\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2247 - f1_m: 0.9687 - val_loss: 0.1938 - val_f1_m: 0.9688\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1646 - f1_m: 0.9663 - val_loss: 0.1654 - val_f1_m: 0.9688\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1431 - f1_m: 0.9639 - val_loss: 0.1639 - val_f1_m: 0.9688\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1379 - f1_m: 0.9688 - val_loss: 0.1614 - val_f1_m: 0.9688\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1348 - f1_m: 0.9688 - val_loss: 0.1604 - val_f1_m: 0.9688\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1315 - f1_m: 0.9688 - val_loss: 0.1572 - val_f1_m: 0.9688\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1294 - f1_m: 0.9663 - val_loss: 0.1557 - val_f1_m: 0.9688\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1274 - f1_m: 0.9663 - val_loss: 0.1550 - val_f1_m: 0.9688\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1264 - f1_m: 0.9663 - val_loss: 0.1541 - val_f1_m: 0.9688\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1238 - f1_m: 0.9688 - val_loss: 0.1521 - val_f1_m: 0.9688\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1220 - f1_m: 0.9663 - val_loss: 0.1505 - val_f1_m: 0.9688\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1204 - f1_m: 0.9688 - val_loss: 0.1490 - val_f1_m: 0.9688\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1188 - f1_m: 0.9688 - val_loss: 0.1481 - val_f1_m: 0.9688\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1174 - f1_m: 0.9663 - val_loss: 0.1477 - val_f1_m: 0.9688\n",
      "0.9663461446762085\n",
      "16/16 [==============================] - 0s 874us/step\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6834 - f1_m: 0.7909 - val_loss: 0.6641 - val_f1_m: 1.0000\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6522 - f1_m: 0.9567 - val_loss: 0.6260 - val_f1_m: 1.0000\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6101 - f1_m: 0.9567 - val_loss: 0.5684 - val_f1_m: 1.0000\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5466 - f1_m: 0.9591 - val_loss: 0.4819 - val_f1_m: 1.0000\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4555 - f1_m: 0.9591 - val_loss: 0.3627 - val_f1_m: 1.0000\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3435 - f1_m: 0.9543 - val_loss: 0.2281 - val_f1_m: 1.0000\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2396 - f1_m: 0.9567 - val_loss: 0.1222 - val_f1_m: 1.0000\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1830 - f1_m: 0.9567 - val_loss: 0.0653 - val_f1_m: 1.0000\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1648 - f1_m: 0.9543 - val_loss: 0.0457 - val_f1_m: 1.0000\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1570 - f1_m: 0.9591 - val_loss: 0.0446 - val_f1_m: 1.0000\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1530 - f1_m: 0.9567 - val_loss: 0.0420 - val_f1_m: 1.0000\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1497 - f1_m: 0.9591 - val_loss: 0.0419 - val_f1_m: 1.0000\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1473 - f1_m: 0.9567 - val_loss: 0.0405 - val_f1_m: 1.0000\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1451 - f1_m: 0.9567 - val_loss: 0.0390 - val_f1_m: 1.0000\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1432 - f1_m: 0.9519 - val_loss: 0.0390 - val_f1_m: 1.0000\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1419 - f1_m: 0.9591 - val_loss: 0.0418 - val_f1_m: 1.0000\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1401 - f1_m: 0.9591 - val_loss: 0.0389 - val_f1_m: 1.0000\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1386 - f1_m: 0.9567 - val_loss: 0.0371 - val_f1_m: 1.0000\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1372 - f1_m: 0.9567 - val_loss: 0.0373 - val_f1_m: 1.0000\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1356 - f1_m: 0.9591 - val_loss: 0.0376 - val_f1_m: 1.0000\n",
      "0.9591346383094788\n",
      "16/16 [==============================] - 0s 828us/step\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6827 - f1_m: 0.8029 - val_loss: 0.6662 - val_f1_m: 0.9687\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6513 - f1_m: 0.9688 - val_loss: 0.6313 - val_f1_m: 0.9687\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6091 - f1_m: 0.9688 - val_loss: 0.5801 - val_f1_m: 0.9687\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5462 - f1_m: 0.9663 - val_loss: 0.5049 - val_f1_m: 0.9687\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4565 - f1_m: 0.9688 - val_loss: 0.4052 - val_f1_m: 0.9687\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3427 - f1_m: 0.9663 - val_loss: 0.3003 - val_f1_m: 0.9687\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2376 - f1_m: 0.9688 - val_loss: 0.2265 - val_f1_m: 0.9687\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1716 - f1_m: 0.9663 - val_loss: 0.2006 - val_f1_m: 0.9687\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1526 - f1_m: 0.9663 - val_loss: 0.1965 - val_f1_m: 0.9687\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1445 - f1_m: 0.9663 - val_loss: 0.1937 - val_f1_m: 0.9687\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1402 - f1_m: 0.9663 - val_loss: 0.1906 - val_f1_m: 0.9687\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1358 - f1_m: 0.9688 - val_loss: 0.1889 - val_f1_m: 0.9687\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1341 - f1_m: 0.9688 - val_loss: 0.1882 - val_f1_m: 0.9687\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1322 - f1_m: 0.9688 - val_loss: 0.1830 - val_f1_m: 0.9687\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1303 - f1_m: 0.9688 - val_loss: 0.1804 - val_f1_m: 0.9687\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1278 - f1_m: 0.9688 - val_loss: 0.1790 - val_f1_m: 0.9687\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1263 - f1_m: 0.9688 - val_loss: 0.1766 - val_f1_m: 0.9687\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1247 - f1_m: 0.9688 - val_loss: 0.1748 - val_f1_m: 0.9687\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1237 - f1_m: 0.9688 - val_loss: 0.1720 - val_f1_m: 0.9687\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1215 - f1_m: 0.9688 - val_loss: 0.1696 - val_f1_m: 0.9687\n",
      "0.96875\n",
      "16/16 [==============================] - 0s 908us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 14ms/step - loss: 0.6826 - f1_m: 0.7933 - val_loss: 0.6660 - val_f1_m: 0.9766\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6518 - f1_m: 0.9663 - val_loss: 0.6306 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6096 - f1_m: 0.9663 - val_loss: 0.5777 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5455 - f1_m: 0.9663 - val_loss: 0.4988 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4513 - f1_m: 0.9663 - val_loss: 0.3918 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3342 - f1_m: 0.9663 - val_loss: 0.2735 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2218 - f1_m: 0.9663 - val_loss: 0.1869 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1580 - f1_m: 0.9663 - val_loss: 0.1518 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1369 - f1_m: 0.9591 - val_loss: 0.1422 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1299 - f1_m: 0.9663 - val_loss: 0.1380 - val_f1_m: 0.9766\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1257 - f1_m: 0.9663 - val_loss: 0.1340 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1225 - f1_m: 0.9663 - val_loss: 0.1302 - val_f1_m: 0.9766\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1199 - f1_m: 0.9639 - val_loss: 0.1277 - val_f1_m: 0.9766\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1172 - f1_m: 0.9663 - val_loss: 0.1251 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1152 - f1_m: 0.9615 - val_loss: 0.1228 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1131 - f1_m: 0.9663 - val_loss: 0.1203 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1110 - f1_m: 0.9663 - val_loss: 0.1183 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1093 - f1_m: 0.9663 - val_loss: 0.1160 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1075 - f1_m: 0.9639 - val_loss: 0.1143 - val_f1_m: 0.9766\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1061 - f1_m: 0.9639 - val_loss: 0.1133 - val_f1_m: 0.9766\n",
      "0.9639422297477722\n",
      "16/16 [==============================] - 0s 964us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6832 - f1_m: 0.8005 - val_loss: 0.6666 - val_f1_m: 0.9766\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6516 - f1_m: 0.9663 - val_loss: 0.6316 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6095 - f1_m: 0.9639 - val_loss: 0.5800 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5469 - f1_m: 0.9639 - val_loss: 0.5034 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4556 - f1_m: 0.9639 - val_loss: 0.3994 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3417 - f1_m: 0.9663 - val_loss: 0.2858 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2385 - f1_m: 0.9663 - val_loss: 0.2059 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1756 - f1_m: 0.9663 - val_loss: 0.1757 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1571 - f1_m: 0.9663 - val_loss: 0.1705 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1486 - f1_m: 0.9663 - val_loss: 0.1716 - val_f1_m: 0.9766\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1436 - f1_m: 0.9663 - val_loss: 0.1683 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1398 - f1_m: 0.9663 - val_loss: 0.1683 - val_f1_m: 0.9766\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1369 - f1_m: 0.9615 - val_loss: 0.1639 - val_f1_m: 0.9766\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1338 - f1_m: 0.9663 - val_loss: 0.1625 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1320 - f1_m: 0.9615 - val_loss: 0.1619 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1300 - f1_m: 0.9663 - val_loss: 0.1568 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1273 - f1_m: 0.9663 - val_loss: 0.1552 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1258 - f1_m: 0.9639 - val_loss: 0.1524 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1239 - f1_m: 0.9639 - val_loss: 0.1509 - val_f1_m: 0.9766\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1223 - f1_m: 0.9663 - val_loss: 0.1511 - val_f1_m: 0.9766\n",
      "0.9663461446762085\n",
      "16/16 [==============================] - 0s 932us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6828 - f1_m: 0.8053 - val_loss: 0.6668 - val_f1_m: 0.9687\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6507 - f1_m: 0.9663 - val_loss: 0.6316 - val_f1_m: 0.9687\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6068 - f1_m: 0.9688 - val_loss: 0.5793 - val_f1_m: 0.9687\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5408 - f1_m: 0.9688 - val_loss: 0.5021 - val_f1_m: 0.9687\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4459 - f1_m: 0.9687 - val_loss: 0.3993 - val_f1_m: 0.9687\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3303 - f1_m: 0.9688 - val_loss: 0.2888 - val_f1_m: 0.9687\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2232 - f1_m: 0.9615 - val_loss: 0.2144 - val_f1_m: 0.9687\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1628 - f1_m: 0.9639 - val_loss: 0.1935 - val_f1_m: 0.9687\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1405 - f1_m: 0.9663 - val_loss: 0.1934 - val_f1_m: 0.9687\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1339 - f1_m: 0.9688 - val_loss: 0.1944 - val_f1_m: 0.9687\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1311 - f1_m: 0.9663 - val_loss: 0.1968 - val_f1_m: 0.9687\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1286 - f1_m: 0.9688 - val_loss: 0.1970 - val_f1_m: 0.9687\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1276 - f1_m: 0.9663 - val_loss: 0.2004 - val_f1_m: 0.9687\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1252 - f1_m: 0.9663 - val_loss: 0.1979 - val_f1_m: 0.9687\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1242 - f1_m: 0.9663 - val_loss: 0.1978 - val_f1_m: 0.9687\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1230 - f1_m: 0.9688 - val_loss: 0.1980 - val_f1_m: 0.9687\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1219 - f1_m: 0.9688 - val_loss: 0.1975 - val_f1_m: 0.9687\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1213 - f1_m: 0.9688 - val_loss: 0.1966 - val_f1_m: 0.9687\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1202 - f1_m: 0.9639 - val_loss: 0.1989 - val_f1_m: 0.9687\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1195 - f1_m: 0.9688 - val_loss: 0.1948 - val_f1_m: 0.9687\n",
      "0.96875\n",
      "16/16 [==============================] - 0s 777us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6838 - f1_m: 0.7764 - val_loss: 0.6681 - val_f1_m: 0.9609\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6524 - f1_m: 0.9712 - val_loss: 0.6354 - val_f1_m: 0.9609\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6103 - f1_m: 0.9712 - val_loss: 0.5863 - val_f1_m: 0.9609\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5464 - f1_m: 0.9712 - val_loss: 0.5133 - val_f1_m: 0.9609\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4550 - f1_m: 0.9688 - val_loss: 0.4136 - val_f1_m: 0.9609\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3386 - f1_m: 0.9712 - val_loss: 0.3075 - val_f1_m: 0.9609\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2289 - f1_m: 0.9688 - val_loss: 0.2362 - val_f1_m: 0.9609\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1629 - f1_m: 0.9712 - val_loss: 0.2141 - val_f1_m: 0.9609\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1402 - f1_m: 0.9712 - val_loss: 0.2098 - val_f1_m: 0.9609\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1323 - f1_m: 0.9712 - val_loss: 0.2058 - val_f1_m: 0.9609\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1274 - f1_m: 0.9712 - val_loss: 0.2018 - val_f1_m: 0.9609\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1242 - f1_m: 0.9688 - val_loss: 0.1979 - val_f1_m: 0.9609\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1217 - f1_m: 0.9712 - val_loss: 0.1935 - val_f1_m: 0.9609\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1198 - f1_m: 0.9688 - val_loss: 0.1917 - val_f1_m: 0.9609\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1175 - f1_m: 0.9712 - val_loss: 0.1862 - val_f1_m: 0.9609\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1163 - f1_m: 0.9688 - val_loss: 0.1844 - val_f1_m: 0.9609\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1142 - f1_m: 0.9688 - val_loss: 0.1833 - val_f1_m: 0.9609\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1128 - f1_m: 0.9712 - val_loss: 0.1803 - val_f1_m: 0.9609\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1115 - f1_m: 0.9688 - val_loss: 0.1797 - val_f1_m: 0.9609\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1098 - f1_m: 0.9663 - val_loss: 0.1770 - val_f1_m: 0.9609\n",
      "0.9663461446762085\n",
      "16/16 [==============================] - 0s 841us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6837 - f1_m: 0.7837 - val_loss: 0.6655 - val_f1_m: 0.9844\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6529 - f1_m: 0.9639 - val_loss: 0.6301 - val_f1_m: 0.9844\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6125 - f1_m: 0.9615 - val_loss: 0.5767 - val_f1_m: 0.9844\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5513 - f1_m: 0.9639 - val_loss: 0.4970 - val_f1_m: 0.9844\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4616 - f1_m: 0.9615 - val_loss: 0.3857 - val_f1_m: 0.9844\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3497 - f1_m: 0.9615 - val_loss: 0.2590 - val_f1_m: 0.9844\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2430 - f1_m: 0.9639 - val_loss: 0.1604 - val_f1_m: 0.9844\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1851 - f1_m: 0.9591 - val_loss: 0.1118 - val_f1_m: 0.9844\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1641 - f1_m: 0.9639 - val_loss: 0.0995 - val_f1_m: 0.9844\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1594 - f1_m: 0.9639 - val_loss: 0.0935 - val_f1_m: 0.9844\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1578 - f1_m: 0.9567 - val_loss: 0.0902 - val_f1_m: 0.9844\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1556 - f1_m: 0.9639 - val_loss: 0.0928 - val_f1_m: 0.9844\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1528 - f1_m: 0.9615 - val_loss: 0.0893 - val_f1_m: 0.9844\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1516 - f1_m: 0.9639 - val_loss: 0.0907 - val_f1_m: 0.9844\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1493 - f1_m: 0.9615 - val_loss: 0.0877 - val_f1_m: 0.9844\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1484 - f1_m: 0.9615 - val_loss: 0.0862 - val_f1_m: 0.9844\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1462 - f1_m: 0.9615 - val_loss: 0.0856 - val_f1_m: 0.9844\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1454 - f1_m: 0.9615 - val_loss: 0.0861 - val_f1_m: 0.9844\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1443 - f1_m: 0.9639 - val_loss: 0.0840 - val_f1_m: 0.9844\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1438 - f1_m: 0.9615 - val_loss: 0.0813 - val_f1_m: 0.9844\n",
      "0.9615384340286255\n",
      "16/16 [==============================] - 0s 912us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6840 - f1_m: 0.7692 - val_loss: 0.6697 - val_f1_m: 0.9453\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6520 - f1_m: 0.9760 - val_loss: 0.6393 - val_f1_m: 0.9453\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6104 - f1_m: 0.9736 - val_loss: 0.5936 - val_f1_m: 0.9453\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5466 - f1_m: 0.9736 - val_loss: 0.5265 - val_f1_m: 0.9453\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4528 - f1_m: 0.9736 - val_loss: 0.4356 - val_f1_m: 0.9453\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3318 - f1_m: 0.9760 - val_loss: 0.3386 - val_f1_m: 0.9453\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2167 - f1_m: 0.9712 - val_loss: 0.2785 - val_f1_m: 0.9453\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1410 - f1_m: 0.9760 - val_loss: 0.2720 - val_f1_m: 0.9453\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1144 - f1_m: 0.9760 - val_loss: 0.2846 - val_f1_m: 0.9453\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1064 - f1_m: 0.9760 - val_loss: 0.2911 - val_f1_m: 0.9453\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1039 - f1_m: 0.9736 - val_loss: 0.2946 - val_f1_m: 0.9453\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1015 - f1_m: 0.9736 - val_loss: 0.2949 - val_f1_m: 0.9453\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0996 - f1_m: 0.9736 - val_loss: 0.2887 - val_f1_m: 0.9453\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0981 - f1_m: 0.9760 - val_loss: 0.2872 - val_f1_m: 0.9453\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0969 - f1_m: 0.9760 - val_loss: 0.2856 - val_f1_m: 0.9453\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0959 - f1_m: 0.9760 - val_loss: 0.2854 - val_f1_m: 0.9453\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0947 - f1_m: 0.9760 - val_loss: 0.2835 - val_f1_m: 0.9453\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0939 - f1_m: 0.9760 - val_loss: 0.2850 - val_f1_m: 0.9453\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0931 - f1_m: 0.9760 - val_loss: 0.2806 - val_f1_m: 0.9453\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0919 - f1_m: 0.9760 - val_loss: 0.2799 - val_f1_m: 0.9453\n",
      "0.9759615659713745\n",
      "16/16 [==============================] - 0s 835us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 14ms/step - loss: 0.6833 - f1_m: 0.7933 - val_loss: 0.6654 - val_f1_m: 0.9844\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6522 - f1_m: 0.9639 - val_loss: 0.6293 - val_f1_m: 0.9844\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6091 - f1_m: 0.9639 - val_loss: 0.5749 - val_f1_m: 0.9844\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5450 - f1_m: 0.9639 - val_loss: 0.4944 - val_f1_m: 0.9844\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4527 - f1_m: 0.9615 - val_loss: 0.3865 - val_f1_m: 0.9844\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3406 - f1_m: 0.9639 - val_loss: 0.2669 - val_f1_m: 0.9844\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2356 - f1_m: 0.9615 - val_loss: 0.1770 - val_f1_m: 0.9844\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1764 - f1_m: 0.9615 - val_loss: 0.1360 - val_f1_m: 0.9844\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1625 - f1_m: 0.9591 - val_loss: 0.1240 - val_f1_m: 0.9844\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1559 - f1_m: 0.9639 - val_loss: 0.1232 - val_f1_m: 0.9844\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1535 - f1_m: 0.9639 - val_loss: 0.1217 - val_f1_m: 0.9844\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1522 - f1_m: 0.9639 - val_loss: 0.1211 - val_f1_m: 0.9844\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1503 - f1_m: 0.9615 - val_loss: 0.1200 - val_f1_m: 0.9844\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1489 - f1_m: 0.9639 - val_loss: 0.1194 - val_f1_m: 0.9844\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1476 - f1_m: 0.9639 - val_loss: 0.1197 - val_f1_m: 0.9844\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1465 - f1_m: 0.9639 - val_loss: 0.1184 - val_f1_m: 0.9844\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1459 - f1_m: 0.9639 - val_loss: 0.1173 - val_f1_m: 0.9844\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1449 - f1_m: 0.9615 - val_loss: 0.1190 - val_f1_m: 0.9844\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1438 - f1_m: 0.9591 - val_loss: 0.1187 - val_f1_m: 0.9844\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1430 - f1_m: 0.9615 - val_loss: 0.1204 - val_f1_m: 0.9844\n",
      "0.9615384340286255\n",
      "16/16 [==============================] - 0s 841us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 14ms/step - loss: 0.6834 - f1_m: 0.7764 - val_loss: 0.6645 - val_f1_m: 0.9844\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6530 - f1_m: 0.9639 - val_loss: 0.6284 - val_f1_m: 0.9844\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6116 - f1_m: 0.9639 - val_loss: 0.5722 - val_f1_m: 0.9844\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5512 - f1_m: 0.9639 - val_loss: 0.4866 - val_f1_m: 0.9844\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4620 - f1_m: 0.9639 - val_loss: 0.3712 - val_f1_m: 0.9844\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3494 - f1_m: 0.9639 - val_loss: 0.2444 - val_f1_m: 0.9844\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2413 - f1_m: 0.9639 - val_loss: 0.1436 - val_f1_m: 0.9844\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1775 - f1_m: 0.9591 - val_loss: 0.0938 - val_f1_m: 0.9844\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1531 - f1_m: 0.9639 - val_loss: 0.0807 - val_f1_m: 0.9844\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1482 - f1_m: 0.9615 - val_loss: 0.0748 - val_f1_m: 0.9844\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1447 - f1_m: 0.9615 - val_loss: 0.0727 - val_f1_m: 0.9844\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1418 - f1_m: 0.9615 - val_loss: 0.0729 - val_f1_m: 0.9844\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1406 - f1_m: 0.9639 - val_loss: 0.0742 - val_f1_m: 0.9844\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1386 - f1_m: 0.9639 - val_loss: 0.0730 - val_f1_m: 0.9844\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1368 - f1_m: 0.9639 - val_loss: 0.0725 - val_f1_m: 0.9844\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1359 - f1_m: 0.9639 - val_loss: 0.0725 - val_f1_m: 0.9844\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1342 - f1_m: 0.9615 - val_loss: 0.0717 - val_f1_m: 0.9844\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1328 - f1_m: 0.9615 - val_loss: 0.0722 - val_f1_m: 0.9844\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1313 - f1_m: 0.9639 - val_loss: 0.0716 - val_f1_m: 0.9844\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1302 - f1_m: 0.9615 - val_loss: 0.0711 - val_f1_m: 0.9844\n",
      "0.9615384340286255\n",
      "16/16 [==============================] - 0s 697us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 12ms/step - loss: 0.6835 - f1_m: 0.7668 - val_loss: 0.6670 - val_f1_m: 0.9688\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6521 - f1_m: 0.9688 - val_loss: 0.6320 - val_f1_m: 0.9688\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6095 - f1_m: 0.9639 - val_loss: 0.5804 - val_f1_m: 0.9688\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5458 - f1_m: 0.9663 - val_loss: 0.5033 - val_f1_m: 0.9688\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4540 - f1_m: 0.9663 - val_loss: 0.3971 - val_f1_m: 0.9688\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3366 - f1_m: 0.9688 - val_loss: 0.2800 - val_f1_m: 0.9688\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2231 - f1_m: 0.9663 - val_loss: 0.1922 - val_f1_m: 0.9688\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1600 - f1_m: 0.9663 - val_loss: 0.1561 - val_f1_m: 0.9688\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1317 - f1_m: 0.9639 - val_loss: 0.1465 - val_f1_m: 0.9688\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1226 - f1_m: 0.9688 - val_loss: 0.1394 - val_f1_m: 0.9688\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1182 - f1_m: 0.9688 - val_loss: 0.1349 - val_f1_m: 0.9688\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1145 - f1_m: 0.9663 - val_loss: 0.1329 - val_f1_m: 0.9688\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1127 - f1_m: 0.9688 - val_loss: 0.1301 - val_f1_m: 0.9688\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1107 - f1_m: 0.9687 - val_loss: 0.1280 - val_f1_m: 0.9688\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1093 - f1_m: 0.9663 - val_loss: 0.1261 - val_f1_m: 0.9688\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1076 - f1_m: 0.9687 - val_loss: 0.1234 - val_f1_m: 0.9688\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1062 - f1_m: 0.9688 - val_loss: 0.1228 - val_f1_m: 0.9688\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1046 - f1_m: 0.9688 - val_loss: 0.1211 - val_f1_m: 0.9688\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1034 - f1_m: 0.9663 - val_loss: 0.1200 - val_f1_m: 0.9688\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1022 - f1_m: 0.9688 - val_loss: 0.1187 - val_f1_m: 0.9688\n",
      "0.96875\n",
      "16/16 [==============================] - 0s 862us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6825 - f1_m: 0.8221 - val_loss: 0.6667 - val_f1_m: 0.9609\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6510 - f1_m: 0.9663 - val_loss: 0.6316 - val_f1_m: 0.9609\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6075 - f1_m: 0.9712 - val_loss: 0.5795 - val_f1_m: 0.9609\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5411 - f1_m: 0.9712 - val_loss: 0.5021 - val_f1_m: 0.9609\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4489 - f1_m: 0.9712 - val_loss: 0.3964 - val_f1_m: 0.9609\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3309 - f1_m: 0.9712 - val_loss: 0.2843 - val_f1_m: 0.9609\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2218 - f1_m: 0.9712 - val_loss: 0.2072 - val_f1_m: 0.9609\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1597 - f1_m: 0.9712 - val_loss: 0.1837 - val_f1_m: 0.9609\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1382 - f1_m: 0.9688 - val_loss: 0.1806 - val_f1_m: 0.9609\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1319 - f1_m: 0.9712 - val_loss: 0.1798 - val_f1_m: 0.9609\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1285 - f1_m: 0.9712 - val_loss: 0.1789 - val_f1_m: 0.9609\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1271 - f1_m: 0.9688 - val_loss: 0.1754 - val_f1_m: 0.9609\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1240 - f1_m: 0.9688 - val_loss: 0.1745 - val_f1_m: 0.9609\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1234 - f1_m: 0.9712 - val_loss: 0.1736 - val_f1_m: 0.9609\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1206 - f1_m: 0.9712 - val_loss: 0.1743 - val_f1_m: 0.9609\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1189 - f1_m: 0.9712 - val_loss: 0.1734 - val_f1_m: 0.9609\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1177 - f1_m: 0.9688 - val_loss: 0.1733 - val_f1_m: 0.9609\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1169 - f1_m: 0.9712 - val_loss: 0.1720 - val_f1_m: 0.9609\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1159 - f1_m: 0.9688 - val_loss: 0.1733 - val_f1_m: 0.9609\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1149 - f1_m: 0.9712 - val_loss: 0.1700 - val_f1_m: 0.9609\n",
      "0.9711538553237915\n",
      "16/16 [==============================] - 0s 848us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 14ms/step - loss: 0.6835 - f1_m: 0.8005 - val_loss: 0.6668 - val_f1_m: 0.9219\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6526 - f1_m: 0.9639 - val_loss: 0.6320 - val_f1_m: 0.9219\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6115 - f1_m: 0.9639 - val_loss: 0.5810 - val_f1_m: 0.9219\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5508 - f1_m: 0.9663 - val_loss: 0.5050 - val_f1_m: 0.9219\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4622 - f1_m: 0.9663 - val_loss: 0.3989 - val_f1_m: 0.9219\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3505 - f1_m: 0.9663 - val_loss: 0.2788 - val_f1_m: 0.9219\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2440 - f1_m: 0.9639 - val_loss: 0.1887 - val_f1_m: 0.9219\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1859 - f1_m: 0.9639 - val_loss: 0.1500 - val_f1_m: 0.9219\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1651 - f1_m: 0.9663 - val_loss: 0.1415 - val_f1_m: 0.9219\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1571 - f1_m: 0.9639 - val_loss: 0.1415 - val_f1_m: 0.9219\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1525 - f1_m: 0.9639 - val_loss: 0.1423 - val_f1_m: 0.9219\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1492 - f1_m: 0.9663 - val_loss: 0.1412 - val_f1_m: 0.9219\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1465 - f1_m: 0.9639 - val_loss: 0.1412 - val_f1_m: 0.9219\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1449 - f1_m: 0.9639 - val_loss: 0.1407 - val_f1_m: 0.9219\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1418 - f1_m: 0.9639 - val_loss: 0.1410 - val_f1_m: 0.9219\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1394 - f1_m: 0.9663 - val_loss: 0.1403 - val_f1_m: 0.9219\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1372 - f1_m: 0.9663 - val_loss: 0.1396 - val_f1_m: 0.9219\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1359 - f1_m: 0.9663 - val_loss: 0.1387 - val_f1_m: 0.9219\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1344 - f1_m: 0.9639 - val_loss: 0.1379 - val_f1_m: 0.9219\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1327 - f1_m: 0.9663 - val_loss: 0.1386 - val_f1_m: 0.9219\n",
      "0.9663461446762085\n",
      "16/16 [==============================] - 0s 859us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 12ms/step - loss: 0.6837 - f1_m: 0.7861 - val_loss: 0.6649 - val_f1_m: 0.9922\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6526 - f1_m: 0.9615 - val_loss: 0.6273 - val_f1_m: 0.9922\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6108 - f1_m: 0.9591 - val_loss: 0.5702 - val_f1_m: 0.9922\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5471 - f1_m: 0.9615 - val_loss: 0.4843 - val_f1_m: 0.9922\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4572 - f1_m: 0.9519 - val_loss: 0.3667 - val_f1_m: 0.9922\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3421 - f1_m: 0.9615 - val_loss: 0.2403 - val_f1_m: 0.9922\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2371 - f1_m: 0.9591 - val_loss: 0.1399 - val_f1_m: 0.9922\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1742 - f1_m: 0.9615 - val_loss: 0.0939 - val_f1_m: 0.9922\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1563 - f1_m: 0.9615 - val_loss: 0.0785 - val_f1_m: 0.9922\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1520 - f1_m: 0.9591 - val_loss: 0.0734 - val_f1_m: 0.9922\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1490 - f1_m: 0.9615 - val_loss: 0.0725 - val_f1_m: 0.9922\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1470 - f1_m: 0.9615 - val_loss: 0.0715 - val_f1_m: 0.9922\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1448 - f1_m: 0.9615 - val_loss: 0.0708 - val_f1_m: 0.9922\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1435 - f1_m: 0.9543 - val_loss: 0.0694 - val_f1_m: 0.9922\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1420 - f1_m: 0.9591 - val_loss: 0.0719 - val_f1_m: 0.9922\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1407 - f1_m: 0.9615 - val_loss: 0.0703 - val_f1_m: 0.9922\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1396 - f1_m: 0.9591 - val_loss: 0.0688 - val_f1_m: 0.9922\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1390 - f1_m: 0.9591 - val_loss: 0.0689 - val_f1_m: 0.9922\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1380 - f1_m: 0.9615 - val_loss: 0.0691 - val_f1_m: 0.9922\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1374 - f1_m: 0.9591 - val_loss: 0.0675 - val_f1_m: 0.9922\n",
      "0.9591346383094788\n",
      "16/16 [==============================] - 0s 748us/step\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 14ms/step - loss: 0.6825 - f1_m: 0.7981 - val_loss: 0.6662 - val_f1_m: 0.9766\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6514 - f1_m: 0.9663 - val_loss: 0.6304 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6091 - f1_m: 0.9663 - val_loss: 0.5773 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5455 - f1_m: 0.9639 - val_loss: 0.4983 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4537 - f1_m: 0.9663 - val_loss: 0.3895 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3397 - f1_m: 0.9663 - val_loss: 0.2662 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2342 - f1_m: 0.9615 - val_loss: 0.1725 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1728 - f1_m: 0.9639 - val_loss: 0.1333 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1565 - f1_m: 0.9615 - val_loss: 0.1206 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1508 - f1_m: 0.9615 - val_loss: 0.1160 - val_f1_m: 0.9766\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1485 - f1_m: 0.9663 - val_loss: 0.1136 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1468 - f1_m: 0.9663 - val_loss: 0.1109 - val_f1_m: 0.9766\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1453 - f1_m: 0.9663 - val_loss: 0.1084 - val_f1_m: 0.9766\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1441 - f1_m: 0.9567 - val_loss: 0.1062 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1425 - f1_m: 0.9615 - val_loss: 0.1045 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1411 - f1_m: 0.9663 - val_loss: 0.1049 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1400 - f1_m: 0.9663 - val_loss: 0.1016 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1387 - f1_m: 0.9639 - val_loss: 0.1002 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1379 - f1_m: 0.9663 - val_loss: 0.0979 - val_f1_m: 0.9766\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1371 - f1_m: 0.9663 - val_loss: 0.0967 - val_f1_m: 0.9766\n",
      "0.9663461446762085\n",
      "16/16 [==============================] - 0s 870us/step\n",
      "if any\n",
      "483 17\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkv/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6827 - f1_m: 0.7857 - val_loss: 0.6659 - val_f1_m: 0.9766\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6510 - f1_m: 0.9688 - val_loss: 0.6299 - val_f1_m: 0.9766\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6078 - f1_m: 0.9687 - val_loss: 0.5762 - val_f1_m: 0.9766\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5433 - f1_m: 0.9633 - val_loss: 0.4964 - val_f1_m: 0.9766\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4504 - f1_m: 0.9660 - val_loss: 0.3885 - val_f1_m: 0.9766\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3354 - f1_m: 0.9688 - val_loss: 0.2684 - val_f1_m: 0.9766\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2305 - f1_m: 0.9688 - val_loss: 0.1792 - val_f1_m: 0.9766\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1653 - f1_m: 0.9688 - val_loss: 0.1456 - val_f1_m: 0.9766\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1426 - f1_m: 0.9688 - val_loss: 0.1379 - val_f1_m: 0.9766\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1367 - f1_m: 0.9688 - val_loss: 0.1370 - val_f1_m: 0.9766\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1327 - f1_m: 0.9688 - val_loss: 0.1368 - val_f1_m: 0.9766\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1283 - f1_m: 0.9688 - val_loss: 0.1358 - val_f1_m: 0.9766\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1266 - f1_m: 0.9688 - val_loss: 0.1357 - val_f1_m: 0.9766\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1243 - f1_m: 0.9660 - val_loss: 0.1347 - val_f1_m: 0.9766\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1225 - f1_m: 0.9688 - val_loss: 0.1349 - val_f1_m: 0.9766\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1211 - f1_m: 0.9688 - val_loss: 0.1341 - val_f1_m: 0.9766\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1196 - f1_m: 0.9688 - val_loss: 0.1345 - val_f1_m: 0.9766\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1179 - f1_m: 0.9660 - val_loss: 0.1341 - val_f1_m: 0.9766\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1160 - f1_m: 0.9688 - val_loss: 0.1338 - val_f1_m: 0.9766\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1147 - f1_m: 0.9688 - val_loss: 0.1339 - val_f1_m: 0.9766\n",
      "0.96875\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "if any\n",
      "10\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "All Done\n"
     ]
    }
   ],
   "source": [
    "#now for ANN-4\n",
    "#now trying for aNN-2 get-initial_model2\n",
    "#the below code is for binary classification since MNSIT is a multi-class low-resolution image dataset. I am updating the code in the next cell.\n",
    "N=500\n",
    "#Positive=Pos\n",
    "#Negative=Neg\n",
    "positiveN=int((Positive.shape[0]/dataset.shape[0])*N)\n",
    "negativeN=int(N-positiveN)\n",
    "print(positiveN, negativeN)\n",
    "#target variable\n",
    "#target_variable=\"default.payment.next.month\"\n",
    "df1=Positive.sample(positiveN)\n",
    "Positive.drop(df1.index, inplace=True)\n",
    "df2=Negative.sample(negativeN)\n",
    "Negative.drop(df2.index, inplace=True)\n",
    "test_data=df1.append(df2, ignore_index=True)\n",
    "test_data=test_data.sample(frac = 1) #This is to shuffel the training and testing data\n",
    "test_data=test_data.sample(frac = 1)\n",
    "test_data=test_data.sample(frac = 1)\n",
    "X_test=test_data.drop(columns=[target_variable])\n",
    "y_test=to_categorical(test_data[target_variable])\n",
    "\n",
    "# adding dense layer\n",
    "initial_model= get_initial_model_4(X_test.shape[1], 2)\n",
    "initial_model.set_weights(update_weights(initial_model.get_weights()))\n",
    "Models=[]\n",
    "val_acc=[]\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "val_loss=[]\n",
    "train_loss=[]\n",
    "add_weights=[]\n",
    "while Positive.empty==False and Negative.empty==False:\n",
    "  print(positiveN, negativeN)\n",
    "  df1=Positive.sample(min(positiveN, len(Positive)))\n",
    "  Positive.drop(df1.index, inplace=True)\n",
    "  df2=Negative.sample(min(negativeN, len(Negative)))\n",
    "  Negative.drop(df2.index, inplace=True)\n",
    "  train_data=df1.append(df2, ignore_index=True)\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "    \n",
    "  #all models have different initialization\n",
    "  # define the sequential model\n",
    "  \"\"\"initial_model = keras.Sequential()\n",
    "\n",
    "    # adding dense layer\n",
    "  initial_model.add(Dense(5, input_dim=X_test.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "  initial_model.add(Dense(10, activation='relu'))\n",
    "  initial_model.add(Dense(5, activation='relu'))\n",
    "\n",
    "    # adding dense layer with softmax activation/output layer\n",
    "  initial_model.add(Dense(2, activation='softmax'))\n",
    "  #initial_model.summary()\"\"\"\n",
    "  ann_model=get_initial_model_4(X_test.shape[1], 2) #same intial weights\n",
    "  ann_model.set_weights(initial_model.get_weights())\n",
    "  X_train=train_data.drop(columns=[target_variable])\n",
    "  #train_data[target_variable]=train_data[target_variable]-1 #only for skin_nonskin dataset\n",
    "  y_train=to_categorical(train_data[target_variable])\n",
    "  #print(y_train)\n",
    "  ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_m]) # metrics=['accuracy']\n",
    "  history = ann_model.fit(X_train, y_train, epochs=20, validation_split=0.2, verbose=1)\n",
    "  print(history.history['f1_m'][-1])\n",
    "  ann_model.set_weights(update_weights(ann_model.get_weights()))\n",
    "  pred_test=ann_model.predict(X_test)\n",
    "  present=False\n",
    "  for i in range(len(Models)):\n",
    "    if (check_models(Models[i][0], ann_model.get_weights())):\n",
    "      print(\"if any\")\n",
    "      Models[i][1]=Models[i][1]+1\n",
    "      add_weights[i].append(ann_model.get_weights())\n",
    "      val_acc[i].append(history.history['val_f1_m'])\n",
    "      train_acc[i].append(history.history['f1_m'])\n",
    "      test_acc[i].append(f1_m(y_test, pred_test).numpy())\n",
    "      val_loss[i].append(history.history['val_loss'])\n",
    "      train_loss[i].append(history.history['loss'])\n",
    "      present=True\n",
    "      break;\n",
    "  if present==False:\n",
    "    add_weights.append([ann_model.get_weights()])\n",
    "    Models.append([ann_model.get_weights(), 1])\n",
    "    val_acc.append([history.history['val_f1_m']])\n",
    "    train_acc.append([history.history['f1_m']])\n",
    "    test_acc.append([f1_m(y_test, pred_test).numpy()])\n",
    "    val_loss.append([history.history['val_loss']])\n",
    "    train_loss.append([history.history['loss']])\n",
    "for i in range(len(Models)):\n",
    "  print(Models[i][1])\n",
    "print(\"All Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(add_weights[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to average out all the models in the epsilon range\n",
    "#the problem is different here than compared with model comparison. Shape not important.\n",
    "def get_avg_weights_4(models_weights, inp_shape, out_shape):\n",
    "    avg_sum=get_initial_model_4(inp_shape, out_shape).get_weights()\n",
    "    #print(avg_sum)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                for k in range(len(avg_sum[i][j])):\n",
    "                    avg_sum[i][j][k]=0\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=0\n",
    "    #print(avg_sum)\n",
    "    print(models_weights[0])\n",
    "    for i in range(len(models_weights)):\n",
    "        for j in range(0, len(avg_sum),2):\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #if(isinstance(avg_sum[j][0], np.ndarray)):\n",
    "            if (j+2<=len(avg_sum)):\n",
    "                for k in range(len(avg_sum[j])):\n",
    "                    for l in range(len(avg_sum[j][k])):\n",
    "                        print(models_weights[i][j][k][l])\n",
    "                        avg_sum[j][k][l]=avg_sum[j][k][l]+models_weights[i][j][k][l]\n",
    "                #print(isinstance(avg_sum[j], np.ndarray))\n",
    "                #else: gayab kr diya\n",
    "                print('andr aara h')\n",
    "                for k in range(len(avg_sum[j+1])):\n",
    "                    avg_sum[j+1][k]=avg_sum[j+1][k]+models_weights[i][j+1][k]\n",
    "    print(\"yhn tk\")\n",
    "    mean_size=len(models_weights)\n",
    "    print(mean_size)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                #print(\"yhn tk\")\n",
    "                avg_sum[i][j]=[avg_sum[i][j][k]/mean_size for k in range(len(avg_sum[i][j]))]\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=avg_sum[i+1][j]/mean_size\n",
    "    print(\"Done\")\n",
    "    return avg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.13 , -0.07 , -0.1  , -0.07 ,  0.03 ],\n",
      "       [-0.14 , -0.11 , -0.22 , -0.17 ,  0.1  ],\n",
      "       [ 0.001,  0.07 , -0.05 , -0.04 , -0.09 ],\n",
      "       [-0.03 , -0.08 ,  0.04 , -0.11 , -0.08 ],\n",
      "       [-0.06 ,  0.08 , -0.03 , -0.06 , -0.13 ],\n",
      "       [ 0.07 ,  0.16 ,  0.24 ,  0.15 ,  0.13 ],\n",
      "       [ 0.12 ,  0.14 ,  0.1  ,  0.2  ,  0.14 ],\n",
      "       [ 0.05 ,  0.08 ,  0.06 ,  0.09 ,  0.12 ]], dtype=float32), array([0.1 , 0.14, 0.11, 0.16, 0.13], dtype=float32), array([[-0.01  , -0.25  ,  0.12  , -0.08  , -0.01  , -0.23  , -0.19  ,\n",
      "         0.29  ,  0.35  ,  0.16  ],\n",
      "       [-0.02  ,  0.01  ,  0.38  , -0.15  , -0.24  ,  0.32  ,  0.5   ,\n",
      "        -0.24  ,  0.63  ,  0.09  ],\n",
      "       [-0.27  ,  0.12  ,  0.27  ,  0.54  , -0.03  , -0.14  , -0.33  ,\n",
      "         0.57  ,  0.73  ,  0.29  ],\n",
      "       [ 0.23  , -0.56  ,  0.79  , -0.19  , -0.07  ,  0.15  ,  0.69  ,\n",
      "        -0.0008, -0.23  , -0.51  ],\n",
      "       [-0.07  ,  0.16  ,  0.41  ,  0.67  ,  0.66  , -0.51  ,  0.68  ,\n",
      "         0.63  ,  0.3   ,  0.51  ]], dtype=float32), array([ 0.15, -0.04,  0.13,  0.12,  0.11, -0.01,  0.11,  0.08,  0.11,\n",
      "       -0.04], dtype=float32), array([[ 0.04, -0.15,  0.19,  0.38,  0.14, -0.26, -0.25,  0.13, -0.13,\n",
      "        -0.03, -0.4 ,  0.54, -0.39,  0.29,  0.03,  0.1 , -0.28,  0.5 ,\n",
      "         0.37, -0.21],\n",
      "       [ 0.13,  0.22,  0.02,  0.16, -0.39,  0.22,  0.09,  0.31,  0.23,\n",
      "         0.05,  0.26, -0.09, -0.38,  0.12,  0.01, -0.23,  0.19,  0.28,\n",
      "        -0.31, -0.31],\n",
      "       [ 0.08,  0.05,  0.35,  0.35,  0.53,  0.5 , -0.22,  0.36, -0.42,\n",
      "        -0.16, -0.38,  0.03, -0.42,  0.45, -0.15,  0.22,  0.05, -0.06,\n",
      "         0.25,  0.56],\n",
      "       [ 0.08,  0.22, -0.28, -0.22,  0.47,  0.1 , -0.01, -0.24, -0.17,\n",
      "         0.32,  0.14,  0.41,  0.2 ,  0.24, -0.22,  0.36, -0.31,  0.14,\n",
      "         0.32,  0.36],\n",
      "       [ 0.36,  0.04, -0.24,  0.47, -0.17, -0.32,  0.01,  0.44,  0.12,\n",
      "         0.23, -0.35, -0.3 , -0.29,  0.58,  0.49,  0.4 , -0.24,  0.44,\n",
      "        -0.18, -0.04],\n",
      "       [-0.31, -0.2 , -0.44, -0.29, -0.24,  0.01,  0.41, -0.07,  0.21,\n",
      "         0.07,  0.12,  0.06, -0.3 ,  0.11, -0.28, -0.17, -0.29, -0.28,\n",
      "        -0.44, -0.44],\n",
      "       [ 0.41,  0.41,  0.08,  0.12,  0.14, -0.17, -0.23,  0.56, -0.38,\n",
      "         0.45,  0.45,  0.25, -0.33,  0.22, -0.19, -0.19, -0.26,  0.11,\n",
      "        -0.14,  0.05],\n",
      "       [-0.18,  0.2 ,  0.4 ,  0.48, -0.07,  0.17, -0.11,  0.28, -0.03,\n",
      "        -0.03, -0.28,  0.4 , -0.31,  0.24,  0.46,  0.25,  0.36,  0.36,\n",
      "        -0.27,  0.47],\n",
      "       [ 0.51,  0.32,  0.09,  0.52, -0.03,  0.05,  0.2 ,  0.52, -0.15,\n",
      "        -0.34,  0.34, -0.06, -0.11,  0.1 ,  0.38,  0.36, -0.31, -0.05,\n",
      "         0.37,  0.16],\n",
      "       [-0.36, -0.37, -0.36,  0.33, -0.02, -0.01, -0.43,  0.27, -0.21,\n",
      "         0.35, -0.36,  0.12,  0.4 , -0.19,  0.15,  0.06,  0.21,  0.47,\n",
      "         0.09, -0.08]], dtype=float32), array([ 0.11,  0.1 ,  0.11,  0.07,  0.11,  0.06, -0.01,  0.09,  0.  ,\n",
      "        0.1 , -0.02,  0.09,  0.  ,  0.1 ,  0.05,  0.11, -0.02,  0.11,\n",
      "        0.12,  0.1 ], dtype=float32), array([[ 0.33  ,  0.07  , -0.11  ,  0.23  , -0.09  ,  0.27  , -0.28  ,\n",
      "         0.39  ,  0.41  ,  0.51  ],\n",
      "       [ 0.19  , -0.37  , -0.44  ,  0.5   , -0.34  ,  0.01  , -0.21  ,\n",
      "        -0.23  ,  0.32  ,  0.24  ],\n",
      "       [ 0.52  ,  0.15  , -0.22  ,  0.22  , -0.02  , -0.19  ,  0.1   ,\n",
      "         0.2   ,  0.27  ,  0.35  ],\n",
      "       [-0.15  ,  0.39  ,  0.02  , -0.16  ,  0.15  ,  0.44  , -0.19  ,\n",
      "        -0.13  ,  0.55  , -0.21  ],\n",
      "       [ 0.58  , -0.3   , -0.14  ,  0.19  , -0.27  ,  0.42  ,  0.29  ,\n",
      "         0.23  , -0.25  ,  0.35  ],\n",
      "       [ 0.45  , -0.3   ,  0.2   ,  0.06  ,  0.37  , -0.13  , -0.02  ,\n",
      "        -0.33  ,  0.01  ,  0.43  ],\n",
      "       [-0.27  , -0.35  ,  0.05  , -0.43  , -0.27  , -0.03  ,  0.3   ,\n",
      "         0.12  ,  0.43  , -0.4   ],\n",
      "       [ 0.56  ,  0.39  ,  0.31  , -0.16  , -0.32  ,  0.54  ,  0.22  ,\n",
      "        -0.14  ,  0.22  ,  0.26  ],\n",
      "       [-0.01  ,  0.41  ,  0.18  , -0.18  ,  0.32  , -0.37  , -0.004 ,\n",
      "        -0.04  , -0.04  ,  0.13  ],\n",
      "       [ 0.54  ,  0.26  ,  0.43  ,  0.06  ,  0.29  ,  0.24  ,  0.23  ,\n",
      "        -0.06  ,  0.33  ,  0.03  ],\n",
      "       [-0.03  ,  0.18  ,  0.06  , -0.07  ,  0.01  , -0.48  , -0.24  ,\n",
      "         0.32  ,  0.14  , -0.21  ],\n",
      "       [-0.15  ,  0.17  , -0.18  , -0.01  , -0.32  ,  0.36  , -0.28  ,\n",
      "        -0.16  ,  0.03  ,  0.46  ],\n",
      "       [-0.23  ,  0.44  , -0.12  ,  0.04  , -0.32  , -0.16  , -0.23  ,\n",
      "        -0.42  , -0.36  , -0.25  ],\n",
      "       [ 0.48  , -0.32  ,  0.01  ,  0.15  ,  0.23  ,  0.31  , -0.28  ,\n",
      "        -0.26  ,  0.28  , -0.03  ],\n",
      "       [ 0.22  ,  0.1   ,  0.41  , -0.06  , -0.02  , -0.15  , -0.32  ,\n",
      "         0.34  ,  0.47  ,  0.05  ],\n",
      "       [ 0.38  ,  0.09  , -0.36  ,  0.46  ,  0.18  ,  0.35  ,  0.11  ,\n",
      "         0.31  , -0.11  ,  0.46  ],\n",
      "       [ 0.29  , -0.01  ,  0.41  , -0.04  ,  0.24  , -0.29  , -0.06  ,\n",
      "         0.21  ,  0.12  ,  0.03  ],\n",
      "       [ 0.0008, -0.27  , -0.12  ,  0.27  , -0.45  ,  0.39  ,  0.32  ,\n",
      "        -0.04  , -0.01  , -0.13  ],\n",
      "       [ 0.19  , -0.34  ,  0.01  , -0.11  ,  0.1   ,  0.44  ,  0.11  ,\n",
      "         0.38  ,  0.36  ,  0.08  ],\n",
      "       [ 0.33  , -0.26  ,  0.04  ,  0.23  ,  0.09  , -0.17  ,  0.17  ,\n",
      "        -0.44  ,  0.38  ,  0.51  ]], dtype=float32), array([ 0.1 , -0.03, -0.02,  0.11,  0.01,  0.1 , -0.05, -0.03,  0.08,\n",
      "        0.07], dtype=float32), array([[-0.4   ,  0.35  , -0.4   , -0.2   ,  0.14  ],\n",
      "       [-0.31  , -0.6   , -0.15  ,  0.54  , -0.15  ],\n",
      "       [-0.16  , -0.44  ,  0.3   , -0.02  , -0.23  ],\n",
      "       [ 0.1   ,  0.23  ,  0.49  ,  0.15  , -0.09  ],\n",
      "       [ 0.46  , -0.01  ,  0.48  ,  0.27  , -0.59  ],\n",
      "       [ 0.54  ,  0.44  , -0.33  , -0.23  ,  0.1   ],\n",
      "       [-0.03  , -0.15  , -0.55  , -0.29  ,  0.19  ],\n",
      "       [ 0.14  , -0.12  ,  0.08  , -0.17  , -0.0009],\n",
      "       [-0.41  ,  0.46  ,  0.31  ,  0.2   ,  0.28  ],\n",
      "       [-0.12  ,  0.09  ,  0.27  ,  0.22  , -0.54  ]], dtype=float32), array([-0.01,  0.09, -0.01, -0.01, -0.06], dtype=float32), array([[-0.08,  0.22],\n",
      "       [ 0.69, -0.34],\n",
      "       [ 0.02,  0.68],\n",
      "       [-0.65, -0.37],\n",
      "       [-0.11,  0.43]], dtype=float32), array([ 0.08, -0.08], dtype=float32)]\n",
      "-0.13\n",
      "-0.07\n",
      "-0.1\n",
      "-0.07\n",
      "0.03\n",
      "-0.14\n",
      "-0.11\n",
      "-0.22\n",
      "-0.17\n",
      "0.1\n",
      "0.001\n",
      "0.07\n",
      "-0.05\n",
      "-0.04\n",
      "-0.09\n",
      "-0.03\n",
      "-0.08\n",
      "0.04\n",
      "-0.11\n",
      "-0.08\n",
      "-0.06\n",
      "0.08\n",
      "-0.03\n",
      "-0.06\n",
      "-0.13\n",
      "0.07\n",
      "0.16\n",
      "0.24\n",
      "0.15\n",
      "0.13\n",
      "0.12\n",
      "0.14\n",
      "0.1\n",
      "0.2\n",
      "0.14\n",
      "0.05\n",
      "0.08\n",
      "0.06\n",
      "0.09\n",
      "0.12\n",
      "andr aara h\n",
      "-0.01\n",
      "-0.25\n",
      "0.12\n",
      "-0.08\n",
      "-0.01\n",
      "-0.23\n",
      "-0.19\n",
      "0.29\n",
      "0.35\n",
      "0.16\n",
      "-0.02\n",
      "0.01\n",
      "0.38\n",
      "-0.15\n",
      "-0.24\n",
      "0.32\n",
      "0.5\n",
      "-0.24\n",
      "0.63\n",
      "0.09\n",
      "-0.27\n",
      "0.12\n",
      "0.27\n",
      "0.54\n",
      "-0.03\n",
      "-0.14\n",
      "-0.33\n",
      "0.57\n",
      "0.73\n",
      "0.29\n",
      "0.23\n",
      "-0.56\n",
      "0.79\n",
      "-0.19\n",
      "-0.07\n",
      "0.15\n",
      "0.69\n",
      "-0.0008\n",
      "-0.23\n",
      "-0.51\n",
      "-0.07\n",
      "0.16\n",
      "0.41\n",
      "0.67\n",
      "0.66\n",
      "-0.51\n",
      "0.68\n",
      "0.63\n",
      "0.3\n",
      "0.51\n",
      "andr aara h\n",
      "0.04\n",
      "-0.15\n",
      "0.19\n",
      "0.38\n",
      "0.14\n",
      "-0.26\n",
      "-0.25\n",
      "0.13\n",
      "-0.13\n",
      "-0.03\n",
      "-0.4\n",
      "0.54\n",
      "-0.39\n",
      "0.29\n",
      "0.03\n",
      "0.1\n",
      "-0.28\n",
      "0.5\n",
      "0.37\n",
      "-0.21\n",
      "0.13\n",
      "0.22\n",
      "0.02\n",
      "0.16\n",
      "-0.39\n",
      "0.22\n",
      "0.09\n",
      "0.31\n",
      "0.23\n",
      "0.05\n",
      "0.26\n",
      "-0.09\n",
      "-0.38\n",
      "0.12\n",
      "0.01\n",
      "-0.23\n",
      "0.19\n",
      "0.28\n",
      "-0.31\n",
      "-0.31\n",
      "0.08\n",
      "0.05\n",
      "0.35\n",
      "0.35\n",
      "0.53\n",
      "0.5\n",
      "-0.22\n",
      "0.36\n",
      "-0.42\n",
      "-0.16\n",
      "-0.38\n",
      "0.03\n",
      "-0.42\n",
      "0.45\n",
      "-0.15\n",
      "0.22\n",
      "0.05\n",
      "-0.06\n",
      "0.25\n",
      "0.56\n",
      "0.08\n",
      "0.22\n",
      "-0.28\n",
      "-0.22\n",
      "0.47\n",
      "0.1\n",
      "-0.01\n",
      "-0.24\n",
      "-0.17\n",
      "0.32\n",
      "0.14\n",
      "0.41\n",
      "0.2\n",
      "0.24\n",
      "-0.22\n",
      "0.36\n",
      "-0.31\n",
      "0.14\n",
      "0.32\n",
      "0.36\n",
      "0.36\n",
      "0.04\n",
      "-0.24\n",
      "0.47\n",
      "-0.17\n",
      "-0.32\n",
      "0.01\n",
      "0.44\n",
      "0.12\n",
      "0.23\n",
      "-0.35\n",
      "-0.3\n",
      "-0.29\n",
      "0.58\n",
      "0.49\n",
      "0.4\n",
      "-0.24\n",
      "0.44\n",
      "-0.18\n",
      "-0.04\n",
      "-0.31\n",
      "-0.2\n",
      "-0.44\n",
      "-0.29\n",
      "-0.24\n",
      "0.01\n",
      "0.41\n",
      "-0.07\n",
      "0.21\n",
      "0.07\n",
      "0.12\n",
      "0.06\n",
      "-0.3\n",
      "0.11\n",
      "-0.28\n",
      "-0.17\n",
      "-0.29\n",
      "-0.28\n",
      "-0.44\n",
      "-0.44\n",
      "0.41\n",
      "0.41\n",
      "0.08\n",
      "0.12\n",
      "0.14\n",
      "-0.17\n",
      "-0.23\n",
      "0.56\n",
      "-0.38\n",
      "0.45\n",
      "0.45\n",
      "0.25\n",
      "-0.33\n",
      "0.22\n",
      "-0.19\n",
      "-0.19\n",
      "-0.26\n",
      "0.11\n",
      "-0.14\n",
      "0.05\n",
      "-0.18\n",
      "0.2\n",
      "0.4\n",
      "0.48\n",
      "-0.07\n",
      "0.17\n",
      "-0.11\n",
      "0.28\n",
      "-0.03\n",
      "-0.03\n",
      "-0.28\n",
      "0.4\n",
      "-0.31\n",
      "0.24\n",
      "0.46\n",
      "0.25\n",
      "0.36\n",
      "0.36\n",
      "-0.27\n",
      "0.47\n",
      "0.51\n",
      "0.32\n",
      "0.09\n",
      "0.52\n",
      "-0.03\n",
      "0.05\n",
      "0.2\n",
      "0.52\n",
      "-0.15\n",
      "-0.34\n",
      "0.34\n",
      "-0.06\n",
      "-0.11\n",
      "0.1\n",
      "0.38\n",
      "0.36\n",
      "-0.31\n",
      "-0.05\n",
      "0.37\n",
      "0.16\n",
      "-0.36\n",
      "-0.37\n",
      "-0.36\n",
      "0.33\n",
      "-0.02\n",
      "-0.01\n",
      "-0.43\n",
      "0.27\n",
      "-0.21\n",
      "0.35\n",
      "-0.36\n",
      "0.12\n",
      "0.4\n",
      "-0.19\n",
      "0.15\n",
      "0.06\n",
      "0.21\n",
      "0.47\n",
      "0.09\n",
      "-0.08\n",
      "andr aara h\n",
      "0.33\n",
      "0.07\n",
      "-0.11\n",
      "0.23\n",
      "-0.09\n",
      "0.27\n",
      "-0.28\n",
      "0.39\n",
      "0.41\n",
      "0.51\n",
      "0.19\n",
      "-0.37\n",
      "-0.44\n",
      "0.5\n",
      "-0.34\n",
      "0.01\n",
      "-0.21\n",
      "-0.23\n",
      "0.32\n",
      "0.24\n",
      "0.52\n",
      "0.15\n",
      "-0.22\n",
      "0.22\n",
      "-0.02\n",
      "-0.19\n",
      "0.1\n",
      "0.2\n",
      "0.27\n",
      "0.35\n",
      "-0.15\n",
      "0.39\n",
      "0.02\n",
      "-0.16\n",
      "0.15\n",
      "0.44\n",
      "-0.19\n",
      "-0.13\n",
      "0.55\n",
      "-0.21\n",
      "0.58\n",
      "-0.3\n",
      "-0.14\n",
      "0.19\n",
      "-0.27\n",
      "0.42\n",
      "0.29\n",
      "0.23\n",
      "-0.25\n",
      "0.35\n",
      "0.45\n",
      "-0.3\n",
      "0.2\n",
      "0.06\n",
      "0.37\n",
      "-0.13\n",
      "-0.02\n",
      "-0.33\n",
      "0.01\n",
      "0.43\n",
      "-0.27\n",
      "-0.35\n",
      "0.05\n",
      "-0.43\n",
      "-0.27\n",
      "-0.03\n",
      "0.3\n",
      "0.12\n",
      "0.43\n",
      "-0.4\n",
      "0.56\n",
      "0.39\n",
      "0.31\n",
      "-0.16\n",
      "-0.32\n",
      "0.54\n",
      "0.22\n",
      "-0.14\n",
      "0.22\n",
      "0.26\n",
      "-0.01\n",
      "0.41\n",
      "0.18\n",
      "-0.18\n",
      "0.32\n",
      "-0.37\n",
      "-0.004\n",
      "-0.04\n",
      "-0.04\n",
      "0.13\n",
      "0.54\n",
      "0.26\n",
      "0.43\n",
      "0.06\n",
      "0.29\n",
      "0.24\n",
      "0.23\n",
      "-0.06\n",
      "0.33\n",
      "0.03\n",
      "-0.03\n",
      "0.18\n",
      "0.06\n",
      "-0.07\n",
      "0.01\n",
      "-0.48\n",
      "-0.24\n",
      "0.32\n",
      "0.14\n",
      "-0.21\n",
      "-0.15\n",
      "0.17\n",
      "-0.18\n",
      "-0.01\n",
      "-0.32\n",
      "0.36\n",
      "-0.28\n",
      "-0.16\n",
      "0.03\n",
      "0.46\n",
      "-0.23\n",
      "0.44\n",
      "-0.12\n",
      "0.04\n",
      "-0.32\n",
      "-0.16\n",
      "-0.23\n",
      "-0.42\n",
      "-0.36\n",
      "-0.25\n",
      "0.48\n",
      "-0.32\n",
      "0.01\n",
      "0.15\n",
      "0.23\n",
      "0.31\n",
      "-0.28\n",
      "-0.26\n",
      "0.28\n",
      "-0.03\n",
      "0.22\n",
      "0.1\n",
      "0.41\n",
      "-0.06\n",
      "-0.02\n",
      "-0.15\n",
      "-0.32\n",
      "0.34\n",
      "0.47\n",
      "0.05\n",
      "0.38\n",
      "0.09\n",
      "-0.36\n",
      "0.46\n",
      "0.18\n",
      "0.35\n",
      "0.11\n",
      "0.31\n",
      "-0.11\n",
      "0.46\n",
      "0.29\n",
      "-0.01\n",
      "0.41\n",
      "-0.04\n",
      "0.24\n",
      "-0.29\n",
      "-0.06\n",
      "0.21\n",
      "0.12\n",
      "0.03\n",
      "0.0008\n",
      "-0.27\n",
      "-0.12\n",
      "0.27\n",
      "-0.45\n",
      "0.39\n",
      "0.32\n",
      "-0.04\n",
      "-0.01\n",
      "-0.13\n",
      "0.19\n",
      "-0.34\n",
      "0.01\n",
      "-0.11\n",
      "0.1\n",
      "0.44\n",
      "0.11\n",
      "0.38\n",
      "0.36\n",
      "0.08\n",
      "0.33\n",
      "-0.26\n",
      "0.04\n",
      "0.23\n",
      "0.09\n",
      "-0.17\n",
      "0.17\n",
      "-0.44\n",
      "0.38\n",
      "0.51\n",
      "andr aara h\n",
      "-0.4\n",
      "0.35\n",
      "-0.4\n",
      "-0.2\n",
      "0.14\n",
      "-0.31\n",
      "-0.6\n",
      "-0.15\n",
      "0.54\n",
      "-0.15\n",
      "-0.16\n",
      "-0.44\n",
      "0.3\n",
      "-0.02\n",
      "-0.23\n",
      "0.1\n",
      "0.23\n",
      "0.49\n",
      "0.15\n",
      "-0.09\n",
      "0.46\n",
      "-0.01\n",
      "0.48\n",
      "0.27\n",
      "-0.59\n",
      "0.54\n",
      "0.44\n",
      "-0.33\n",
      "-0.23\n",
      "0.1\n",
      "-0.03\n",
      "-0.15\n",
      "-0.55\n",
      "-0.29\n",
      "0.19\n",
      "0.14\n",
      "-0.12\n",
      "0.08\n",
      "-0.17\n",
      "-0.0009\n",
      "-0.41\n",
      "0.46\n",
      "0.31\n",
      "0.2\n",
      "0.28\n",
      "-0.12\n",
      "0.09\n",
      "0.27\n",
      "0.22\n",
      "-0.54\n",
      "andr aara h\n",
      "-0.08\n",
      "0.22\n",
      "0.69\n",
      "-0.34\n",
      "0.02\n",
      "0.68\n",
      "-0.65\n",
      "-0.37\n",
      "-0.11\n",
      "0.43\n",
      "andr aara h\n",
      "-0.17\n",
      "-0.08\n",
      "-0.16\n",
      "-0.11\n",
      "0.04\n",
      "-0.14\n",
      "-0.07\n",
      "-0.19\n",
      "-0.14\n",
      "0.2\n",
      "-0.01\n",
      "0.07\n",
      "-0.004\n",
      "-0.04\n",
      "-0.06\n",
      "-0.06\n",
      "-0.09\n",
      "0.02\n",
      "-0.12\n",
      "-0.09\n",
      "-0.08\n",
      "0.03\n",
      "-0.07\n",
      "-0.06\n",
      "-0.12\n",
      "0.11\n",
      "0.15\n",
      "0.19\n",
      "0.09\n",
      "0.07\n",
      "0.04\n",
      "0.11\n",
      "0.01\n",
      "0.14\n",
      "0.14\n",
      "0.12\n",
      "0.15\n",
      "0.11\n",
      "0.14\n",
      "0.14\n",
      "andr aara h\n",
      "-0.01\n",
      "-0.28\n",
      "0.13\n",
      "-0.07\n",
      "0.08\n",
      "-0.2\n",
      "-0.19\n",
      "0.33\n",
      "0.36\n",
      "0.19\n",
      "-0.01\n",
      "0.09\n",
      "0.37\n",
      "-0.09\n",
      "-0.17\n",
      "0.32\n",
      "0.5\n",
      "-0.19\n",
      "0.63\n",
      "0.14\n",
      "-0.36\n",
      "0.1\n",
      "0.25\n",
      "0.51\n",
      "-0.06\n",
      "-0.11\n",
      "-0.36\n",
      "0.54\n",
      "0.71\n",
      "0.27\n",
      "0.22\n",
      "-0.65\n",
      "0.78\n",
      "-0.2\n",
      "-0.06\n",
      "0.16\n",
      "0.69\n",
      "0.01\n",
      "-0.23\n",
      "-0.5\n",
      "-0.03\n",
      "0.19\n",
      "0.45\n",
      "0.71\n",
      "0.69\n",
      "-0.59\n",
      "0.72\n",
      "0.66\n",
      "0.33\n",
      "0.54\n",
      "andr aara h\n",
      "0.04\n",
      "-0.15\n",
      "0.18\n",
      "0.37\n",
      "0.12\n",
      "-0.28\n",
      "-0.25\n",
      "0.12\n",
      "-0.13\n",
      "-0.03\n",
      "-0.42\n",
      "0.54\n",
      "-0.39\n",
      "0.29\n",
      "-0.01\n",
      "0.09\n",
      "-0.28\n",
      "0.49\n",
      "0.36\n",
      "-0.22\n",
      "0.18\n",
      "0.28\n",
      "0.05\n",
      "0.18\n",
      "-0.34\n",
      "0.31\n",
      "0.09\n",
      "0.36\n",
      "0.23\n",
      "0.11\n",
      "0.26\n",
      "-0.02\n",
      "-0.38\n",
      "0.17\n",
      "0.02\n",
      "-0.17\n",
      "0.19\n",
      "0.33\n",
      "-0.27\n",
      "-0.26\n",
      "0.08\n",
      "0.04\n",
      "0.35\n",
      "0.35\n",
      "0.52\n",
      "0.49\n",
      "-0.22\n",
      "0.36\n",
      "-0.42\n",
      "-0.17\n",
      "-0.37\n",
      "0.03\n",
      "-0.42\n",
      "0.45\n",
      "-0.18\n",
      "0.22\n",
      "0.05\n",
      "-0.06\n",
      "0.25\n",
      "0.56\n",
      "0.09\n",
      "0.23\n",
      "-0.29\n",
      "-0.22\n",
      "0.48\n",
      "0.13\n",
      "-0.01\n",
      "-0.23\n",
      "-0.17\n",
      "0.31\n",
      "0.19\n",
      "0.42\n",
      "0.2\n",
      "0.24\n",
      "-0.25\n",
      "0.37\n",
      "-0.31\n",
      "0.15\n",
      "0.33\n",
      "0.36\n",
      "0.38\n",
      "0.07\n",
      "-0.22\n",
      "0.47\n",
      "-0.15\n",
      "-0.27\n",
      "0.01\n",
      "0.45\n",
      "0.12\n",
      "0.25\n",
      "-0.39\n",
      "-0.28\n",
      "-0.29\n",
      "0.6\n",
      "0.48\n",
      "0.42\n",
      "-0.24\n",
      "0.46\n",
      "-0.17\n",
      "-0.02\n",
      "-0.3\n",
      "-0.2\n",
      "-0.43\n",
      "-0.27\n",
      "-0.24\n",
      "0.01\n",
      "0.4\n",
      "-0.06\n",
      "0.21\n",
      "0.09\n",
      "0.11\n",
      "0.08\n",
      "-0.3\n",
      "0.11\n",
      "-0.26\n",
      "-0.16\n",
      "-0.29\n",
      "-0.27\n",
      "-0.43\n",
      "-0.44\n",
      "0.43\n",
      "0.42\n",
      "0.09\n",
      "0.13\n",
      "0.14\n",
      "-0.17\n",
      "-0.24\n",
      "0.57\n",
      "-0.38\n",
      "0.46\n",
      "0.44\n",
      "0.27\n",
      "-0.33\n",
      "0.23\n",
      "-0.18\n",
      "-0.17\n",
      "-0.26\n",
      "0.11\n",
      "-0.13\n",
      "0.07\n",
      "-0.17\n",
      "0.19\n",
      "0.39\n",
      "0.47\n",
      "-0.06\n",
      "0.19\n",
      "-0.11\n",
      "0.27\n",
      "-0.03\n",
      "-0.06\n",
      "-0.19\n",
      "0.41\n",
      "-0.31\n",
      "0.24\n",
      "0.44\n",
      "0.25\n",
      "0.36\n",
      "0.37\n",
      "-0.27\n",
      "0.47\n",
      "0.51\n",
      "0.31\n",
      "0.08\n",
      "0.52\n",
      "-0.03\n",
      "0.03\n",
      "0.2\n",
      "0.52\n",
      "-0.15\n",
      "-0.35\n",
      "0.35\n",
      "-0.06\n",
      "-0.11\n",
      "0.1\n",
      "0.36\n",
      "0.36\n",
      "-0.31\n",
      "-0.05\n",
      "0.36\n",
      "0.16\n",
      "-0.31\n",
      "-0.33\n",
      "-0.33\n",
      "0.36\n",
      "0.02\n",
      "0.06\n",
      "-0.43\n",
      "0.31\n",
      "-0.21\n",
      "0.38\n",
      "-0.4\n",
      "0.17\n",
      "0.4\n",
      "-0.15\n",
      "0.18\n",
      "0.1\n",
      "0.21\n",
      "0.52\n",
      "0.13\n",
      "-0.04\n",
      "andr aara h\n",
      "0.33\n",
      "0.1\n",
      "-0.11\n",
      "0.23\n",
      "-0.09\n",
      "0.27\n",
      "-0.28\n",
      "0.39\n",
      "0.41\n",
      "0.51\n",
      "0.19\n",
      "-0.34\n",
      "-0.44\n",
      "0.5\n",
      "-0.34\n",
      "0.01\n",
      "-0.22\n",
      "-0.22\n",
      "0.31\n",
      "0.24\n",
      "0.51\n",
      "0.18\n",
      "-0.22\n",
      "0.21\n",
      "-0.02\n",
      "-0.2\n",
      "0.1\n",
      "0.21\n",
      "0.26\n",
      "0.33\n",
      "-0.14\n",
      "0.41\n",
      "0.02\n",
      "-0.16\n",
      "0.15\n",
      "0.45\n",
      "-0.2\n",
      "-0.13\n",
      "0.56\n",
      "-0.21\n",
      "0.57\n",
      "-0.27\n",
      "-0.13\n",
      "0.19\n",
      "-0.27\n",
      "0.42\n",
      "0.29\n",
      "0.23\n",
      "-0.26\n",
      "0.35\n",
      "0.45\n",
      "-0.27\n",
      "0.2\n",
      "0.05\n",
      "0.37\n",
      "-0.13\n",
      "-0.03\n",
      "-0.33\n",
      "0.01\n",
      "0.41\n",
      "-0.27\n",
      "-0.35\n",
      "0.05\n",
      "-0.43\n",
      "-0.27\n",
      "-0.02\n",
      "0.29\n",
      "0.12\n",
      "0.42\n",
      "-0.39\n",
      "0.57\n",
      "0.42\n",
      "0.31\n",
      "-0.16\n",
      "-0.32\n",
      "0.55\n",
      "0.22\n",
      "-0.14\n",
      "0.22\n",
      "0.26\n",
      "-0.01\n",
      "0.41\n",
      "0.18\n",
      "-0.18\n",
      "0.32\n",
      "-0.37\n",
      "-0.004\n",
      "-0.04\n",
      "-0.04\n",
      "0.13\n",
      "0.57\n",
      "0.21\n",
      "0.43\n",
      "0.09\n",
      "0.29\n",
      "0.28\n",
      "0.23\n",
      "-0.06\n",
      "0.36\n",
      "0.06\n",
      "0.01\n",
      "0.22\n",
      "0.06\n",
      "-0.03\n",
      "0.01\n",
      "-0.44\n",
      "-0.24\n",
      "0.32\n",
      "0.17\n",
      "-0.18\n",
      "-0.15\n",
      "0.18\n",
      "-0.19\n",
      "-0.02\n",
      "-0.31\n",
      "0.36\n",
      "-0.28\n",
      "-0.16\n",
      "0.02\n",
      "0.47\n",
      "-0.23\n",
      "0.44\n",
      "-0.12\n",
      "0.04\n",
      "-0.32\n",
      "-0.16\n",
      "-0.23\n",
      "-0.42\n",
      "-0.36\n",
      "-0.25\n",
      "0.48\n",
      "-0.3\n",
      "0.01\n",
      "0.15\n",
      "0.23\n",
      "0.31\n",
      "-0.28\n",
      "-0.25\n",
      "0.28\n",
      "-0.03\n",
      "0.21\n",
      "0.03\n",
      "0.4\n",
      "-0.11\n",
      "-0.01\n",
      "-0.16\n",
      "-0.33\n",
      "0.35\n",
      "0.44\n",
      "0.06\n",
      "0.38\n",
      "0.09\n",
      "-0.36\n",
      "0.45\n",
      "0.17\n",
      "0.35\n",
      "0.1\n",
      "0.31\n",
      "-0.11\n",
      "0.46\n",
      "0.29\n",
      "-0.01\n",
      "0.41\n",
      "-0.04\n",
      "0.26\n",
      "-0.29\n",
      "-0.07\n",
      "0.21\n",
      "0.12\n",
      "0.03\n",
      "0.01\n",
      "-0.31\n",
      "-0.12\n",
      "0.28\n",
      "-0.45\n",
      "0.4\n",
      "0.32\n",
      "-0.04\n",
      "-0.01\n",
      "-0.11\n",
      "0.18\n",
      "-0.31\n",
      "0.01\n",
      "-0.12\n",
      "0.09\n",
      "0.43\n",
      "0.11\n",
      "0.38\n",
      "0.35\n",
      "0.07\n",
      "0.33\n",
      "-0.24\n",
      "0.04\n",
      "0.22\n",
      "0.09\n",
      "-0.17\n",
      "0.17\n",
      "-0.44\n",
      "0.38\n",
      "0.5\n",
      "andr aara h\n",
      "-0.39\n",
      "0.35\n",
      "-0.4\n",
      "-0.21\n",
      "0.13\n",
      "-0.31\n",
      "-0.62\n",
      "-0.15\n",
      "0.54\n",
      "-0.15\n",
      "-0.16\n",
      "-0.44\n",
      "0.3\n",
      "-0.02\n",
      "-0.22\n",
      "0.1\n",
      "0.23\n",
      "0.49\n",
      "0.15\n",
      "-0.1\n",
      "0.46\n",
      "-0.01\n",
      "0.47\n",
      "0.27\n",
      "-0.59\n",
      "0.55\n",
      "0.44\n",
      "-0.33\n",
      "-0.23\n",
      "0.09\n",
      "-0.03\n",
      "-0.15\n",
      "-0.55\n",
      "-0.29\n",
      "0.19\n",
      "0.15\n",
      "-0.11\n",
      "0.08\n",
      "-0.17\n",
      "0.0007\n",
      "-0.41\n",
      "0.46\n",
      "0.3\n",
      "0.19\n",
      "0.28\n",
      "-0.11\n",
      "0.08\n",
      "0.27\n",
      "0.22\n",
      "-0.55\n",
      "andr aara h\n",
      "-0.09\n",
      "0.23\n",
      "0.69\n",
      "-0.34\n",
      "0.06\n",
      "0.64\n",
      "-0.65\n",
      "-0.37\n",
      "-0.11\n",
      "0.43\n",
      "andr aara h\n",
      "-0.15\n",
      "-0.05\n",
      "-0.15\n",
      "-0.03\n",
      "0.04\n",
      "-0.15\n",
      "-0.09\n",
      "-0.2\n",
      "-0.14\n",
      "0.19\n",
      "0.02\n",
      "0.11\n",
      "0.002\n",
      "0.04\n",
      "0.04\n",
      "-0.02\n",
      "-0.1\n",
      "0.04\n",
      "-0.16\n",
      "-0.14\n",
      "-0.09\n",
      "0.06\n",
      "-0.03\n",
      "-0.02\n",
      "-0.09\n",
      "0.11\n",
      "0.18\n",
      "0.21\n",
      "0.14\n",
      "0.04\n",
      "0.08\n",
      "0.12\n",
      "0.02\n",
      "0.15\n",
      "0.13\n",
      "0.17\n",
      "0.11\n",
      "0.13\n",
      "0.12\n",
      "0.16\n",
      "andr aara h\n",
      "0.05\n",
      "-0.13\n",
      "0.13\n",
      "-0.14\n",
      "0.05\n",
      "-0.27\n",
      "-0.21\n",
      "0.29\n",
      "0.36\n",
      "0.2\n",
      "0.04\n",
      "0.06\n",
      "0.39\n",
      "-0.2\n",
      "-0.26\n",
      "0.24\n",
      "0.5\n",
      "-0.27\n",
      "0.64\n",
      "0.06\n",
      "-0.33\n",
      "0.12\n",
      "0.27\n",
      "0.5\n",
      "-0.01\n",
      "-0.13\n",
      "-0.39\n",
      "0.56\n",
      "0.72\n",
      "0.26\n",
      "0.23\n",
      "-0.52\n",
      "0.77\n",
      "-0.26\n",
      "-0.11\n",
      "0.11\n",
      "0.66\n",
      "-0.05\n",
      "-0.25\n",
      "-0.52\n",
      "-0.05\n",
      "0.27\n",
      "0.43\n",
      "0.71\n",
      "0.69\n",
      "-0.58\n",
      "0.7\n",
      "0.67\n",
      "0.32\n",
      "0.58\n",
      "andr aara h\n",
      "0.03\n",
      "-0.15\n",
      "0.19\n",
      "0.37\n",
      "0.13\n",
      "-0.28\n",
      "-0.25\n",
      "0.12\n",
      "-0.13\n",
      "-0.06\n",
      "-0.49\n",
      "0.54\n",
      "-0.39\n",
      "0.28\n",
      "-0.01\n",
      "0.1\n",
      "-0.28\n",
      "0.5\n",
      "0.37\n",
      "-0.22\n",
      "0.2\n",
      "0.29\n",
      "0.12\n",
      "0.23\n",
      "-0.32\n",
      "0.33\n",
      "0.09\n",
      "0.38\n",
      "0.23\n",
      "0.13\n",
      "0.25\n",
      "-0.01\n",
      "-0.38\n",
      "0.19\n",
      "0.08\n",
      "-0.15\n",
      "0.2\n",
      "0.36\n",
      "-0.23\n",
      "-0.23\n",
      "0.07\n",
      "0.04\n",
      "0.34\n",
      "0.35\n",
      "0.52\n",
      "0.5\n",
      "-0.22\n",
      "0.35\n",
      "-0.42\n",
      "-0.19\n",
      "-0.43\n",
      "0.02\n",
      "-0.42\n",
      "0.45\n",
      "-0.18\n",
      "0.22\n",
      "0.05\n",
      "-0.06\n",
      "0.25\n",
      "0.56\n",
      "0.1\n",
      "0.24\n",
      "-0.26\n",
      "-0.2\n",
      "0.49\n",
      "0.14\n",
      "-0.01\n",
      "-0.22\n",
      "-0.17\n",
      "0.32\n",
      "0.22\n",
      "0.43\n",
      "0.2\n",
      "0.26\n",
      "-0.2\n",
      "0.38\n",
      "-0.31\n",
      "0.17\n",
      "0.34\n",
      "0.38\n",
      "0.39\n",
      "0.07\n",
      "-0.2\n",
      "0.51\n",
      "-0.14\n",
      "-0.26\n",
      "0.01\n",
      "0.46\n",
      "0.12\n",
      "0.25\n",
      "-0.24\n",
      "-0.26\n",
      "-0.29\n",
      "0.61\n",
      "0.51\n",
      "0.43\n",
      "-0.24\n",
      "0.48\n",
      "-0.15\n",
      "-0.01\n",
      "-0.23\n",
      "-0.11\n",
      "-0.36\n",
      "-0.21\n",
      "-0.15\n",
      "0.09\n",
      "0.41\n",
      "0.01\n",
      "0.21\n",
      "0.15\n",
      "0.05\n",
      "0.14\n",
      "-0.3\n",
      "0.2\n",
      "-0.22\n",
      "-0.08\n",
      "-0.29\n",
      "-0.18\n",
      "-0.34\n",
      "-0.35\n",
      "0.42\n",
      "0.41\n",
      "0.08\n",
      "0.12\n",
      "0.14\n",
      "-0.16\n",
      "-0.22\n",
      "0.56\n",
      "-0.38\n",
      "0.44\n",
      "0.38\n",
      "0.25\n",
      "-0.33\n",
      "0.22\n",
      "-0.2\n",
      "-0.18\n",
      "-0.26\n",
      "0.11\n",
      "-0.14\n",
      "0.06\n",
      "-0.17\n",
      "0.2\n",
      "0.41\n",
      "0.49\n",
      "-0.06\n",
      "0.2\n",
      "-0.12\n",
      "0.28\n",
      "-0.03\n",
      "-0.04\n",
      "-0.15\n",
      "0.41\n",
      "-0.31\n",
      "0.25\n",
      "0.48\n",
      "0.26\n",
      "0.36\n",
      "0.37\n",
      "-0.27\n",
      "0.48\n",
      "0.52\n",
      "0.33\n",
      "0.1\n",
      "0.52\n",
      "-0.02\n",
      "0.06\n",
      "0.2\n",
      "0.52\n",
      "-0.15\n",
      "-0.36\n",
      "0.3\n",
      "-0.05\n",
      "-0.11\n",
      "0.11\n",
      "0.37\n",
      "0.37\n",
      "-0.31\n",
      "-0.04\n",
      "0.37\n",
      "0.17\n",
      "-0.28\n",
      "-0.3\n",
      "-0.24\n",
      "0.42\n",
      "0.05\n",
      "0.1\n",
      "-0.44\n",
      "0.35\n",
      "-0.21\n",
      "0.43\n",
      "-0.39\n",
      "0.21\n",
      "0.4\n",
      "-0.11\n",
      "0.25\n",
      "0.13\n",
      "0.21\n",
      "0.56\n",
      "0.17\n",
      "-0.003\n",
      "andr aara h\n",
      "0.33\n",
      "0.07\n",
      "-0.12\n",
      "0.23\n",
      "-0.08\n",
      "0.26\n",
      "-0.28\n",
      "0.39\n",
      "0.41\n",
      "0.51\n",
      "0.19\n",
      "-0.37\n",
      "-0.45\n",
      "0.49\n",
      "-0.34\n",
      "0.01\n",
      "-0.22\n",
      "-0.22\n",
      "0.31\n",
      "0.24\n",
      "0.5\n",
      "0.13\n",
      "-0.22\n",
      "0.2\n",
      "-0.02\n",
      "-0.21\n",
      "0.1\n",
      "0.2\n",
      "0.26\n",
      "0.33\n",
      "-0.14\n",
      "0.39\n",
      "0.02\n",
      "-0.17\n",
      "0.16\n",
      "0.45\n",
      "-0.2\n",
      "-0.13\n",
      "0.56\n",
      "-0.2\n",
      "0.57\n",
      "-0.3\n",
      "-0.14\n",
      "0.19\n",
      "-0.26\n",
      "0.42\n",
      "0.29\n",
      "0.23\n",
      "-0.25\n",
      "0.35\n",
      "0.46\n",
      "-0.3\n",
      "0.19\n",
      "0.06\n",
      "0.37\n",
      "-0.13\n",
      "-0.03\n",
      "-0.33\n",
      "0.01\n",
      "0.42\n",
      "-0.27\n",
      "-0.35\n",
      "0.04\n",
      "-0.43\n",
      "-0.27\n",
      "-0.02\n",
      "0.29\n",
      "0.12\n",
      "0.42\n",
      "-0.39\n",
      "0.57\n",
      "0.39\n",
      "0.3\n",
      "-0.16\n",
      "-0.32\n",
      "0.55\n",
      "0.22\n",
      "-0.13\n",
      "0.22\n",
      "0.26\n",
      "-0.01\n",
      "0.41\n",
      "0.18\n",
      "-0.18\n",
      "0.32\n",
      "-0.37\n",
      "-0.004\n",
      "-0.04\n",
      "-0.04\n",
      "0.13\n",
      "0.57\n",
      "0.26\n",
      "0.43\n",
      "0.09\n",
      "0.29\n",
      "0.27\n",
      "0.25\n",
      "-0.06\n",
      "0.36\n",
      "0.06\n",
      "0.1\n",
      "0.17\n",
      "0.06\n",
      "0.06\n",
      "0.02\n",
      "-0.35\n",
      "-0.25\n",
      "0.33\n",
      "0.25\n",
      "-0.13\n",
      "-0.14\n",
      "0.17\n",
      "-0.19\n",
      "-0.01\n",
      "-0.31\n",
      "0.37\n",
      "-0.27\n",
      "-0.16\n",
      "0.03\n",
      "0.48\n",
      "-0.23\n",
      "0.44\n",
      "-0.12\n",
      "0.04\n",
      "-0.32\n",
      "-0.16\n",
      "-0.23\n",
      "-0.42\n",
      "-0.36\n",
      "-0.25\n",
      "0.48\n",
      "-0.32\n",
      "0.004\n",
      "0.16\n",
      "0.23\n",
      "0.31\n",
      "-0.28\n",
      "-0.25\n",
      "0.29\n",
      "-0.02\n",
      "0.28\n",
      "0.1\n",
      "0.4\n",
      "-0.02\n",
      "-0.02\n",
      "-0.09\n",
      "-0.33\n",
      "0.35\n",
      "0.52\n",
      "0.11\n",
      "0.39\n",
      "0.09\n",
      "-0.36\n",
      "0.46\n",
      "0.18\n",
      "0.36\n",
      "0.11\n",
      "0.31\n",
      "-0.1\n",
      "0.47\n",
      "0.29\n",
      "-0.02\n",
      "0.41\n",
      "-0.04\n",
      "0.25\n",
      "-0.29\n",
      "-0.07\n",
      "0.21\n",
      "0.13\n",
      "0.02\n",
      "0.03\n",
      "-0.27\n",
      "-0.12\n",
      "0.29\n",
      "-0.45\n",
      "0.42\n",
      "0.33\n",
      "-0.04\n",
      "0.01\n",
      "-0.1\n",
      "0.18\n",
      "-0.36\n",
      "0.003\n",
      "-0.12\n",
      "0.1\n",
      "0.43\n",
      "0.11\n",
      "0.38\n",
      "0.35\n",
      "0.07\n",
      "0.33\n",
      "-0.27\n",
      "0.03\n",
      "0.22\n",
      "0.09\n",
      "-0.18\n",
      "0.17\n",
      "-0.44\n",
      "0.38\n",
      "0.5\n",
      "andr aara h\n",
      "-0.39\n",
      "0.35\n",
      "-0.41\n",
      "-0.21\n",
      "0.13\n",
      "-0.31\n",
      "-0.6\n",
      "-0.15\n",
      "0.54\n",
      "-0.16\n",
      "-0.16\n",
      "-0.44\n",
      "0.3\n",
      "-0.02\n",
      "-0.23\n",
      "0.1\n",
      "0.23\n",
      "0.48\n",
      "0.14\n",
      "-0.1\n",
      "0.46\n",
      "0.01\n",
      "0.47\n",
      "0.26\n",
      "-0.6\n",
      "0.55\n",
      "0.44\n",
      "-0.34\n",
      "-0.24\n",
      "0.09\n",
      "-0.03\n",
      "-0.15\n",
      "-0.55\n",
      "-0.29\n",
      "0.18\n",
      "0.15\n",
      "-0.11\n",
      "0.08\n",
      "-0.17\n",
      "0.004\n",
      "-0.41\n",
      "0.46\n",
      "0.3\n",
      "0.18\n",
      "0.27\n",
      "-0.12\n",
      "0.08\n",
      "0.26\n",
      "0.21\n",
      "-0.55\n",
      "andr aara h\n",
      "-0.09\n",
      "0.23\n",
      "0.69\n",
      "-0.34\n",
      "0.05\n",
      "0.65\n",
      "-0.65\n",
      "-0.37\n",
      "-0.1\n",
      "0.42\n",
      "andr aara h\n",
      "-0.11\n",
      "-0.07\n",
      "-0.1\n",
      "-0.03\n",
      "0.06\n",
      "-0.12\n",
      "-0.09\n",
      "-0.17\n",
      "-0.13\n",
      "0.14\n",
      "0.06\n",
      "0.15\n",
      "-0.01\n",
      "0.06\n",
      "-0.01\n",
      "-0.1\n",
      "-0.13\n",
      "0.03\n",
      "-0.17\n",
      "-0.09\n",
      "-0.08\n",
      "0.05\n",
      "-0.01\n",
      "-0.02\n",
      "-0.09\n",
      "0.09\n",
      "0.13\n",
      "0.11\n",
      "0.09\n",
      "-0.02\n",
      "0.06\n",
      "0.12\n",
      "0.02\n",
      "0.18\n",
      "0.14\n",
      "0.14\n",
      "0.14\n",
      "0.17\n",
      "0.17\n",
      "0.22\n",
      "andr aara h\n",
      "0.05\n",
      "-0.26\n",
      "0.14\n",
      "-0.09\n",
      "0.01\n",
      "-0.23\n",
      "-0.17\n",
      "0.29\n",
      "0.36\n",
      "0.18\n",
      "0.05\n",
      "-0.01\n",
      "0.42\n",
      "-0.09\n",
      "-0.19\n",
      "0.26\n",
      "0.53\n",
      "-0.22\n",
      "0.67\n",
      "0.06\n",
      "-0.33\n",
      "0.12\n",
      "0.23\n",
      "0.48\n",
      "-0.07\n",
      "-0.07\n",
      "-0.36\n",
      "0.52\n",
      "0.69\n",
      "0.27\n",
      "0.25\n",
      "-0.54\n",
      "0.79\n",
      "-0.19\n",
      "-0.04\n",
      "0.13\n",
      "0.7\n",
      "-0.01\n",
      "-0.23\n",
      "-0.52\n",
      "-0.04\n",
      "0.2\n",
      "0.44\n",
      "0.69\n",
      "0.68\n",
      "-0.6\n",
      "0.71\n",
      "0.66\n",
      "0.32\n",
      "0.53\n",
      "andr aara h\n",
      "0.08\n",
      "-0.1\n",
      "0.22\n",
      "0.42\n",
      "0.17\n",
      "-0.21\n",
      "-0.25\n",
      "0.17\n",
      "-0.13\n",
      "-0.01\n",
      "-0.5\n",
      "0.6\n",
      "-0.39\n",
      "0.33\n",
      "0.06\n",
      "0.14\n",
      "-0.28\n",
      "0.54\n",
      "0.41\n",
      "-0.17\n",
      "0.15\n",
      "0.24\n",
      "0.03\n",
      "0.19\n",
      "-0.38\n",
      "0.29\n",
      "0.09\n",
      "0.34\n",
      "0.23\n",
      "0.09\n",
      "0.26\n",
      "-0.06\n",
      "-0.38\n",
      "0.14\n",
      "0.04\n",
      "-0.21\n",
      "0.2\n",
      "0.31\n",
      "-0.3\n",
      "-0.29\n",
      "0.09\n",
      "0.06\n",
      "0.35\n",
      "0.36\n",
      "0.53\n",
      "0.52\n",
      "-0.22\n",
      "0.36\n",
      "-0.42\n",
      "-0.16\n",
      "-0.43\n",
      "0.04\n",
      "-0.42\n",
      "0.46\n",
      "-0.17\n",
      "0.23\n",
      "0.05\n",
      "-0.06\n",
      "0.26\n",
      "0.57\n",
      "0.07\n",
      "0.2\n",
      "-0.31\n",
      "-0.23\n",
      "0.46\n",
      "0.11\n",
      "-0.01\n",
      "-0.25\n",
      "-0.17\n",
      "0.3\n",
      "0.23\n",
      "0.4\n",
      "0.2\n",
      "0.22\n",
      "-0.24\n",
      "0.35\n",
      "-0.31\n",
      "0.12\n",
      "0.3\n",
      "0.34\n",
      "0.37\n",
      "0.05\n",
      "-0.22\n",
      "0.49\n",
      "-0.16\n",
      "-0.28\n",
      "0.01\n",
      "0.44\n",
      "0.12\n",
      "0.24\n",
      "-0.23\n",
      "-0.29\n",
      "-0.29\n",
      "0.59\n",
      "0.5\n",
      "0.41\n",
      "-0.24\n",
      "0.46\n",
      "-0.18\n",
      "-0.04\n",
      "-0.26\n",
      "-0.14\n",
      "-0.38\n",
      "-0.24\n",
      "-0.17\n",
      "0.08\n",
      "0.41\n",
      "-0.02\n",
      "0.21\n",
      "0.11\n",
      "0.04\n",
      "0.11\n",
      "-0.3\n",
      "0.18\n",
      "-0.22\n",
      "-0.11\n",
      "-0.29\n",
      "-0.22\n",
      "-0.35\n",
      "-0.37\n",
      "0.44\n",
      "0.44\n",
      "0.11\n",
      "0.15\n",
      "0.17\n",
      "-0.13\n",
      "-0.23\n",
      "0.58\n",
      "-0.38\n",
      "0.47\n",
      "0.38\n",
      "0.29\n",
      "-0.33\n",
      "0.25\n",
      "-0.17\n",
      "-0.16\n",
      "-0.26\n",
      "0.15\n",
      "-0.11\n",
      "0.09\n",
      "-0.19\n",
      "0.17\n",
      "0.37\n",
      "0.46\n",
      "-0.09\n",
      "0.17\n",
      "-0.11\n",
      "0.25\n",
      "-0.03\n",
      "-0.05\n",
      "-0.15\n",
      "0.38\n",
      "-0.31\n",
      "0.22\n",
      "0.44\n",
      "0.23\n",
      "0.36\n",
      "0.33\n",
      "-0.29\n",
      "0.45\n",
      "0.51\n",
      "0.32\n",
      "0.08\n",
      "0.52\n",
      "-0.03\n",
      "0.05\n",
      "0.19\n",
      "0.52\n",
      "-0.15\n",
      "-0.34\n",
      "0.29\n",
      "-0.06\n",
      "-0.11\n",
      "0.1\n",
      "0.36\n",
      "0.36\n",
      "-0.3\n",
      "-0.06\n",
      "0.37\n",
      "0.16\n",
      "-0.34\n",
      "-0.37\n",
      "-0.36\n",
      "0.35\n",
      "-0.01\n",
      "0.03\n",
      "-0.44\n",
      "0.28\n",
      "-0.21\n",
      "0.37\n",
      "-0.38\n",
      "0.13\n",
      "0.4\n",
      "-0.18\n",
      "0.18\n",
      "0.07\n",
      "0.21\n",
      "0.48\n",
      "0.1\n",
      "-0.07\n",
      "andr aara h\n",
      "0.34\n",
      "0.06\n",
      "-0.11\n",
      "0.24\n",
      "-0.06\n",
      "0.27\n",
      "-0.28\n",
      "0.39\n",
      "0.42\n",
      "0.52\n",
      "0.19\n",
      "-0.37\n",
      "-0.44\n",
      "0.5\n",
      "-0.32\n",
      "0.01\n",
      "-0.21\n",
      "-0.22\n",
      "0.32\n",
      "0.25\n",
      "0.52\n",
      "0.14\n",
      "-0.22\n",
      "0.22\n",
      "0.01\n",
      "-0.19\n",
      "0.1\n",
      "0.21\n",
      "0.27\n",
      "0.36\n",
      "-0.14\n",
      "0.39\n",
      "0.02\n",
      "-0.16\n",
      "0.17\n",
      "0.45\n",
      "-0.19\n",
      "-0.12\n",
      "0.56\n",
      "-0.2\n",
      "0.58\n",
      "-0.3\n",
      "-0.13\n",
      "0.2\n",
      "-0.24\n",
      "0.43\n",
      "0.29\n",
      "0.23\n",
      "-0.25\n",
      "0.36\n",
      "0.45\n",
      "-0.3\n",
      "0.2\n",
      "0.05\n",
      "0.39\n",
      "-0.13\n",
      "-0.03\n",
      "-0.33\n",
      "0.01\n",
      "0.42\n",
      "-0.27\n",
      "-0.35\n",
      "0.05\n",
      "-0.43\n",
      "-0.27\n",
      "-0.03\n",
      "0.3\n",
      "0.13\n",
      "0.42\n",
      "-0.39\n",
      "0.57\n",
      "0.39\n",
      "0.31\n",
      "-0.15\n",
      "-0.3\n",
      "0.55\n",
      "0.22\n",
      "-0.13\n",
      "0.23\n",
      "0.27\n",
      "-0.01\n",
      "0.41\n",
      "0.18\n",
      "-0.18\n",
      "0.32\n",
      "-0.37\n",
      "-0.004\n",
      "-0.04\n",
      "-0.04\n",
      "0.13\n",
      "0.56\n",
      "0.27\n",
      "0.43\n",
      "0.08\n",
      "0.29\n",
      "0.26\n",
      "0.26\n",
      "-0.06\n",
      "0.34\n",
      "0.04\n",
      "0.08\n",
      "0.17\n",
      "0.06\n",
      "0.06\n",
      "0.01\n",
      "-0.35\n",
      "-0.24\n",
      "0.33\n",
      "0.24\n",
      "-0.13\n",
      "-0.15\n",
      "0.17\n",
      "-0.18\n",
      "-0.02\n",
      "-0.31\n",
      "0.36\n",
      "-0.27\n",
      "-0.16\n",
      "0.03\n",
      "0.46\n",
      "-0.23\n",
      "0.44\n",
      "-0.12\n",
      "0.04\n",
      "-0.32\n",
      "-0.16\n",
      "-0.23\n",
      "-0.42\n",
      "-0.36\n",
      "-0.25\n",
      "0.48\n",
      "-0.32\n",
      "0.01\n",
      "0.16\n",
      "0.26\n",
      "0.31\n",
      "-0.28\n",
      "-0.25\n",
      "0.29\n",
      "-0.02\n",
      "0.23\n",
      "0.1\n",
      "0.41\n",
      "-0.08\n",
      "-0.03\n",
      "-0.14\n",
      "-0.33\n",
      "0.35\n",
      "0.47\n",
      "0.06\n",
      "0.38\n",
      "0.09\n",
      "-0.36\n",
      "0.45\n",
      "0.19\n",
      "0.35\n",
      "0.1\n",
      "0.31\n",
      "-0.12\n",
      "0.46\n",
      "0.29\n",
      "-0.01\n",
      "0.41\n",
      "-0.04\n",
      "0.26\n",
      "-0.29\n",
      "-0.07\n",
      "0.21\n",
      "0.12\n",
      "0.03\n",
      "-0.0007\n",
      "-0.26\n",
      "-0.12\n",
      "0.27\n",
      "-0.47\n",
      "0.39\n",
      "0.34\n",
      "-0.03\n",
      "-0.02\n",
      "-0.12\n",
      "0.19\n",
      "-0.36\n",
      "0.01\n",
      "-0.12\n",
      "0.11\n",
      "0.43\n",
      "0.12\n",
      "0.38\n",
      "0.35\n",
      "0.07\n",
      "0.32\n",
      "-0.26\n",
      "0.04\n",
      "0.23\n",
      "0.11\n",
      "-0.18\n",
      "0.17\n",
      "-0.44\n",
      "0.38\n",
      "0.51\n",
      "andr aara h\n",
      "-0.39\n",
      "0.36\n",
      "-0.41\n",
      "-0.21\n",
      "0.13\n",
      "-0.31\n",
      "-0.6\n",
      "-0.15\n",
      "0.54\n",
      "-0.15\n",
      "-0.16\n",
      "-0.44\n",
      "0.3\n",
      "-0.02\n",
      "-0.22\n",
      "0.1\n",
      "0.23\n",
      "0.48\n",
      "0.15\n",
      "-0.1\n",
      "0.46\n",
      "0.03\n",
      "0.46\n",
      "0.26\n",
      "-0.61\n",
      "0.55\n",
      "0.44\n",
      "-0.34\n",
      "-0.24\n",
      "0.09\n",
      "-0.03\n",
      "-0.15\n",
      "-0.55\n",
      "-0.29\n",
      "0.19\n",
      "0.15\n",
      "-0.11\n",
      "0.08\n",
      "-0.17\n",
      "0.002\n",
      "-0.41\n",
      "0.46\n",
      "0.3\n",
      "0.18\n",
      "0.27\n",
      "-0.11\n",
      "0.09\n",
      "0.26\n",
      "0.21\n",
      "-0.55\n",
      "andr aara h\n",
      "-0.09\n",
      "0.23\n",
      "0.69\n",
      "-0.34\n",
      "0.05\n",
      "0.65\n",
      "-0.65\n",
      "-0.37\n",
      "-0.1\n",
      "0.42\n",
      "andr aara h\n",
      "-0.12\n",
      "-0.1\n",
      "-0.13\n",
      "-0.09\n",
      "0.005\n",
      "-0.11\n",
      "-0.1\n",
      "-0.18\n",
      "-0.15\n",
      "0.1\n",
      "-0.005\n",
      "0.05\n",
      "-0.07\n",
      "-0.05\n",
      "-0.04\n",
      "-0.05\n",
      "-0.09\n",
      "0.05\n",
      "-0.14\n",
      "-0.11\n",
      "-0.04\n",
      "0.06\n",
      "0.06\n",
      "-0.08\n",
      "-0.09\n",
      "0.01\n",
      "0.12\n",
      "0.16\n",
      "0.07\n",
      "0.02\n",
      "0.09\n",
      "0.12\n",
      "0.02\n",
      "0.17\n",
      "0.13\n",
      "0.14\n",
      "0.1\n",
      "0.19\n",
      "0.11\n",
      "0.18\n",
      "andr aara h\n",
      "0.03\n",
      "-0.28\n",
      "0.12\n",
      "-0.02\n",
      "0.02\n",
      "-0.19\n",
      "-0.18\n",
      "0.32\n",
      "0.35\n",
      "0.13\n",
      "0.01\n",
      "-0.05\n",
      "0.36\n",
      "-0.21\n",
      "-0.28\n",
      "0.31\n",
      "0.51\n",
      "-0.23\n",
      "0.63\n",
      "0.05\n",
      "-0.29\n",
      "0.16\n",
      "0.27\n",
      "0.56\n",
      "-0.06\n",
      "-0.14\n",
      "-0.33\n",
      "0.57\n",
      "0.72\n",
      "0.27\n",
      "0.25\n",
      "-0.65\n",
      "0.79\n",
      "-0.16\n",
      "-0.1\n",
      "0.16\n",
      "0.7\n",
      "0.01\n",
      "-0.23\n",
      "-0.63\n",
      "-0.09\n",
      "0.18\n",
      "0.41\n",
      "0.67\n",
      "0.65\n",
      "-0.64\n",
      "0.68\n",
      "0.62\n",
      "0.3\n",
      "0.5\n",
      "andr aara h\n",
      "0.05\n",
      "-0.16\n",
      "0.19\n",
      "0.41\n",
      "0.13\n",
      "-0.29\n",
      "-0.25\n",
      "0.15\n",
      "-0.13\n",
      "-0.02\n",
      "-0.46\n",
      "0.55\n",
      "-0.39\n",
      "0.29\n",
      "-0.01\n",
      "0.1\n",
      "-0.28\n",
      "0.5\n",
      "0.37\n",
      "-0.22\n",
      "0.16\n",
      "0.25\n",
      "0.07\n",
      "0.23\n",
      "-0.36\n",
      "0.29\n",
      "0.09\n",
      "0.37\n",
      "0.23\n",
      "0.1\n",
      "0.28\n",
      "-0.05\n",
      "-0.38\n",
      "0.16\n",
      "0.03\n",
      "-0.19\n",
      "0.19\n",
      "0.32\n",
      "-0.26\n",
      "-0.28\n",
      "0.08\n",
      "0.04\n",
      "0.35\n",
      "0.36\n",
      "0.52\n",
      "0.48\n",
      "-0.23\n",
      "0.37\n",
      "-0.42\n",
      "-0.16\n",
      "-0.38\n",
      "0.03\n",
      "-0.42\n",
      "0.45\n",
      "-0.19\n",
      "0.22\n",
      "0.06\n",
      "-0.07\n",
      "0.25\n",
      "0.56\n",
      "0.08\n",
      "0.22\n",
      "-0.29\n",
      "-0.25\n",
      "0.47\n",
      "0.14\n",
      "-0.01\n",
      "-0.25\n",
      "-0.17\n",
      "0.32\n",
      "0.24\n",
      "0.42\n",
      "0.2\n",
      "0.23\n",
      "-0.27\n",
      "0.35\n",
      "-0.31\n",
      "0.13\n",
      "0.31\n",
      "0.36\n",
      "0.36\n",
      "0.04\n",
      "-0.23\n",
      "0.45\n",
      "-0.16\n",
      "-0.28\n",
      "0.01\n",
      "0.43\n",
      "0.12\n",
      "0.22\n",
      "-0.23\n",
      "-0.29\n",
      "-0.29\n",
      "0.58\n",
      "0.44\n",
      "0.4\n",
      "-0.25\n",
      "0.44\n",
      "-0.19\n",
      "-0.04\n",
      "-0.3\n",
      "-0.2\n",
      "-0.43\n",
      "-0.17\n",
      "-0.23\n",
      "0.01\n",
      "0.4\n",
      "-0.04\n",
      "0.21\n",
      "0.1\n",
      "0.12\n",
      "0.09\n",
      "-0.3\n",
      "0.12\n",
      "-0.08\n",
      "-0.16\n",
      "-0.28\n",
      "-0.23\n",
      "-0.43\n",
      "-0.43\n",
      "0.42\n",
      "0.39\n",
      "0.08\n",
      "0.14\n",
      "0.13\n",
      "-0.2\n",
      "-0.23\n",
      "0.57\n",
      "-0.38\n",
      "0.45\n",
      "0.43\n",
      "0.25\n",
      "-0.33\n",
      "0.21\n",
      "-0.23\n",
      "-0.19\n",
      "-0.26\n",
      "0.1\n",
      "-0.15\n",
      "0.04\n",
      "-0.17\n",
      "0.21\n",
      "0.4\n",
      "0.45\n",
      "-0.06\n",
      "0.22\n",
      "-0.11\n",
      "0.27\n",
      "-0.03\n",
      "-0.03\n",
      "-0.19\n",
      "0.42\n",
      "-0.31\n",
      "0.25\n",
      "0.42\n",
      "0.25\n",
      "0.37\n",
      "0.36\n",
      "-0.26\n",
      "0.49\n",
      "0.51\n",
      "0.31\n",
      "0.09\n",
      "0.53\n",
      "-0.04\n",
      "0.02\n",
      "0.19\n",
      "0.53\n",
      "-0.15\n",
      "-0.32\n",
      "0.35\n",
      "-0.06\n",
      "-0.11\n",
      "0.1\n",
      "0.35\n",
      "0.35\n",
      "-0.3\n",
      "-0.07\n",
      "0.36\n",
      "0.14\n",
      "-0.36\n",
      "-0.38\n",
      "-0.36\n",
      "0.33\n",
      "-0.03\n",
      "0.02\n",
      "-0.44\n",
      "0.27\n",
      "-0.21\n",
      "0.36\n",
      "-0.31\n",
      "0.13\n",
      "0.4\n",
      "-0.2\n",
      "0.14\n",
      "0.05\n",
      "0.21\n",
      "0.45\n",
      "0.08\n",
      "-0.09\n",
      "andr aara h\n",
      "0.34\n",
      "0.12\n",
      "-0.1\n",
      "0.22\n",
      "-0.1\n",
      "0.27\n",
      "-0.28\n",
      "0.39\n",
      "0.41\n",
      "0.49\n",
      "0.19\n",
      "-0.34\n",
      "-0.44\n",
      "0.49\n",
      "-0.35\n",
      "0.01\n",
      "-0.22\n",
      "-0.22\n",
      "0.31\n",
      "0.22\n",
      "0.52\n",
      "0.15\n",
      "-0.22\n",
      "0.22\n",
      "-0.03\n",
      "-0.19\n",
      "0.1\n",
      "0.21\n",
      "0.27\n",
      "0.33\n",
      "-0.14\n",
      "0.42\n",
      "0.02\n",
      "-0.17\n",
      "0.15\n",
      "0.45\n",
      "-0.2\n",
      "-0.12\n",
      "0.55\n",
      "-0.22\n",
      "0.58\n",
      "-0.27\n",
      "-0.13\n",
      "0.19\n",
      "-0.27\n",
      "0.42\n",
      "0.29\n",
      "0.23\n",
      "-0.25\n",
      "0.34\n",
      "0.46\n",
      "-0.29\n",
      "0.2\n",
      "0.07\n",
      "0.36\n",
      "-0.12\n",
      "-0.02\n",
      "-0.32\n",
      "0.02\n",
      "0.42\n",
      "-0.28\n",
      "-0.34\n",
      "0.05\n",
      "-0.43\n",
      "-0.27\n",
      "-0.03\n",
      "0.3\n",
      "0.12\n",
      "0.42\n",
      "-0.4\n",
      "0.57\n",
      "0.42\n",
      "0.31\n",
      "-0.16\n",
      "-0.33\n",
      "0.55\n",
      "0.22\n",
      "-0.13\n",
      "0.22\n",
      "0.24\n",
      "-0.01\n",
      "0.41\n",
      "0.18\n",
      "-0.18\n",
      "0.32\n",
      "-0.37\n",
      "-0.004\n",
      "-0.04\n",
      "-0.04\n",
      "0.13\n",
      "0.54\n",
      "0.25\n",
      "0.43\n",
      "0.06\n",
      "0.29\n",
      "0.24\n",
      "0.23\n",
      "-0.06\n",
      "0.32\n",
      "0.04\n",
      "0.04\n",
      "0.26\n",
      "0.06\n",
      "-0.02\n",
      "0.01\n",
      "-0.42\n",
      "-0.24\n",
      "0.33\n",
      "0.2\n",
      "-0.22\n",
      "-0.15\n",
      "0.18\n",
      "-0.19\n",
      "-0.02\n",
      "-0.3\n",
      "0.36\n",
      "-0.29\n",
      "-0.16\n",
      "0.02\n",
      "0.46\n",
      "-0.23\n",
      "0.44\n",
      "-0.12\n",
      "0.04\n",
      "-0.32\n",
      "-0.16\n",
      "-0.23\n",
      "-0.42\n",
      "-0.36\n",
      "-0.25\n",
      "0.48\n",
      "-0.29\n",
      "0.01\n",
      "0.15\n",
      "0.23\n",
      "0.31\n",
      "-0.28\n",
      "-0.25\n",
      "0.28\n",
      "-0.03\n",
      "0.19\n",
      "0.1\n",
      "0.4\n",
      "-0.08\n",
      "-0.02\n",
      "-0.17\n",
      "-0.32\n",
      "0.35\n",
      "0.43\n",
      "0.06\n",
      "0.38\n",
      "0.1\n",
      "-0.36\n",
      "0.46\n",
      "0.17\n",
      "0.35\n",
      "0.11\n",
      "0.32\n",
      "-0.11\n",
      "0.46\n",
      "0.29\n",
      "-0.02\n",
      "0.42\n",
      "-0.04\n",
      "0.26\n",
      "-0.29\n",
      "-0.06\n",
      "0.21\n",
      "0.12\n",
      "0.03\n",
      "0.003\n",
      "-0.28\n",
      "-0.12\n",
      "0.27\n",
      "-0.43\n",
      "0.4\n",
      "0.32\n",
      "-0.04\n",
      "-0.02\n",
      "-0.11\n",
      "0.19\n",
      "-0.32\n",
      "0.01\n",
      "-0.12\n",
      "0.09\n",
      "0.44\n",
      "0.11\n",
      "0.39\n",
      "0.36\n",
      "0.06\n",
      "0.33\n",
      "-0.25\n",
      "0.04\n",
      "0.23\n",
      "0.08\n",
      "-0.17\n",
      "0.17\n",
      "-0.43\n",
      "0.38\n",
      "0.5\n",
      "andr aara h\n",
      "-0.4\n",
      "0.35\n",
      "-0.4\n",
      "-0.21\n",
      "0.15\n",
      "-0.31\n",
      "-0.68\n",
      "-0.15\n",
      "0.54\n",
      "-0.1\n",
      "-0.16\n",
      "-0.44\n",
      "0.3\n",
      "-0.02\n",
      "-0.22\n",
      "0.1\n",
      "0.23\n",
      "0.5\n",
      "0.14\n",
      "-0.09\n",
      "0.46\n",
      "-0.01\n",
      "0.47\n",
      "0.27\n",
      "-0.59\n",
      "0.54\n",
      "0.44\n",
      "-0.33\n",
      "-0.23\n",
      "0.1\n",
      "-0.04\n",
      "-0.15\n",
      "-0.55\n",
      "-0.29\n",
      "0.19\n",
      "0.15\n",
      "-0.12\n",
      "0.08\n",
      "-0.17\n",
      "0.005\n",
      "-0.41\n",
      "0.46\n",
      "0.31\n",
      "0.19\n",
      "0.29\n",
      "-0.12\n",
      "0.08\n",
      "0.28\n",
      "0.22\n",
      "-0.54\n",
      "andr aara h\n",
      "-0.09\n",
      "0.23\n",
      "0.69\n",
      "-0.34\n",
      "0.01\n",
      "0.69\n",
      "-0.65\n",
      "-0.37\n",
      "-0.14\n",
      "0.46\n",
      "andr aara h\n",
      "-0.14\n",
      "-0.11\n",
      "-0.13\n",
      "-0.13\n",
      "-0.003\n",
      "-0.13\n",
      "-0.11\n",
      "-0.18\n",
      "-0.19\n",
      "0.14\n",
      "0.03\n",
      "0.1\n",
      "0.004\n",
      "-0.02\n",
      "0.01\n",
      "-0.05\n",
      "-0.09\n",
      "0.06\n",
      "-0.11\n",
      "-0.13\n",
      "-0.07\n",
      "0.04\n",
      "0.01\n",
      "-0.03\n",
      "-0.08\n",
      "0.11\n",
      "0.16\n",
      "0.25\n",
      "0.13\n",
      "0.15\n",
      "0.14\n",
      "0.14\n",
      "0.06\n",
      "0.17\n",
      "0.11\n",
      "0.05\n",
      "0.05\n",
      "0.09\n",
      "0.08\n",
      "0.15\n",
      "andr aara h\n",
      "0.07\n",
      "-0.25\n",
      "0.14\n",
      "-0.14\n",
      "-0.03\n",
      "-0.27\n",
      "-0.17\n",
      "0.29\n",
      "0.36\n",
      "0.13\n",
      "0.02\n",
      "-0.08\n",
      "0.39\n",
      "-0.23\n",
      "-0.35\n",
      "0.24\n",
      "0.51\n",
      "-0.27\n",
      "0.64\n",
      "-0.02\n",
      "-0.31\n",
      "0.13\n",
      "0.26\n",
      "0.52\n",
      "-0.03\n",
      "-0.11\n",
      "-0.34\n",
      "0.55\n",
      "0.72\n",
      "0.29\n",
      "0.25\n",
      "-0.6\n",
      "0.78\n",
      "-0.28\n",
      "-0.13\n",
      "0.11\n",
      "0.69\n",
      "-0.03\n",
      "-0.24\n",
      "-0.61\n",
      "-0.08\n",
      "0.2\n",
      "0.41\n",
      "0.67\n",
      "0.65\n",
      "-0.61\n",
      "0.68\n",
      "0.64\n",
      "0.3\n",
      "0.49\n",
      "andr aara h\n",
      "0.05\n",
      "-0.13\n",
      "0.2\n",
      "0.4\n",
      "0.15\n",
      "-0.27\n",
      "-0.25\n",
      "0.14\n",
      "-0.13\n",
      "-0.04\n",
      "-0.43\n",
      "0.54\n",
      "-0.39\n",
      "0.3\n",
      "-0.03\n",
      "0.12\n",
      "-0.28\n",
      "0.52\n",
      "0.38\n",
      "-0.2\n",
      "0.18\n",
      "0.28\n",
      "0.1\n",
      "0.21\n",
      "-0.33\n",
      "0.28\n",
      "0.09\n",
      "0.37\n",
      "0.23\n",
      "0.12\n",
      "0.27\n",
      "-0.03\n",
      "-0.38\n",
      "0.18\n",
      "0.06\n",
      "-0.18\n",
      "0.2\n",
      "0.34\n",
      "-0.25\n",
      "-0.25\n",
      "0.08\n",
      "0.04\n",
      "0.35\n",
      "0.35\n",
      "0.52\n",
      "0.5\n",
      "-0.23\n",
      "0.35\n",
      "-0.42\n",
      "-0.19\n",
      "-0.41\n",
      "0.03\n",
      "-0.42\n",
      "0.45\n",
      "-0.2\n",
      "0.22\n",
      "0.05\n",
      "-0.06\n",
      "0.25\n",
      "0.57\n",
      "0.08\n",
      "0.23\n",
      "-0.28\n",
      "-0.21\n",
      "0.48\n",
      "0.11\n",
      "-0.02\n",
      "-0.24\n",
      "-0.17\n",
      "0.31\n",
      "0.28\n",
      "0.4\n",
      "0.2\n",
      "0.24\n",
      "-0.22\n",
      "0.36\n",
      "-0.31\n",
      "0.15\n",
      "0.33\n",
      "0.36\n",
      "0.37\n",
      "0.05\n",
      "-0.21\n",
      "0.49\n",
      "-0.15\n",
      "-0.28\n",
      "0.01\n",
      "0.44\n",
      "0.12\n",
      "0.23\n",
      "-0.18\n",
      "-0.28\n",
      "-0.29\n",
      "0.59\n",
      "0.5\n",
      "0.41\n",
      "-0.25\n",
      "0.46\n",
      "-0.17\n",
      "-0.03\n",
      "-0.24\n",
      "-0.11\n",
      "-0.35\n",
      "-0.21\n",
      "-0.15\n",
      "0.1\n",
      "0.4\n",
      "0.001\n",
      "0.21\n",
      "0.15\n",
      "0.04\n",
      "0.14\n",
      "-0.3\n",
      "0.2\n",
      "-0.22\n",
      "-0.09\n",
      "-0.28\n",
      "-0.16\n",
      "-0.34\n",
      "-0.35\n",
      "0.41\n",
      "0.41\n",
      "0.09\n",
      "0.12\n",
      "0.14\n",
      "-0.17\n",
      "-0.23\n",
      "0.56\n",
      "-0.38\n",
      "0.42\n",
      "0.4\n",
      "0.25\n",
      "-0.33\n",
      "0.22\n",
      "-0.23\n",
      "-0.19\n",
      "-0.26\n",
      "0.11\n",
      "-0.15\n",
      "0.06\n",
      "-0.18\n",
      "0.18\n",
      "0.39\n",
      "0.48\n",
      "-0.07\n",
      "0.17\n",
      "-0.12\n",
      "0.27\n",
      "-0.03\n",
      "-0.04\n",
      "-0.09\n",
      "0.38\n",
      "-0.31\n",
      "0.24\n",
      "0.45\n",
      "0.24\n",
      "0.36\n",
      "0.36\n",
      "-0.28\n",
      "0.46\n",
      "0.51\n",
      "0.32\n",
      "0.09\n",
      "0.52\n",
      "-0.02\n",
      "0.05\n",
      "0.19\n",
      "0.51\n",
      "-0.15\n",
      "-0.36\n",
      "0.31\n",
      "-0.06\n",
      "-0.11\n",
      "0.1\n",
      "0.35\n",
      "0.36\n",
      "-0.31\n",
      "-0.04\n",
      "0.37\n",
      "0.16\n",
      "-0.35\n",
      "-0.37\n",
      "-0.33\n",
      "0.34\n",
      "-0.01\n",
      "0.01\n",
      "-0.44\n",
      "0.28\n",
      "-0.21\n",
      "0.37\n",
      "-0.34\n",
      "0.13\n",
      "0.4\n",
      "-0.18\n",
      "0.16\n",
      "0.07\n",
      "0.21\n",
      "0.49\n",
      "0.1\n",
      "-0.07\n",
      "andr aara h\n",
      "0.33\n",
      "0.07\n",
      "-0.11\n",
      "0.23\n",
      "-0.09\n",
      "0.26\n",
      "-0.28\n",
      "0.39\n",
      "0.41\n",
      "0.51\n",
      "0.18\n",
      "-0.37\n",
      "-0.44\n",
      "0.49\n",
      "-0.34\n",
      "0.01\n",
      "-0.22\n",
      "-0.23\n",
      "0.31\n",
      "0.24\n",
      "0.51\n",
      "0.13\n",
      "-0.22\n",
      "0.2\n",
      "-0.02\n",
      "-0.2\n",
      "0.1\n",
      "0.19\n",
      "0.26\n",
      "0.34\n",
      "-0.15\n",
      "0.39\n",
      "0.02\n",
      "-0.17\n",
      "0.15\n",
      "0.44\n",
      "-0.2\n",
      "-0.13\n",
      "0.56\n",
      "-0.2\n",
      "0.57\n",
      "-0.3\n",
      "-0.14\n",
      "0.19\n",
      "-0.27\n",
      "0.42\n",
      "0.29\n",
      "0.23\n",
      "-0.26\n",
      "0.35\n",
      "0.45\n",
      "-0.3\n",
      "0.2\n",
      "0.05\n",
      "0.37\n",
      "-0.13\n",
      "-0.03\n",
      "-0.33\n",
      "0.01\n",
      "0.43\n",
      "-0.27\n",
      "-0.35\n",
      "0.05\n",
      "-0.43\n",
      "-0.26\n",
      "-0.03\n",
      "0.29\n",
      "0.13\n",
      "0.42\n",
      "-0.39\n",
      "0.56\n",
      "0.39\n",
      "0.31\n",
      "-0.16\n",
      "-0.32\n",
      "0.54\n",
      "0.22\n",
      "-0.14\n",
      "0.22\n",
      "0.26\n",
      "-0.01\n",
      "0.41\n",
      "0.18\n",
      "-0.18\n",
      "0.32\n",
      "-0.37\n",
      "-0.004\n",
      "-0.04\n",
      "-0.04\n",
      "0.13\n",
      "0.54\n",
      "0.26\n",
      "0.43\n",
      "0.07\n",
      "0.29\n",
      "0.24\n",
      "0.25\n",
      "-0.06\n",
      "0.32\n",
      "0.05\n",
      "0.04\n",
      "0.16\n",
      "0.06\n",
      "0.01\n",
      "0.01\n",
      "-0.4\n",
      "-0.24\n",
      "0.33\n",
      "0.21\n",
      "-0.14\n",
      "-0.15\n",
      "0.17\n",
      "-0.18\n",
      "-0.02\n",
      "-0.32\n",
      "0.36\n",
      "-0.28\n",
      "-0.16\n",
      "0.02\n",
      "0.47\n",
      "-0.23\n",
      "0.44\n",
      "-0.12\n",
      "0.04\n",
      "-0.32\n",
      "-0.16\n",
      "-0.23\n",
      "-0.42\n",
      "-0.36\n",
      "-0.25\n",
      "0.48\n",
      "-0.32\n",
      "0.01\n",
      "0.15\n",
      "0.23\n",
      "0.31\n",
      "-0.28\n",
      "-0.26\n",
      "0.28\n",
      "-0.02\n",
      "0.24\n",
      "0.1\n",
      "0.4\n",
      "-0.07\n",
      "-0.02\n",
      "-0.12\n",
      "-0.33\n",
      "0.35\n",
      "0.48\n",
      "0.06\n",
      "0.38\n",
      "0.09\n",
      "-0.36\n",
      "0.45\n",
      "0.17\n",
      "0.35\n",
      "0.1\n",
      "0.31\n",
      "-0.11\n",
      "0.46\n",
      "0.29\n",
      "-0.01\n",
      "0.41\n",
      "-0.04\n",
      "0.25\n",
      "-0.29\n",
      "-0.06\n",
      "0.21\n",
      "0.12\n",
      "0.02\n",
      "0.01\n",
      "-0.27\n",
      "-0.12\n",
      "0.28\n",
      "-0.47\n",
      "0.39\n",
      "0.33\n",
      "-0.05\n",
      "-0.01\n",
      "-0.12\n",
      "0.18\n",
      "-0.36\n",
      "0.01\n",
      "-0.12\n",
      "0.09\n",
      "0.43\n",
      "0.11\n",
      "0.38\n",
      "0.35\n",
      "0.07\n",
      "0.32\n",
      "-0.26\n",
      "0.04\n",
      "0.22\n",
      "0.09\n",
      "-0.18\n",
      "0.17\n",
      "-0.44\n",
      "0.38\n",
      "0.51\n",
      "andr aara h\n",
      "-0.4\n",
      "0.35\n",
      "-0.41\n",
      "-0.21\n",
      "0.13\n",
      "-0.31\n",
      "-0.6\n",
      "-0.15\n",
      "0.54\n",
      "-0.16\n",
      "-0.16\n",
      "-0.44\n",
      "0.3\n",
      "-0.02\n",
      "-0.23\n",
      "0.09\n",
      "0.23\n",
      "0.48\n",
      "0.14\n",
      "-0.1\n",
      "0.45\n",
      "0.0005\n",
      "0.47\n",
      "0.27\n",
      "-0.59\n",
      "0.54\n",
      "0.43\n",
      "-0.34\n",
      "-0.24\n",
      "0.09\n",
      "-0.03\n",
      "-0.15\n",
      "-0.55\n",
      "-0.29\n",
      "0.18\n",
      "0.14\n",
      "-0.11\n",
      "0.08\n",
      "-0.17\n",
      "0.004\n",
      "-0.41\n",
      "0.46\n",
      "0.3\n",
      "0.18\n",
      "0.28\n",
      "-0.12\n",
      "0.08\n",
      "0.26\n",
      "0.21\n",
      "-0.55\n",
      "andr aara h\n",
      "-0.09\n",
      "0.23\n",
      "0.68\n",
      "-0.33\n",
      "0.05\n",
      "0.65\n",
      "-0.65\n",
      "-0.37\n",
      "-0.1\n",
      "0.42\n",
      "andr aara h\n",
      "-0.16\n",
      "-0.07\n",
      "-0.13\n",
      "-0.05\n",
      "0.01\n",
      "-0.13\n",
      "-0.08\n",
      "-0.17\n",
      "-0.13\n",
      "0.11\n",
      "-0.02\n",
      "0.09\n",
      "-0.02\n",
      "0.01\n",
      "-0.05\n",
      "-0.02\n",
      "-0.1\n",
      "0.03\n",
      "-0.14\n",
      "-0.1\n",
      "-0.09\n",
      "0.06\n",
      "-0.1\n",
      "-0.01\n",
      "-0.16\n",
      "0.09\n",
      "0.17\n",
      "0.24\n",
      "0.11\n",
      "0.13\n",
      "0.11\n",
      "0.13\n",
      "0.06\n",
      "0.18\n",
      "0.17\n",
      "0.02\n",
      "0.09\n",
      "0.11\n",
      "0.08\n",
      "0.11\n",
      "andr aara h\n",
      "0.01\n",
      "-0.21\n",
      "0.14\n",
      "-0.14\n",
      "0.05\n",
      "-0.27\n",
      "-0.18\n",
      "0.29\n",
      "0.36\n",
      "0.21\n",
      "-0.01\n",
      "0.06\n",
      "0.39\n",
      "-0.2\n",
      "-0.28\n",
      "0.26\n",
      "0.52\n",
      "-0.26\n",
      "0.64\n",
      "0.04\n",
      "-0.35\n",
      "0.13\n",
      "0.27\n",
      "0.53\n",
      "0.01\n",
      "-0.12\n",
      "-0.32\n",
      "0.57\n",
      "0.73\n",
      "0.32\n",
      "0.2\n",
      "-0.49\n",
      "0.77\n",
      "-0.27\n",
      "-0.1\n",
      "0.13\n",
      "0.67\n",
      "-0.04\n",
      "-0.24\n",
      "-0.52\n",
      "-0.07\n",
      "0.23\n",
      "0.43\n",
      "0.7\n",
      "0.69\n",
      "-0.54\n",
      "0.7\n",
      "0.67\n",
      "0.32\n",
      "0.57\n",
      "andr aara h\n",
      "0.02\n",
      "-0.17\n",
      "0.16\n",
      "0.35\n",
      "0.11\n",
      "-0.3\n",
      "-0.25\n",
      "0.1\n",
      "-0.13\n",
      "-0.08\n",
      "-0.42\n",
      "0.51\n",
      "-0.39\n",
      "0.27\n",
      "-0.03\n",
      "0.07\n",
      "-0.28\n",
      "0.47\n",
      "0.34\n",
      "-0.24\n",
      "0.19\n",
      "0.27\n",
      "0.08\n",
      "0.22\n",
      "-0.33\n",
      "0.3\n",
      "0.09\n",
      "0.37\n",
      "0.23\n",
      "0.12\n",
      "0.26\n",
      "-0.02\n",
      "-0.38\n",
      "0.18\n",
      "0.06\n",
      "-0.16\n",
      "0.19\n",
      "0.34\n",
      "-0.25\n",
      "-0.25\n",
      "0.08\n",
      "0.04\n",
      "0.34\n",
      "0.35\n",
      "0.53\n",
      "0.49\n",
      "-0.23\n",
      "0.35\n",
      "-0.42\n",
      "-0.19\n",
      "-0.39\n",
      "0.03\n",
      "-0.42\n",
      "0.45\n",
      "-0.16\n",
      "0.22\n",
      "0.05\n",
      "-0.06\n",
      "0.25\n",
      "0.56\n",
      "0.1\n",
      "0.24\n",
      "-0.26\n",
      "-0.19\n",
      "0.49\n",
      "0.14\n",
      "-0.01\n",
      "-0.22\n",
      "-0.17\n",
      "0.33\n",
      "0.21\n",
      "0.43\n",
      "0.2\n",
      "0.26\n",
      "-0.2\n",
      "0.39\n",
      "-0.31\n",
      "0.18\n",
      "0.34\n",
      "0.38\n",
      "0.39\n",
      "0.06\n",
      "-0.2\n",
      "0.51\n",
      "-0.14\n",
      "-0.28\n",
      "0.01\n",
      "0.45\n",
      "0.12\n",
      "0.24\n",
      "-0.24\n",
      "-0.28\n",
      "-0.29\n",
      "0.6\n",
      "0.52\n",
      "0.43\n",
      "-0.24\n",
      "0.48\n",
      "-0.16\n",
      "-0.02\n",
      "-0.23\n",
      "-0.12\n",
      "-0.36\n",
      "-0.22\n",
      "-0.15\n",
      "0.1\n",
      "0.4\n",
      "0.003\n",
      "0.21\n",
      "0.1\n",
      "0.05\n",
      "0.15\n",
      "-0.3\n",
      "0.19\n",
      "-0.22\n",
      "-0.09\n",
      "-0.28\n",
      "-0.2\n",
      "-0.35\n",
      "-0.35\n",
      "0.41\n",
      "0.41\n",
      "0.07\n",
      "0.12\n",
      "0.14\n",
      "-0.17\n",
      "-0.24\n",
      "0.56\n",
      "-0.38\n",
      "0.43\n",
      "0.42\n",
      "0.25\n",
      "-0.33\n",
      "0.21\n",
      "-0.17\n",
      "-0.19\n",
      "-0.26\n",
      "0.11\n",
      "-0.15\n",
      "0.06\n",
      "-0.16\n",
      "0.2\n",
      "0.4\n",
      "0.49\n",
      "-0.05\n",
      "0.2\n",
      "-0.11\n",
      "0.29\n",
      "-0.03\n",
      "-0.02\n",
      "-0.15\n",
      "0.42\n",
      "-0.31\n",
      "0.25\n",
      "0.48\n",
      "0.26\n",
      "0.36\n",
      "0.37\n",
      "-0.26\n",
      "0.48\n",
      "0.51\n",
      "0.32\n",
      "0.09\n",
      "0.53\n",
      "-0.02\n",
      "0.05\n",
      "0.19\n",
      "0.52\n",
      "-0.15\n",
      "-0.34\n",
      "0.33\n",
      "-0.04\n",
      "-0.11\n",
      "0.11\n",
      "0.38\n",
      "0.37\n",
      "-0.31\n",
      "-0.04\n",
      "0.37\n",
      "0.16\n",
      "-0.28\n",
      "-0.31\n",
      "-0.26\n",
      "0.42\n",
      "0.05\n",
      "0.09\n",
      "-0.44\n",
      "0.34\n",
      "-0.21\n",
      "0.42\n",
      "-0.39\n",
      "0.2\n",
      "0.4\n",
      "-0.11\n",
      "0.25\n",
      "0.13\n",
      "0.21\n",
      "0.56\n",
      "0.17\n",
      "-0.01\n",
      "andr aara h\n",
      "0.33\n",
      "0.07\n",
      "-0.12\n",
      "0.23\n",
      "-0.08\n",
      "0.26\n",
      "-0.28\n",
      "0.38\n",
      "0.41\n",
      "0.51\n",
      "0.19\n",
      "-0.37\n",
      "-0.45\n",
      "0.49\n",
      "-0.33\n",
      "0.01\n",
      "-0.22\n",
      "-0.23\n",
      "0.31\n",
      "0.24\n",
      "0.5\n",
      "0.14\n",
      "-0.22\n",
      "0.2\n",
      "-0.01\n",
      "-0.21\n",
      "0.1\n",
      "0.21\n",
      "0.25\n",
      "0.33\n",
      "-0.14\n",
      "0.39\n",
      "0.02\n",
      "-0.16\n",
      "0.16\n",
      "0.45\n",
      "-0.19\n",
      "-0.12\n",
      "0.56\n",
      "-0.2\n",
      "0.57\n",
      "-0.29\n",
      "-0.14\n",
      "0.19\n",
      "-0.26\n",
      "0.42\n",
      "0.29\n",
      "0.23\n",
      "-0.26\n",
      "0.35\n",
      "0.46\n",
      "-0.3\n",
      "0.2\n",
      "0.06\n",
      "0.38\n",
      "-0.12\n",
      "-0.02\n",
      "-0.33\n",
      "0.01\n",
      "0.42\n",
      "-0.28\n",
      "-0.34\n",
      "0.06\n",
      "-0.43\n",
      "-0.27\n",
      "-0.03\n",
      "0.3\n",
      "0.12\n",
      "0.42\n",
      "-0.4\n",
      "0.57\n",
      "0.39\n",
      "0.31\n",
      "-0.16\n",
      "-0.31\n",
      "0.55\n",
      "0.22\n",
      "-0.13\n",
      "0.22\n",
      "0.26\n",
      "-0.01\n",
      "0.41\n",
      "0.18\n",
      "-0.18\n",
      "0.32\n",
      "-0.37\n",
      "-0.004\n",
      "-0.04\n",
      "-0.04\n",
      "0.13\n",
      "0.55\n",
      "0.26\n",
      "0.43\n",
      "0.08\n",
      "0.29\n",
      "0.26\n",
      "0.25\n",
      "-0.06\n",
      "0.34\n",
      "0.05\n",
      "0.02\n",
      "0.18\n",
      "0.06\n",
      "-0.01\n",
      "0.01\n",
      "-0.41\n",
      "-0.25\n",
      "0.32\n",
      "0.19\n",
      "-0.16\n",
      "-0.14\n",
      "0.16\n",
      "-0.19\n",
      "-0.02\n",
      "-0.32\n",
      "0.36\n",
      "-0.28\n",
      "-0.16\n",
      "0.02\n",
      "0.47\n",
      "-0.23\n",
      "0.44\n",
      "-0.12\n",
      "0.04\n",
      "-0.32\n",
      "-0.16\n",
      "-0.23\n",
      "-0.42\n",
      "-0.36\n",
      "-0.25\n",
      "0.48\n",
      "-0.32\n",
      "0.01\n",
      "0.16\n",
      "0.24\n",
      "0.31\n",
      "-0.28\n",
      "-0.26\n",
      "0.29\n",
      "-0.02\n",
      "0.27\n",
      "0.1\n",
      "0.4\n",
      "-0.03\n",
      "-0.02\n",
      "-0.1\n",
      "-0.33\n",
      "0.35\n",
      "0.5\n",
      "0.1\n",
      "0.39\n",
      "0.09\n",
      "-0.36\n",
      "0.46\n",
      "0.18\n",
      "0.36\n",
      "0.11\n",
      "0.31\n",
      "-0.1\n",
      "0.47\n",
      "0.29\n",
      "-0.02\n",
      "0.41\n",
      "-0.05\n",
      "0.25\n",
      "-0.29\n",
      "-0.06\n",
      "0.21\n",
      "0.12\n",
      "0.02\n",
      "0.02\n",
      "-0.27\n",
      "-0.12\n",
      "0.29\n",
      "-0.46\n",
      "0.41\n",
      "0.33\n",
      "-0.03\n",
      "0.003\n",
      "-0.11\n",
      "0.18\n",
      "-0.34\n",
      "0.01\n",
      "-0.12\n",
      "0.11\n",
      "0.43\n",
      "0.11\n",
      "0.38\n",
      "0.35\n",
      "0.07\n",
      "0.33\n",
      "-0.26\n",
      "0.04\n",
      "0.23\n",
      "0.1\n",
      "-0.17\n",
      "0.17\n",
      "-0.44\n",
      "0.38\n",
      "0.51\n",
      "andr aara h\n",
      "-0.39\n",
      "0.35\n",
      "-0.41\n",
      "-0.21\n",
      "0.13\n",
      "-0.31\n",
      "-0.6\n",
      "-0.15\n",
      "0.54\n",
      "-0.15\n",
      "-0.16\n",
      "-0.44\n",
      "0.3\n",
      "-0.02\n",
      "-0.23\n",
      "0.1\n",
      "0.23\n",
      "0.48\n",
      "0.15\n",
      "-0.1\n",
      "0.46\n",
      "0.01\n",
      "0.46\n",
      "0.27\n",
      "-0.59\n",
      "0.55\n",
      "0.44\n",
      "-0.34\n",
      "-0.24\n",
      "0.09\n",
      "-0.03\n",
      "-0.15\n",
      "-0.55\n",
      "-0.29\n",
      "0.19\n",
      "0.15\n",
      "-0.12\n",
      "0.08\n",
      "-0.17\n",
      "0.001\n",
      "-0.41\n",
      "0.46\n",
      "0.3\n",
      "0.19\n",
      "0.27\n",
      "-0.12\n",
      "0.08\n",
      "0.26\n",
      "0.22\n",
      "-0.55\n",
      "andr aara h\n",
      "-0.09\n",
      "0.23\n",
      "0.69\n",
      "-0.34\n",
      "0.05\n",
      "0.65\n",
      "-0.65\n",
      "-0.37\n",
      "-0.1\n",
      "0.42\n",
      "andr aara h\n",
      "-0.13\n",
      "-0.05\n",
      "-0.13\n",
      "-0.04\n",
      "0.01\n",
      "-0.12\n",
      "-0.07\n",
      "-0.19\n",
      "-0.12\n",
      "0.14\n",
      "-0.01\n",
      "0.09\n",
      "-0.06\n",
      "-0.002\n",
      "-0.0005\n",
      "-0.02\n",
      "-0.07\n",
      "0.07\n",
      "-0.11\n",
      "-0.14\n",
      "-0.02\n",
      "0.02\n",
      "0.07\n",
      "-0.05\n",
      "-0.05\n",
      "0.09\n",
      "0.18\n",
      "0.26\n",
      "0.13\n",
      "0.15\n",
      "0.07\n",
      "0.12\n",
      "0.03\n",
      "0.16\n",
      "0.1\n",
      "0.17\n",
      "0.14\n",
      "0.17\n",
      "0.16\n",
      "0.16\n",
      "andr aara h\n",
      "-0.04\n",
      "-0.26\n",
      "0.12\n",
      "-0.12\n",
      "0.04\n",
      "-0.19\n",
      "-0.2\n",
      "0.28\n",
      "0.34\n",
      "0.21\n",
      "0.01\n",
      "-0.01\n",
      "0.39\n",
      "-0.12\n",
      "-0.21\n",
      "0.31\n",
      "0.51\n",
      "-0.25\n",
      "0.64\n",
      "0.1\n",
      "-0.36\n",
      "0.17\n",
      "0.27\n",
      "0.54\n",
      "0.01\n",
      "-0.07\n",
      "-0.34\n",
      "0.58\n",
      "0.73\n",
      "0.32\n",
      "0.19\n",
      "-0.57\n",
      "0.77\n",
      "-0.23\n",
      "-0.07\n",
      "0.18\n",
      "0.66\n",
      "-0.03\n",
      "-0.25\n",
      "-0.49\n",
      "-0.07\n",
      "0.21\n",
      "0.43\n",
      "0.71\n",
      "0.69\n",
      "-0.53\n",
      "0.7\n",
      "0.66\n",
      "0.32\n",
      "0.55\n",
      "andr aara h\n",
      "0.02\n",
      "-0.17\n",
      "0.16\n",
      "0.36\n",
      "0.11\n",
      "-0.3\n",
      "-0.25\n",
      "0.11\n",
      "-0.13\n",
      "-0.07\n",
      "-0.48\n",
      "0.49\n",
      "-0.39\n",
      "0.27\n",
      "-0.02\n",
      "0.08\n",
      "-0.28\n",
      "0.48\n",
      "0.35\n",
      "-0.22\n",
      "0.18\n",
      "0.26\n",
      "0.07\n",
      "0.22\n",
      "-0.35\n",
      "0.27\n",
      "0.09\n",
      "0.36\n",
      "0.23\n",
      "0.09\n",
      "0.26\n",
      "-0.05\n",
      "-0.38\n",
      "0.17\n",
      "0.05\n",
      "-0.18\n",
      "0.2\n",
      "0.34\n",
      "-0.26\n",
      "-0.27\n",
      "0.08\n",
      "0.04\n",
      "0.34\n",
      "0.35\n",
      "0.52\n",
      "0.5\n",
      "-0.23\n",
      "0.35\n",
      "-0.42\n",
      "-0.18\n",
      "-0.43\n",
      "0.02\n",
      "-0.42\n",
      "0.45\n",
      "-0.16\n",
      "0.22\n",
      "0.05\n",
      "-0.06\n",
      "0.25\n",
      "0.56\n",
      "0.1\n",
      "0.24\n",
      "-0.27\n",
      "-0.19\n",
      "0.49\n",
      "0.12\n",
      "-0.01\n",
      "-0.22\n",
      "-0.17\n",
      "0.32\n",
      "0.22\n",
      "0.42\n",
      "0.2\n",
      "0.25\n",
      "-0.2\n",
      "0.38\n",
      "-0.31\n",
      "0.17\n",
      "0.33\n",
      "0.37\n",
      "0.38\n",
      "0.06\n",
      "-0.23\n",
      "0.5\n",
      "-0.15\n",
      "-0.3\n",
      "0.01\n",
      "0.44\n",
      "0.12\n",
      "0.24\n",
      "-0.25\n",
      "-0.29\n",
      "-0.29\n",
      "0.59\n",
      "0.5\n",
      "0.42\n",
      "-0.24\n",
      "0.46\n",
      "-0.17\n",
      "-0.03\n",
      "-0.28\n",
      "-0.17\n",
      "-0.42\n",
      "-0.26\n",
      "-0.21\n",
      "0.05\n",
      "0.4\n",
      "-0.05\n",
      "0.21\n",
      "0.09\n",
      "0.05\n",
      "0.1\n",
      "-0.3\n",
      "0.14\n",
      "-0.27\n",
      "-0.14\n",
      "-0.28\n",
      "-0.23\n",
      "-0.4\n",
      "-0.4\n",
      "0.42\n",
      "0.41\n",
      "0.07\n",
      "0.12\n",
      "0.14\n",
      "-0.16\n",
      "-0.24\n",
      "0.56\n",
      "-0.38\n",
      "0.44\n",
      "0.37\n",
      "0.25\n",
      "-0.33\n",
      "0.22\n",
      "-0.2\n",
      "-0.19\n",
      "-0.26\n",
      "0.12\n",
      "-0.14\n",
      "0.06\n",
      "-0.17\n",
      "0.19\n",
      "0.4\n",
      "0.49\n",
      "-0.06\n",
      "0.18\n",
      "-0.11\n",
      "0.28\n",
      "-0.03\n",
      "-0.03\n",
      "-0.15\n",
      "0.41\n",
      "-0.31\n",
      "0.24\n",
      "0.48\n",
      "0.26\n",
      "0.36\n",
      "0.37\n",
      "-0.27\n",
      "0.47\n",
      "0.52\n",
      "0.32\n",
      "0.1\n",
      "0.53\n",
      "-0.02\n",
      "0.06\n",
      "0.19\n",
      "0.52\n",
      "-0.15\n",
      "-0.33\n",
      "0.29\n",
      "-0.05\n",
      "-0.11\n",
      "0.11\n",
      "0.39\n",
      "0.37\n",
      "-0.31\n",
      "-0.04\n",
      "0.37\n",
      "0.17\n",
      "-0.3\n",
      "-0.33\n",
      "-0.29\n",
      "0.39\n",
      "0.02\n",
      "0.06\n",
      "-0.43\n",
      "0.32\n",
      "-0.21\n",
      "0.39\n",
      "-0.39\n",
      "0.17\n",
      "0.4\n",
      "-0.14\n",
      "0.22\n",
      "0.1\n",
      "0.21\n",
      "0.53\n",
      "0.14\n",
      "-0.03\n",
      "andr aara h\n",
      "0.33\n",
      "0.07\n",
      "-0.11\n",
      "0.23\n",
      "-0.08\n",
      "0.26\n",
      "-0.28\n",
      "0.38\n",
      "0.41\n",
      "0.51\n",
      "0.19\n",
      "-0.37\n",
      "-0.44\n",
      "0.5\n",
      "-0.34\n",
      "0.01\n",
      "-0.21\n",
      "-0.23\n",
      "0.31\n",
      "0.24\n",
      "0.5\n",
      "0.14\n",
      "-0.22\n",
      "0.2\n",
      "-0.01\n",
      "-0.21\n",
      "0.09\n",
      "0.21\n",
      "0.26\n",
      "0.34\n",
      "-0.14\n",
      "0.39\n",
      "0.02\n",
      "-0.16\n",
      "0.16\n",
      "0.44\n",
      "-0.19\n",
      "-0.13\n",
      "0.56\n",
      "-0.2\n",
      "0.57\n",
      "-0.3\n",
      "-0.13\n",
      "0.19\n",
      "-0.26\n",
      "0.42\n",
      "0.29\n",
      "0.23\n",
      "-0.26\n",
      "0.35\n",
      "0.45\n",
      "-0.3\n",
      "0.2\n",
      "0.06\n",
      "0.38\n",
      "-0.13\n",
      "-0.03\n",
      "-0.32\n",
      "0.01\n",
      "0.42\n",
      "-0.27\n",
      "-0.35\n",
      "0.04\n",
      "-0.43\n",
      "-0.27\n",
      "-0.02\n",
      "0.29\n",
      "0.12\n",
      "0.42\n",
      "-0.39\n",
      "0.57\n",
      "0.39\n",
      "0.31\n",
      "-0.16\n",
      "-0.32\n",
      "0.54\n",
      "0.22\n",
      "-0.14\n",
      "0.22\n",
      "0.26\n",
      "-0.01\n",
      "0.41\n",
      "0.18\n",
      "-0.18\n",
      "0.32\n",
      "-0.37\n",
      "-0.004\n",
      "-0.04\n",
      "-0.04\n",
      "0.13\n",
      "0.55\n",
      "0.26\n",
      "0.43\n",
      "0.07\n",
      "0.29\n",
      "0.26\n",
      "0.25\n",
      "-0.06\n",
      "0.33\n",
      "0.05\n",
      "0.12\n",
      "0.17\n",
      "0.06\n",
      "0.07\n",
      "0.01\n",
      "-0.33\n",
      "-0.25\n",
      "0.32\n",
      "0.27\n",
      "-0.13\n",
      "-0.15\n",
      "0.17\n",
      "-0.18\n",
      "-0.02\n",
      "-0.31\n",
      "0.36\n",
      "-0.28\n",
      "-0.15\n",
      "0.03\n",
      "0.47\n",
      "-0.23\n",
      "0.44\n",
      "-0.12\n",
      "0.04\n",
      "-0.32\n",
      "-0.16\n",
      "-0.23\n",
      "-0.42\n",
      "-0.36\n",
      "-0.25\n",
      "0.48\n",
      "-0.32\n",
      "0.01\n",
      "0.15\n",
      "0.24\n",
      "0.3\n",
      "-0.28\n",
      "-0.26\n",
      "0.28\n",
      "-0.02\n",
      "0.28\n",
      "0.1\n",
      "0.41\n",
      "-0.02\n",
      "-0.02\n",
      "-0.09\n",
      "-0.33\n",
      "0.35\n",
      "0.52\n",
      "0.1\n",
      "0.39\n",
      "0.09\n",
      "-0.36\n",
      "0.46\n",
      "0.18\n",
      "0.35\n",
      "0.11\n",
      "0.31\n",
      "-0.1\n",
      "0.47\n",
      "0.29\n",
      "-0.01\n",
      "0.41\n",
      "-0.05\n",
      "0.26\n",
      "-0.29\n",
      "-0.07\n",
      "0.21\n",
      "0.13\n",
      "0.02\n",
      "0.01\n",
      "-0.26\n",
      "-0.13\n",
      "0.28\n",
      "-0.46\n",
      "0.41\n",
      "0.33\n",
      "-0.04\n",
      "-0.001\n",
      "-0.11\n",
      "0.19\n",
      "-0.36\n",
      "0.01\n",
      "-0.12\n",
      "0.1\n",
      "0.43\n",
      "0.11\n",
      "0.38\n",
      "0.36\n",
      "0.07\n",
      "0.33\n",
      "-0.26\n",
      "0.04\n",
      "0.23\n",
      "0.09\n",
      "-0.18\n",
      "0.17\n",
      "-0.44\n",
      "0.38\n",
      "0.51\n",
      "andr aara h\n",
      "-0.39\n",
      "0.35\n",
      "-0.41\n",
      "-0.21\n",
      "0.13\n",
      "-0.31\n",
      "-0.6\n",
      "-0.15\n",
      "0.54\n",
      "-0.16\n",
      "-0.16\n",
      "-0.44\n",
      "0.3\n",
      "-0.02\n",
      "-0.22\n",
      "0.1\n",
      "0.23\n",
      "0.48\n",
      "0.14\n",
      "-0.1\n",
      "0.46\n",
      "0.01\n",
      "0.46\n",
      "0.27\n",
      "-0.6\n",
      "0.55\n",
      "0.44\n",
      "-0.34\n",
      "-0.24\n",
      "0.09\n",
      "-0.03\n",
      "-0.15\n",
      "-0.55\n",
      "-0.29\n",
      "0.19\n",
      "0.15\n",
      "-0.12\n",
      "0.08\n",
      "-0.16\n",
      "0.005\n",
      "-0.41\n",
      "0.46\n",
      "0.3\n",
      "0.18\n",
      "0.27\n",
      "-0.11\n",
      "0.08\n",
      "0.26\n",
      "0.21\n",
      "-0.55\n",
      "andr aara h\n",
      "-0.09\n",
      "0.23\n",
      "0.69\n",
      "-0.34\n",
      "0.05\n",
      "0.65\n",
      "-0.65\n",
      "-0.37\n",
      "-0.1\n",
      "0.42\n",
      "andr aara h\n",
      "-0.1\n",
      "-0.04\n",
      "-0.1\n",
      "-0.05\n",
      "-0.01\n",
      "-0.07\n",
      "-0.01\n",
      "-0.15\n",
      "-0.06\n",
      "0.12\n",
      "-0.05\n",
      "0.04\n",
      "-0.05\n",
      "-0.06\n",
      "-0.06\n",
      "-0.01\n",
      "-0.08\n",
      "0.05\n",
      "-0.12\n",
      "-0.12\n",
      "-0.02\n",
      "0.1\n",
      "0.003\n",
      "0.06\n",
      "-0.05\n",
      "-0.01\n",
      "0.11\n",
      "0.13\n",
      "0.07\n",
      "0.07\n",
      "0.05\n",
      "0.13\n",
      "0.07\n",
      "0.15\n",
      "0.11\n",
      "0.09\n",
      "0.12\n",
      "0.15\n",
      "0.14\n",
      "0.23\n",
      "andr aara h\n",
      "0.01\n",
      "-0.27\n",
      "0.1\n",
      "-0.16\n",
      "0.002\n",
      "-0.16\n",
      "-0.19\n",
      "0.27\n",
      "0.32\n",
      "0.17\n",
      "-0.01\n",
      "-0.03\n",
      "0.37\n",
      "-0.12\n",
      "-0.19\n",
      "0.33\n",
      "0.5\n",
      "-0.22\n",
      "0.62\n",
      "0.09\n",
      "-0.3\n",
      "0.14\n",
      "0.25\n",
      "0.5\n",
      "-0.02\n",
      "-0.04\n",
      "-0.32\n",
      "0.54\n",
      "0.7\n",
      "0.31\n",
      "0.23\n",
      "-0.59\n",
      "0.78\n",
      "-0.21\n",
      "-0.05\n",
      "0.19\n",
      "0.69\n",
      "-0.002\n",
      "-0.24\n",
      "-0.52\n",
      "-0.04\n",
      "0.17\n",
      "0.44\n",
      "0.7\n",
      "0.69\n",
      "-0.62\n",
      "0.72\n",
      "0.68\n",
      "0.33\n",
      "0.51\n",
      "andr aara h\n",
      "0.03\n",
      "-0.15\n",
      "0.18\n",
      "0.37\n",
      "0.12\n",
      "-0.28\n",
      "-0.25\n",
      "0.12\n",
      "-0.13\n",
      "-0.04\n",
      "-0.48\n",
      "0.53\n",
      "-0.39\n",
      "0.28\n",
      "0.02\n",
      "0.09\n",
      "-0.28\n",
      "0.49\n",
      "0.36\n",
      "-0.23\n",
      "0.15\n",
      "0.24\n",
      "0.04\n",
      "0.17\n",
      "-0.36\n",
      "0.3\n",
      "0.09\n",
      "0.33\n",
      "0.23\n",
      "0.09\n",
      "0.26\n",
      "-0.07\n",
      "-0.38\n",
      "0.14\n",
      "0.02\n",
      "-0.2\n",
      "0.19\n",
      "0.31\n",
      "-0.28\n",
      "-0.28\n",
      "0.08\n",
      "0.04\n",
      "0.34\n",
      "0.34\n",
      "0.52\n",
      "0.49\n",
      "-0.22\n",
      "0.35\n",
      "-0.42\n",
      "-0.16\n",
      "-0.41\n",
      "0.02\n",
      "-0.43\n",
      "0.45\n",
      "-0.17\n",
      "0.22\n",
      "0.05\n",
      "-0.08\n",
      "0.25\n",
      "0.56\n",
      "0.09\n",
      "0.22\n",
      "-0.27\n",
      "-0.21\n",
      "0.48\n",
      "0.13\n",
      "-0.01\n",
      "-0.23\n",
      "-0.17\n",
      "0.32\n",
      "0.23\n",
      "0.41\n",
      "0.19\n",
      "0.24\n",
      "-0.23\n",
      "0.36\n",
      "-0.31\n",
      "0.14\n",
      "0.33\n",
      "0.36\n",
      "0.38\n",
      "0.05\n",
      "-0.22\n",
      "0.5\n",
      "-0.15\n",
      "-0.26\n",
      "0.01\n",
      "0.45\n",
      "0.12\n",
      "0.24\n",
      "-0.23\n",
      "-0.29\n",
      "-0.29\n",
      "0.6\n",
      "0.51\n",
      "0.42\n",
      "-0.25\n",
      "0.46\n",
      "-0.16\n",
      "-0.02\n",
      "-0.31\n",
      "-0.2\n",
      "-0.43\n",
      "-0.28\n",
      "-0.24\n",
      "0.02\n",
      "0.41\n",
      "-0.07\n",
      "0.21\n",
      "0.1\n",
      "0.05\n",
      "0.07\n",
      "-0.3\n",
      "0.11\n",
      "-0.3\n",
      "-0.17\n",
      "-0.29\n",
      "-0.25\n",
      "-0.44\n",
      "-0.43\n",
      "0.43\n",
      "0.42\n",
      "0.09\n",
      "0.12\n",
      "0.15\n",
      "-0.16\n",
      "-0.23\n",
      "0.57\n",
      "-0.38\n",
      "0.46\n",
      "0.39\n",
      "0.26\n",
      "-0.33\n",
      "0.23\n",
      "-0.17\n",
      "-0.18\n",
      "-0.26\n",
      "0.11\n",
      "-0.14\n",
      "0.07\n",
      "-0.18\n",
      "0.18\n",
      "0.39\n",
      "0.47\n",
      "-0.07\n",
      "0.18\n",
      "-0.11\n",
      "0.27\n",
      "-0.03\n",
      "-0.03\n",
      "-0.14\n",
      "0.38\n",
      "-0.31\n",
      "0.23\n",
      "0.45\n",
      "0.24\n",
      "0.36\n",
      "0.34\n",
      "-0.28\n",
      "0.46\n",
      "0.5\n",
      "0.31\n",
      "0.07\n",
      "0.51\n",
      "-0.03\n",
      "0.03\n",
      "0.2\n",
      "0.51\n",
      "-0.15\n",
      "-0.33\n",
      "0.3\n",
      "-0.08\n",
      "-0.11\n",
      "0.1\n",
      "0.36\n",
      "0.35\n",
      "-0.31\n",
      "-0.08\n",
      "0.36\n",
      "0.15\n",
      "-0.35\n",
      "-0.38\n",
      "-0.35\n",
      "0.33\n",
      "-0.01\n",
      "0.03\n",
      "-0.44\n",
      "0.27\n",
      "-0.21\n",
      "0.35\n",
      "-0.39\n",
      "0.12\n",
      "0.4\n",
      "-0.19\n",
      "0.15\n",
      "0.06\n",
      "0.21\n",
      "0.46\n",
      "0.09\n",
      "-0.08\n",
      "andr aara h\n",
      "0.33\n",
      "0.08\n",
      "-0.11\n",
      "0.23\n",
      "-0.08\n",
      "0.26\n",
      "-0.28\n",
      "0.39\n",
      "0.41\n",
      "0.51\n",
      "0.19\n",
      "-0.37\n",
      "-0.44\n",
      "0.5\n",
      "-0.33\n",
      "0.01\n",
      "-0.22\n",
      "-0.22\n",
      "0.31\n",
      "0.24\n",
      "0.51\n",
      "0.15\n",
      "-0.22\n",
      "0.21\n",
      "-0.01\n",
      "-0.2\n",
      "0.09\n",
      "0.21\n",
      "0.26\n",
      "0.34\n",
      "-0.15\n",
      "0.39\n",
      "0.02\n",
      "-0.17\n",
      "0.16\n",
      "0.44\n",
      "-0.19\n",
      "-0.12\n",
      "0.55\n",
      "-0.21\n",
      "0.57\n",
      "-0.29\n",
      "-0.13\n",
      "0.19\n",
      "-0.26\n",
      "0.42\n",
      "0.29\n",
      "0.23\n",
      "-0.25\n",
      "0.35\n",
      "0.45\n",
      "-0.29\n",
      "0.2\n",
      "0.05\n",
      "0.38\n",
      "-0.13\n",
      "-0.03\n",
      "-0.33\n",
      "0.002\n",
      "0.42\n",
      "-0.27\n",
      "-0.35\n",
      "0.05\n",
      "-0.43\n",
      "-0.27\n",
      "-0.02\n",
      "0.3\n",
      "0.13\n",
      "0.42\n",
      "-0.39\n",
      "0.56\n",
      "0.39\n",
      "0.31\n",
      "-0.16\n",
      "-0.31\n",
      "0.54\n",
      "0.22\n",
      "-0.13\n",
      "0.22\n",
      "0.26\n",
      "-0.01\n",
      "0.41\n",
      "0.18\n",
      "-0.18\n",
      "0.32\n",
      "-0.37\n",
      "-0.004\n",
      "-0.04\n",
      "-0.04\n",
      "0.13\n",
      "0.55\n",
      "0.25\n",
      "0.43\n",
      "0.08\n",
      "0.29\n",
      "0.25\n",
      "0.25\n",
      "-0.06\n",
      "0.34\n",
      "0.03\n",
      "0.07\n",
      "0.17\n",
      "0.06\n",
      "0.02\n",
      "0.01\n",
      "-0.38\n",
      "-0.24\n",
      "0.33\n",
      "0.22\n",
      "-0.15\n",
      "-0.15\n",
      "0.17\n",
      "-0.18\n",
      "-0.02\n",
      "-0.31\n",
      "0.36\n",
      "-0.27\n",
      "-0.16\n",
      "0.02\n",
      "0.46\n",
      "-0.22\n",
      "0.44\n",
      "-0.12\n",
      "0.05\n",
      "-0.33\n",
      "-0.15\n",
      "-0.24\n",
      "-0.42\n",
      "-0.36\n",
      "-0.26\n",
      "0.48\n",
      "-0.32\n",
      "0.01\n",
      "0.16\n",
      "0.24\n",
      "0.31\n",
      "-0.28\n",
      "-0.25\n",
      "0.28\n",
      "-0.03\n",
      "0.23\n",
      "0.1\n",
      "0.41\n",
      "-0.09\n",
      "-0.01\n",
      "-0.14\n",
      "-0.33\n",
      "0.35\n",
      "0.47\n",
      "0.05\n",
      "0.38\n",
      "0.09\n",
      "-0.36\n",
      "0.45\n",
      "0.18\n",
      "0.35\n",
      "0.11\n",
      "0.31\n",
      "-0.11\n",
      "0.46\n",
      "0.29\n",
      "-0.01\n",
      "0.41\n",
      "-0.04\n",
      "0.26\n",
      "-0.29\n",
      "-0.07\n",
      "0.21\n",
      "0.12\n",
      "0.03\n",
      "0.004\n",
      "-0.27\n",
      "-0.12\n",
      "0.27\n",
      "-0.45\n",
      "0.4\n",
      "0.34\n",
      "-0.03\n",
      "-0.02\n",
      "-0.13\n",
      "0.18\n",
      "-0.34\n",
      "0.01\n",
      "-0.12\n",
      "0.1\n",
      "0.43\n",
      "0.11\n",
      "0.38\n",
      "0.35\n",
      "0.06\n",
      "0.32\n",
      "-0.26\n",
      "0.04\n",
      "0.22\n",
      "0.09\n",
      "-0.18\n",
      "0.17\n",
      "-0.44\n",
      "0.38\n",
      "0.5\n",
      "andr aara h\n",
      "-0.39\n",
      "0.35\n",
      "-0.4\n",
      "-0.21\n",
      "0.13\n",
      "-0.31\n",
      "-0.59\n",
      "-0.15\n",
      "0.54\n",
      "-0.16\n",
      "-0.16\n",
      "-0.44\n",
      "0.3\n",
      "-0.02\n",
      "-0.22\n",
      "0.1\n",
      "0.23\n",
      "0.48\n",
      "0.14\n",
      "-0.1\n",
      "0.46\n",
      "0.01\n",
      "0.46\n",
      "0.27\n",
      "-0.6\n",
      "0.55\n",
      "0.43\n",
      "-0.33\n",
      "-0.24\n",
      "0.09\n",
      "-0.03\n",
      "-0.15\n",
      "-0.55\n",
      "-0.29\n",
      "0.19\n",
      "0.15\n",
      "-0.11\n",
      "0.08\n",
      "-0.17\n",
      "-0.001\n",
      "-0.41\n",
      "0.46\n",
      "0.3\n",
      "0.19\n",
      "0.28\n",
      "-0.11\n",
      "0.08\n",
      "0.27\n",
      "0.21\n",
      "-0.55\n",
      "andr aara h\n",
      "-0.09\n",
      "0.23\n",
      "0.68\n",
      "-0.33\n",
      "0.05\n",
      "0.65\n",
      "-0.65\n",
      "-0.37\n",
      "-0.1\n",
      "0.42\n",
      "andr aara h\n",
      "-0.18\n",
      "-0.03\n",
      "-0.15\n",
      "-0.03\n",
      "0.01\n",
      "-0.17\n",
      "-0.09\n",
      "-0.25\n",
      "-0.13\n",
      "0.09\n",
      "-0.04\n",
      "0.06\n",
      "-0.03\n",
      "-0.05\n",
      "-0.05\n",
      "0.04\n",
      "-0.1\n",
      "0.07\n",
      "-0.1\n",
      "-0.12\n",
      "-0.09\n",
      "0.08\n",
      "-0.07\n",
      "0.03\n",
      "-0.1\n",
      "0.1\n",
      "0.13\n",
      "0.17\n",
      "0.14\n",
      "0.02\n",
      "0.12\n",
      "0.12\n",
      "0.1\n",
      "0.16\n",
      "0.11\n",
      "0.1\n",
      "0.13\n",
      "0.13\n",
      "0.13\n",
      "0.18\n",
      "andr aara h\n",
      "0.04\n",
      "-0.09\n",
      "0.17\n",
      "-0.04\n",
      "0.06\n",
      "-0.26\n",
      "-0.16\n",
      "0.35\n",
      "0.39\n",
      "0.19\n",
      "-0.02\n",
      "0.01\n",
      "0.35\n",
      "-0.12\n",
      "-0.21\n",
      "0.33\n",
      "0.51\n",
      "-0.19\n",
      "0.64\n",
      "0.03\n",
      "-0.33\n",
      "0.19\n",
      "0.3\n",
      "0.59\n",
      "0.02\n",
      "-0.21\n",
      "-0.34\n",
      "0.61\n",
      "0.76\n",
      "0.3\n",
      "0.2\n",
      "-0.49\n",
      "0.77\n",
      "-0.18\n",
      "-0.06\n",
      "0.16\n",
      "0.68\n",
      "0.03\n",
      "-0.23\n",
      "-0.55\n",
      "-0.1\n",
      "0.21\n",
      "0.41\n",
      "0.68\n",
      "0.66\n",
      "-0.51\n",
      "0.68\n",
      "0.65\n",
      "0.3\n",
      "0.51\n",
      "andr aara h\n",
      "0.01\n",
      "-0.19\n",
      "0.16\n",
      "0.36\n",
      "0.09\n",
      "-0.33\n",
      "-0.25\n",
      "0.11\n",
      "-0.13\n",
      "-0.08\n",
      "-0.42\n",
      "0.49\n",
      "-0.39\n",
      "0.26\n",
      "-0.03\n",
      "0.07\n",
      "-0.28\n",
      "0.46\n",
      "0.34\n",
      "-0.25\n",
      "0.16\n",
      "0.24\n",
      "0.07\n",
      "0.21\n",
      "-0.37\n",
      "0.29\n",
      "0.09\n",
      "0.34\n",
      "0.23\n",
      "0.07\n",
      "0.25\n",
      "-0.06\n",
      "-0.38\n",
      "0.15\n",
      "0.05\n",
      "-0.2\n",
      "0.2\n",
      "0.32\n",
      "-0.28\n",
      "-0.29\n",
      "0.08\n",
      "0.04\n",
      "0.35\n",
      "0.36\n",
      "0.51\n",
      "0.47\n",
      "-0.23\n",
      "0.36\n",
      "-0.42\n",
      "-0.18\n",
      "-0.36\n",
      "0.03\n",
      "-0.42\n",
      "0.45\n",
      "-0.14\n",
      "0.22\n",
      "0.05\n",
      "-0.07\n",
      "0.25\n",
      "0.56\n",
      "0.09\n",
      "0.24\n",
      "-0.27\n",
      "-0.22\n",
      "0.49\n",
      "0.16\n",
      "-0.02\n",
      "-0.23\n",
      "-0.17\n",
      "0.29\n",
      "0.07\n",
      "0.42\n",
      "0.2\n",
      "0.25\n",
      "-0.2\n",
      "0.38\n",
      "-0.31\n",
      "0.16\n",
      "0.33\n",
      "0.38\n",
      "0.36\n",
      "0.04\n",
      "-0.24\n",
      "0.45\n",
      "-0.17\n",
      "-0.27\n",
      "0.01\n",
      "0.42\n",
      "0.12\n",
      "0.21\n",
      "-0.46\n",
      "-0.3\n",
      "-0.29\n",
      "0.58\n",
      "0.48\n",
      "0.4\n",
      "-0.24\n",
      "0.44\n",
      "-0.19\n",
      "-0.04\n",
      "-0.32\n",
      "-0.21\n",
      "-0.45\n",
      "-0.14\n",
      "-0.26\n",
      "-0.01\n",
      "0.4\n",
      "-0.06\n",
      "0.21\n",
      "0.08\n",
      "0.15\n",
      "0.06\n",
      "-0.3\n",
      "0.1\n",
      "-0.13\n",
      "-0.19\n",
      "-0.29\n",
      "-0.25\n",
      "-0.45\n",
      "-0.45\n",
      "0.4\n",
      "0.38\n",
      "0.07\n",
      "0.13\n",
      "0.11\n",
      "-0.22\n",
      "-0.24\n",
      "0.56\n",
      "-0.38\n",
      "0.43\n",
      "0.46\n",
      "0.23\n",
      "-0.33\n",
      "0.2\n",
      "-0.19\n",
      "-0.21\n",
      "-0.26\n",
      "0.08\n",
      "-0.16\n",
      "0.02\n",
      "-0.16\n",
      "0.22\n",
      "0.41\n",
      "0.48\n",
      "-0.04\n",
      "0.22\n",
      "-0.11\n",
      "0.28\n",
      "-0.03\n",
      "-0.05\n",
      "-0.36\n",
      "0.42\n",
      "-0.31\n",
      "0.26\n",
      "0.49\n",
      "0.27\n",
      "0.36\n",
      "0.39\n",
      "-0.26\n",
      "0.5\n",
      "0.53\n",
      "0.32\n",
      "0.11\n",
      "0.55\n",
      "-0.02\n",
      "0.03\n",
      "0.19\n",
      "0.53\n",
      "-0.15\n",
      "-0.35\n",
      "0.37\n",
      "-0.04\n",
      "-0.11\n",
      "0.12\n",
      "0.42\n",
      "0.37\n",
      "-0.31\n",
      "-0.04\n",
      "0.37\n",
      "0.17\n",
      "-0.32\n",
      "-0.35\n",
      "-0.3\n",
      "0.38\n",
      "0.01\n",
      "0.06\n",
      "-0.44\n",
      "0.31\n",
      "-0.21\n",
      "0.36\n",
      "-0.4\n",
      "0.16\n",
      "0.4\n",
      "-0.16\n",
      "0.21\n",
      "0.09\n",
      "0.21\n",
      "0.51\n",
      "0.12\n",
      "-0.05\n",
      "andr aara h\n",
      "0.33\n",
      "0.12\n",
      "-0.11\n",
      "0.23\n",
      "-0.08\n",
      "0.26\n",
      "-0.27\n",
      "0.38\n",
      "0.41\n",
      "0.49\n",
      "0.19\n",
      "-0.33\n",
      "-0.44\n",
      "0.49\n",
      "-0.33\n",
      "0.01\n",
      "-0.21\n",
      "-0.23\n",
      "0.32\n",
      "0.22\n",
      "0.52\n",
      "0.16\n",
      "-0.22\n",
      "0.21\n",
      "-0.01\n",
      "-0.19\n",
      "0.1\n",
      "0.21\n",
      "0.27\n",
      "0.32\n",
      "-0.14\n",
      "0.42\n",
      "0.02\n",
      "-0.16\n",
      "0.16\n",
      "0.45\n",
      "-0.2\n",
      "-0.13\n",
      "0.56\n",
      "-0.21\n",
      "0.58\n",
      "-0.27\n",
      "-0.13\n",
      "0.19\n",
      "-0.26\n",
      "0.42\n",
      "0.29\n",
      "0.23\n",
      "-0.25\n",
      "0.34\n",
      "0.47\n",
      "-0.29\n",
      "0.2\n",
      "0.07\n",
      "0.38\n",
      "-0.11\n",
      "-0.03\n",
      "-0.33\n",
      "0.02\n",
      "0.43\n",
      "-0.27\n",
      "-0.34\n",
      "0.05\n",
      "-0.43\n",
      "-0.26\n",
      "-0.03\n",
      "0.29\n",
      "0.12\n",
      "0.42\n",
      "-0.4\n",
      "0.57\n",
      "0.43\n",
      "0.31\n",
      "-0.16\n",
      "-0.31\n",
      "0.54\n",
      "0.22\n",
      "-0.14\n",
      "0.22\n",
      "0.24\n",
      "-0.01\n",
      "0.41\n",
      "0.18\n",
      "-0.18\n",
      "0.32\n",
      "-0.37\n",
      "-0.004\n",
      "-0.04\n",
      "-0.04\n",
      "0.13\n",
      "0.51\n",
      "0.26\n",
      "0.43\n",
      "0.04\n",
      "0.29\n",
      "0.22\n",
      "0.25\n",
      "-0.06\n",
      "0.29\n",
      "0.02\n",
      "-0.004\n",
      "0.28\n",
      "0.07\n",
      "-0.05\n",
      "0.02\n",
      "-0.46\n",
      "-0.24\n",
      "0.32\n",
      "0.18\n",
      "-0.24\n",
      "-0.15\n",
      "0.18\n",
      "-0.18\n",
      "-0.02\n",
      "-0.31\n",
      "0.36\n",
      "-0.27\n",
      "-0.16\n",
      "0.02\n",
      "0.46\n",
      "-0.23\n",
      "0.44\n",
      "-0.12\n",
      "0.04\n",
      "-0.32\n",
      "-0.16\n",
      "-0.23\n",
      "-0.42\n",
      "-0.36\n",
      "-0.25\n",
      "0.48\n",
      "-0.29\n",
      "0.01\n",
      "0.15\n",
      "0.24\n",
      "0.3\n",
      "-0.28\n",
      "-0.26\n",
      "0.28\n",
      "-0.03\n",
      "0.3\n",
      "0.06\n",
      "0.4\n",
      "-0.003\n",
      "-0.02\n",
      "-0.07\n",
      "-0.33\n",
      "0.35\n",
      "0.53\n",
      "0.11\n",
      "0.39\n",
      "0.08\n",
      "-0.36\n",
      "0.46\n",
      "0.18\n",
      "0.36\n",
      "0.11\n",
      "0.31\n",
      "-0.1\n",
      "0.48\n",
      "0.29\n",
      "-0.01\n",
      "0.41\n",
      "-0.04\n",
      "0.26\n",
      "-0.29\n",
      "-0.07\n",
      "0.21\n",
      "0.12\n",
      "0.02\n",
      "0.003\n",
      "-0.29\n",
      "-0.12\n",
      "0.27\n",
      "-0.46\n",
      "0.39\n",
      "0.33\n",
      "-0.04\n",
      "-0.01\n",
      "-0.11\n",
      "0.2\n",
      "-0.32\n",
      "0.01\n",
      "-0.11\n",
      "0.11\n",
      "0.44\n",
      "0.12\n",
      "0.38\n",
      "0.36\n",
      "0.07\n",
      "0.33\n",
      "-0.25\n",
      "0.04\n",
      "0.23\n",
      "0.1\n",
      "-0.17\n",
      "0.17\n",
      "-0.44\n",
      "0.39\n",
      "0.51\n",
      "andr aara h\n",
      "-0.4\n",
      "0.36\n",
      "-0.41\n",
      "-0.22\n",
      "0.14\n",
      "-0.31\n",
      "-0.69\n",
      "-0.15\n",
      "0.54\n",
      "-0.09\n",
      "-0.16\n",
      "-0.44\n",
      "0.3\n",
      "-0.02\n",
      "-0.22\n",
      "0.09\n",
      "0.23\n",
      "0.48\n",
      "0.14\n",
      "-0.1\n",
      "0.45\n",
      "0.02\n",
      "0.46\n",
      "0.26\n",
      "-0.6\n",
      "0.55\n",
      "0.44\n",
      "-0.34\n",
      "-0.24\n",
      "0.1\n",
      "-0.03\n",
      "-0.15\n",
      "-0.55\n",
      "-0.29\n",
      "0.19\n",
      "0.14\n",
      "-0.12\n",
      "0.08\n",
      "-0.16\n",
      "0.01\n",
      "-0.41\n",
      "0.46\n",
      "0.3\n",
      "0.18\n",
      "0.29\n",
      "-0.12\n",
      "0.09\n",
      "0.26\n",
      "0.21\n",
      "-0.54\n",
      "andr aara h\n",
      "-0.09\n",
      "0.23\n",
      "0.69\n",
      "-0.34\n",
      "0.05\n",
      "0.65\n",
      "-0.65\n",
      "-0.37\n",
      "-0.13\n",
      "0.45\n",
      "andr aara h\n",
      "yhn tk\n",
      "10\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "mean_model_weights=[]\n",
    "for i in range(1):\n",
    "    mean_model_weights.append(get_avg_weights_4(add_weights[i],X_test.shape[1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 952us/step - loss: 0.1053 - accuracy: 0.9747\n",
      "[0.10531666874885559, 0.9746666550636292]\n"
     ]
    }
   ],
   "source": [
    "#mean models\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = to_categorical(dataset[target_variable])\n",
    "X = dataset.drop(columns=target_variable)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "init_model=get_initial_model_4(X_test.shape[1], 2)\n",
    "init_model.set_weights(mean_model_weights[0])\n",
    "\n",
    "print(init_model.evaluate(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_93\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_312 (Dense)           (None, 5)                 45        \n",
      "                                                                 \n",
      " dense_313 (Dense)           (None, 10)                60        \n",
      "                                                                 \n",
      " dense_314 (Dense)           (None, 20)                220       \n",
      "                                                                 \n",
      " dense_315 (Dense)           (None, 10)                210       \n",
      "                                                                 \n",
      " dense_316 (Dense)           (None, 5)                 55        \n",
      "                                                                 \n",
      " dense_317 (Dense)           (None, 2)                 12        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 602\n",
      "Trainable params: 602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "init_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IP ai4i2020.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
