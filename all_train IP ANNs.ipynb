{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from scipy.stats import ks_2samp\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundR(x, r):\n",
    "    return [np.round(i,r) for i in np.array(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdd8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(model_weights):\n",
    "  for i in range(len(model_weights)):\n",
    "    for j in range(len(model_weights[i])):\n",
    "      if isinstance(model_weights[i][j], np.ndarray):\n",
    "        for k in range(len(model_weights[i][j])):\n",
    "          #print(\"chalra h\", i, j, k)\n",
    "            if (np.round(model_weights[i][j][k],2)==0):\n",
    "                model_weights[i][j][k]=float(f'{model_weights[i][j][k]:.1g}')\n",
    "            else:\n",
    "                model_weights[i][j][k]=np.round(model_weights[i][j][k],2)\n",
    "      else:\n",
    "        if (np.round(model_weights[i][j],2)==0):\n",
    "          model_weights[i][j]=float(f'{model_weights[i][j]:.1g}')\n",
    "        else:\n",
    "          model_weights[i][j]=np.round(model_weights[i][j],2)\n",
    "  return model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b748008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_comparison1(node1, node2, epsilon=0.05):\n",
    "  for x, y in zip(node1,node2):\n",
    "    #print(x,y)\n",
    "    if isinstance(x, list):\n",
    "        if((np.linalg.norm(np.array(x)-np.array(y))/len(x))<=epsilon):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        if(abs(x-y)<=epsilon):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b9ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_comparison(node1, node2):\n",
    "  for x, y in zip(node1,node2):\n",
    "    #print(x,y)\n",
    "    if isinstance(x, list):\n",
    "        if(sorted(x)==sorted(y)):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        if(x==y):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21de1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_comparison(layer1, layer2):\n",
    "  for node1 in layer1:\n",
    "    present=False\n",
    "    for node2 in layer2:\n",
    "      if (node_comparison1(node1, node2)):\n",
    "        present=True\n",
    "    if present==False:\n",
    "      return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e3c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_models(Model_weights1, Model_weights2):\n",
    "  for i in range(0,len(Model_weights1), 2):\n",
    "    layer1=[]\n",
    "    layer2=[]\n",
    "    for j in range(len(Model_weights1[i+1].T)):\n",
    "      Node1=[]\n",
    "      Node2=[]\n",
    "      Node1.append(list(Model_weights1[i].T[j]))\n",
    "      Node1.append(Model_weights1[i+1][j])\n",
    "      if (i+2<len(Model_weights1)):\n",
    "        Node1.append(list(Model_weights1[i+2][j]))\n",
    "      Node2.append(list(Model_weights2[i].T[j]))\n",
    "      Node2.append(Model_weights2[i+1][j])\n",
    "      if (i+2<len(Model_weights2)):\n",
    "        Node2.append(list(Model_weights2[i+2][j]))\n",
    "      layer1.append(Node1)\n",
    "      layer2.append(Node2)\n",
    "    if (layer_comparison(layer1, layer2)):\n",
    "      continue\n",
    "    else:\n",
    "      return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_model(inp, out):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(5, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(5, activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56de1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_model_2(inp, out):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(1024, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_model_3(inp, out):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(10, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(20, activation='relu'),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee36f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_model_4(inp, out):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(5, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(20, activation='relu'),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(5, activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2ff763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DP_initial_model(inp, out):\n",
    "    model=tf.keras.Sequential([\n",
    "        Dense(5, input_dim=inp, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(5, activation='relu'),\n",
    "        Dense(out, activation='softmax')\n",
    "        ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e956b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to average out all the models in the epsilon range\n",
    "#the problem is different here than compared with model comparison. Shape not important.\n",
    "def get_avg_weights(models_weights, inp_shape, out_shape):\n",
    "    avg_sum=get_initial_model(inp_shape, out_shape).get_weights()\n",
    "    #print(avg_sum)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                for k in range(len(avg_sum[i][j])):\n",
    "                    avg_sum[i][j][k]=0\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=0\n",
    "    #print(avg_sum)\n",
    "    print(models_weights[0])\n",
    "    for i in range(len(models_weights)):\n",
    "        for j in range(0, len(avg_sum),2):\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #if(isinstance(avg_sum[j][0], np.ndarray)):\n",
    "            for k in range(len(avg_sum[j])):\n",
    "                avg_sum[j][k]=[avg_sum[j][k][l]+models_weights[i][j][k][l] for l in range(len(avg_sum[j][k]))]\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #else: gayab kr diya\n",
    "            print('andr aara h')\n",
    "            for k in range(len(avg_sum[j+1])):\n",
    "                avg_sum[j+1][k]=avg_sum[j+1][k]+models_weights[i][j+1][k]\n",
    "    print(\"yhn tk\")\n",
    "    mean_size=len(models_weights)\n",
    "    print(mean_size)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                #print(\"yhn tk\")\n",
    "                avg_sum[i][j]=[avg_sum[i][j][k]/mean_size for k in range(len(avg_sum[i][j]))]\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=avg_sum[i+1][j]/mean_size\n",
    "    print(\"Done\")\n",
    "    return avg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c939d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to average out all the models in the epsilon range\n",
    "#the problem is different here than compared with model comparison. Shape not important.\n",
    "def get_avg_weights_2(models_weights, inp_shape, out_shape):\n",
    "    avg_sum=get_initial_model_2(inp_shape, out_shape).get_weights()\n",
    "    #print(avg_sum)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                for k in range(len(avg_sum[i][j])):\n",
    "                    avg_sum[i][j][k]=0\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=0\n",
    "    #print(avg_sum)\n",
    "    print(models_weights[0])\n",
    "    for i in range(len(models_weights)):\n",
    "        for j in range(0, len(avg_sum),2):\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #if(isinstance(avg_sum[j][0], np.ndarray)):\n",
    "            for k in range(len(avg_sum[j])):\n",
    "                avg_sum[j][k]=[avg_sum[j][k][l]+models_weights[i][j][k][l] for l in range(len(avg_sum[j][k]))]\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #else: gayab kr diya\n",
    "            print('andr aara h')\n",
    "            for k in range(len(avg_sum[j+1])):\n",
    "                avg_sum[j+1][k]=avg_sum[j+1][k]+models_weights[i][j+1][k]\n",
    "    print(\"yhn tk\")\n",
    "    mean_size=len(models_weights)\n",
    "    print(mean_size)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                #print(\"yhn tk\")\n",
    "                avg_sum[i][j]=[avg_sum[i][j][k]/mean_size for k in range(len(avg_sum[i][j]))]\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=avg_sum[i+1][j]/mean_size\n",
    "    print(\"Done\")\n",
    "    return avg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefaf4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to average out all the models in the epsilon range\n",
    "#the problem is different here than compared with model comparison. Shape not important.\n",
    "def get_avg_weights_3(models_weights, inp_shape, out_shape):\n",
    "    avg_sum=get_initial_model_3(inp_shape, out_shape).get_weights()\n",
    "    #print(avg_sum)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                for k in range(len(avg_sum[i][j])):\n",
    "                    avg_sum[i][j][k]=0\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=0\n",
    "    #print(avg_sum)\n",
    "    print(models_weights[0])\n",
    "    for i in range(len(models_weights)):\n",
    "        for j in range(0, len(avg_sum),2):\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #if(isinstance(avg_sum[j][0], np.ndarray)):\n",
    "            if (j+2<len(avg_sum)):\n",
    "                for k in range(len(avg_sum[j])):\n",
    "                    avg_sum[j][k]=[avg_sum[j][k][l]+models_weights[i][j][k][l] for l in range(len(avg_sum[j][k]))]\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #else: gayab kr diya\n",
    "                print('andr aara h')\n",
    "                for k in range(len(avg_sum[j+1])):\n",
    "                    avg_sum[j+1][k]=avg_sum[j+1][k]+models_weights[i][j+1][k]\n",
    "    print(\"yhn tk\")\n",
    "    mean_size=len(models_weights)\n",
    "    print(mean_size)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                #print(\"yhn tk\")\n",
    "                avg_sum[i][j]=[avg_sum[i][j][k]/mean_size for k in range(len(avg_sum[i][j]))]\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=avg_sum[i+1][j]/mean_size\n",
    "    print(\"Done\")\n",
    "    return avg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to average out all the models in the epsilon range\n",
    "#the problem is different here than compared with model comparison. Shape not important.\n",
    "def get_avg_weights_4(models_weights, inp_shape, out_shape):\n",
    "    avg_sum=get_initial_model_4(inp_shape, out_shape).get_weights()\n",
    "    #print(avg_sum)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                for k in range(len(avg_sum[i][j])):\n",
    "                    avg_sum[i][j][k]=0\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=0\n",
    "    #print(avg_sum)\n",
    "    print(models_weights[0])\n",
    "    for i in range(len(models_weights)):\n",
    "        for j in range(0, len(avg_sum),2):\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #if(isinstance(avg_sum[j][0], np.ndarray)):\n",
    "            for k in range(len(avg_sum[j])):\n",
    "                avg_sum[j][k]=[avg_sum[j][k][l]+models_weights[i][j][k][l] for l in range(len(avg_sum[j][k]))]\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #else: gayab kr diya\n",
    "            print('andr aara h')\n",
    "            for k in range(len(avg_sum[j+1])):\n",
    "                avg_sum[j+1][k]=avg_sum[j+1][k]+models_weights[i][j+1][k]\n",
    "    print(\"yhn tk\")\n",
    "    mean_size=len(models_weights)\n",
    "    print(mean_size)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                #print(\"yhn tk\")\n",
    "                avg_sum[i][j]=[avg_sum[i][j][k]/mean_size for k in range(len(avg_sum[i][j]))]\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=avg_sum[i+1][j]/mean_size\n",
    "    print(\"Done\")\n",
    "    return avg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4af99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions for f1, precision and recall\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d2ccc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"all_train.csv\",sep=',')\n",
    "dataset=dataset.round(3)\n",
    "#print(dataset['label'])\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "dataset = pd.DataFrame(scaler.fit_transform(dataset), columns=dataset.columns)\n",
    "target_variable=\"# label\"\n",
    "print(dataset.head)\n",
    "Positive=dataset[dataset[target_variable]==0.0]\n",
    "Negative=dataset[dataset[target_variable]==1.0]\n",
    "print(len(Positive))\n",
    "print(len(Negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a37d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the below code is for binary classification since MNSIT is a multi-class low-resolution image dataset. I am updating the code in the next cell.\n",
    "N=10000\n",
    "#Positive=Pos\n",
    "#Negative=Neg\n",
    "positiveN=int((Positive.shape[0]/dataset.shape[0])*N)\n",
    "negativeN=int(N-positiveN)\n",
    "print(positiveN, negativeN)\n",
    "#target variable\n",
    "#target_variable=\"default.payment.next.month\"\n",
    "df1=Positive.sample(positiveN)\n",
    "Positive.drop(df1.index, inplace=True)\n",
    "df2=Negative.sample(negativeN)\n",
    "Negative.drop(df2.index, inplace=True)\n",
    "test_data=df1.append(df2, ignore_index=True)\n",
    "test_data=test_data.sample(frac = 1) #This is to shuffel the training and testing data\n",
    "test_data=test_data.sample(frac = 1)\n",
    "test_data=test_data.sample(frac = 1)\n",
    "X_test=test_data.drop(columns=[target_variable])\n",
    "y_test=to_categorical(test_data[target_variable])\n",
    "\n",
    "# adding dense layer\n",
    "initial_model= get_initial_model(X_test.shape[1], 2)\n",
    "initial_model.set_weights(update_weights(initial_model.get_weights()))\n",
    "Models=[]\n",
    "val_acc=[]\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "val_loss=[]\n",
    "train_loss=[]\n",
    "add_weights=[]\n",
    "while Positive.empty==False and Negative.empty==False:\n",
    "  print(positiveN, negativeN)\n",
    "  df1=Positive.sample(min(positiveN, len(Positive)))\n",
    "  Positive.drop(df1.index, inplace=True)\n",
    "  df2=Negative.sample(min(negativeN, len(Negative)))\n",
    "  Negative.drop(df2.index, inplace=True)\n",
    "  train_data=df1.append(df2, ignore_index=True)\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "    \n",
    "  #all models have different initialization\n",
    "  # define the sequential model\n",
    "  \"\"\"initial_model = keras.Sequential()\n",
    "\n",
    "    # adding dense layer\n",
    "  initial_model.add(Dense(5, input_dim=X_test.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "  initial_model.add(Dense(10, activation='relu'))\n",
    "  initial_model.add(Dense(5, activation='relu'))\n",
    "\n",
    "    # adding dense layer with softmax activation/output layer\n",
    "  initial_model.add(Dense(2, activation='softmax'))\n",
    "  #initial_model.summary()\"\"\"\n",
    "  ann_model=get_initial_model(X_test.shape[1], 2) #same intial weights\n",
    "  ann_model.set_weights(initial_model.get_weights())\n",
    "  X_train=train_data.drop(columns=[target_variable])\n",
    "  #train_data[target_variable]=train_data[target_variable]-1 #only for skin_nonskin dataset\n",
    "  y_train=to_categorical(train_data[target_variable])\n",
    "  #print(y_train)\n",
    "  ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_m]) # metrics=['accuracy']\n",
    "  history = ann_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "  print(history.history['f1_m'][-1])\n",
    "  ann_model.set_weights(update_weights(ann_model.get_weights()))\n",
    "  pred_test=ann_model.predict(X_test)\n",
    "  present=False\n",
    "  for i in range(len(Models)):\n",
    "    if (check_models(Models[i][0], ann_model.get_weights())):\n",
    "      print(\"if any\")\n",
    "      Models[i][1]=Models[i][1]+1\n",
    "      add_weights[i].append(ann_model.get_weights())\n",
    "      val_acc[i].append(history.history['val_f1_m'])\n",
    "      train_acc[i].append(history.history['f1_m'])\n",
    "      test_acc[i].append(f1_m(y_test, pred_test))\n",
    "      val_loss[i].append(history.history['val_loss'])\n",
    "      train_loss[i].append(history.history['loss'])\n",
    "      present=True\n",
    "      break;\n",
    "  if present==False:\n",
    "    add_weights.append([ann_model.get_weights()])\n",
    "    Models.append([ann_model.get_weights(), 1])\n",
    "    val_acc.append([history.history['val_f1_m']])\n",
    "    train_acc.append([history.history['f1_m']])\n",
    "    test_acc.append([f1_m(y_test, pred_test)])\n",
    "    val_loss.append([history.history['val_loss']])\n",
    "    train_loss.append([history.history['loss']])\n",
    "for i in range(len(Models)):\n",
    "  print(Models[i][1])\n",
    "print(\"All Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f906d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Models)):\n",
    "  print(Models[i][1])\n",
    "print(len(Models))\n",
    "#test_Acc = [i.numpy() for i in test_acc]\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305fc3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this works for getting sorted recurrent models by frequency\n",
    "A=np.argsort(np.array(Models).T[1])[::-1][:5]\n",
    "print(np.array(Models).T[1])\n",
    "print(A)\n",
    "temp=list(np.array(Models)[A])\n",
    "print(temp[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5304275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducing the metrics lists to top 5 models only\n",
    "val_acc=list(np.array(val_acc)[A])\n",
    "test_acc=list(np.array(test_acc)[A])\n",
    "train_acc=list(np.array(train_acc)[A])\n",
    "val_loss=list(np.array(val_loss)[A])\n",
    "train_loss=list(np.array(train_loss)[A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_model_weights=[]\n",
    "for i in range(5):\n",
    "    mean_model_weights.append(get_avg_weights(add_weights[A[i]],X_test.shape[1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbad5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean models\n",
    "from sklearn.model_selection import train_test_split\n",
    "mean_models=[]\n",
    "mean_model_train_metrics=[]\n",
    "mean_model_loss=[]\n",
    "mean_model_acc=[]\n",
    "mean_model_test_metrics=[]\n",
    "mean_model_test_loss=[]\n",
    "mean_model_test_acc=[]\n",
    "y = to_categorical(dataset[target_variable])\n",
    "X = dataset.drop(columns=target_variable)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "for i in range(5):\n",
    "    init_model=get_initial_model(X_test.shape[1], 2)\n",
    "    init_model.set_weights(get_avg_weights(add_weights[A[i]],X_test.shape[1], 2))\n",
    "    mean_model_train_metrics.append(init_model.evaluate(X_train, y_train))\n",
    "    mean_model_loss.append(mean_model_train_metrics[i][0])\n",
    "    mean_model_acc.append(mean_model_train_metrics[i][1])\n",
    "    mean_model_test_metrics.append(init_model.evaluate(X_test, y_test))\n",
    "    mean_model_test_loss.append(mean_model_test_metrics[i][0])\n",
    "    mean_model_test_acc.append(mean_model_test_metrics[i][1])\n",
    "print(\"Done for model selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a3c3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_model_loss, mean_model_acc)\n",
    "print(mean_model_test_loss, mean_model_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf1da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmark model\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = to_categorical(dataset[target_variable])\n",
    "X = dataset.drop(columns=target_variable)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "benchmark_model=get_initial_model(X_test.shape[1], 2) #same intial weights\n",
    "benchmark_model.set_weights(initial_model.get_weights())\n",
    "history = benchmark_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "benchmark_model.set_weights(update_weights(benchmark_model.get_weights()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmark metrics\n",
    "benchmark_loss=history.history['loss']\n",
    "benchmark_val_loss=history.history['val_loss']\n",
    "benchmark_acc=history.history['accuracy']\n",
    "benchmark_val_acc=history.history['val_accuracy']\n",
    "benchmark_test_metrics=benchmark_model.evaluate(X_test, y_test)\n",
    "benchmark_test_loss=benchmark_test_metrics[0]\n",
    "benchmark_test_accuracy=benchmark_test_metrics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d616521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = to_categorical(dataset[target_variable])\n",
    "X = dataset.drop(columns=target_variable)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "benchmark_model=get_initial_model(X_test.shape[1], 2) #same intial weights\n",
    "benchmark_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from here onwards the comparison and computation of DP:\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_privacy\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "l2_norm_clip = 2.5\n",
    "noise_multiplier = 2.5\n",
    "num_microbatches = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed9f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = to_categorical(dataset[target_variable])\n",
    "X = dataset.drop(columns=target_variable)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "DP_model=get_DP_initial_model(X_test.shape[1], 2) #same intial weights\n",
    "optimizer = tensorflow_privacy.DPKerasAdamOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "DP_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[f1_m])\n",
    "history = DP_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "#benchmark_model.set_weights(update_weights(benchmark_model.get_weights()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=X_train.shape[0]*0.8,\n",
    "                                              batch_size=50,\n",
    "                                              noise_multiplier=2.5,\n",
    "                                              epochs=10,\n",
    "                                              delta=1e-5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3236e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for epsilon ≈ 1\n",
    "DP_1_loss=history.history['loss']\n",
    "DP_1_f1=history.history['f1_m']\n",
    "DP_1_f1_train=DP_model.evaluate(X_train,y_train)\n",
    "DP_1_f1_test=DP_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a492e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for epsilon ≈ 0.5\n",
    "DP_0_5_loss=history.history['loss']\n",
    "DP_0_5_f1=history.history['f1_m']\n",
    "DP_0_5_f1_train=DP_model.evaluate(X_train,y_train)\n",
    "DP_0_5_f1_test=DP_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f4c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for epsilon ≈ 0.1\n",
    "DP_0_1_loss=history.history['loss']\n",
    "DP_0_1_f1=history.history['f1_m']\n",
    "DP_0_1_f1_train=DP_model.evaluate(X_train,y_train)\n",
    "DP_0_1_f1_test=DP_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting graphs for loss and accuracies for the top 5 recurrent models:\n",
    "#plotting val_loss and loss for the models generated and the benchmark model.\n",
    "from random import randint\n",
    "import matplotlib.patches as mpatches\n",
    "color = ['red', 'yellow','blue', 'green','cyan']\n",
    "leg=[]\n",
    "for i in range(len(color)):\n",
    "    leg.append(mpatches.Patch(color=color[i], label=str(len(val_acc[i]))))\n",
    "n = len(val_acc)\n",
    "x_axis=np.arange(1, 11, 1)\n",
    "print(x_axis)\n",
    "for i in range(n):\n",
    "    for j in range(len(val_acc[i])):\n",
    "        plt.plot(x_axis,val_acc[i][j], color=color[i], alpha=0.2)\n",
    "    plt.plot(x_axis, np.mean(val_acc[i], axis=0), color=color[i])\n",
    "#plt.xlim(-0.5,20.5)\n",
    "plt.xticks([2.5,5.0,7.5,10.0,12.5,15.0,17.5,20.0],[2,5,7,10,12,15,17,20])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"f1 score\")\n",
    "plt.legend(handles=leg, bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", borderaxespad=0, ncol=5)\n",
    "plt.plot(x_axis, benchmark_val_acc, color='black')\n",
    "plt.savefig(\"fig/all_train_F1_Val_20Epochs_1000.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3012947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train accuracy \n",
    "from random import randint\n",
    "color = []\n",
    "print(train_acc[4][0])\n",
    "color = ['red', 'yellow','blue', 'green','cyan']\n",
    "x_axis=np.arange(1, 11, 1)\n",
    "print(x_axis)\n",
    "for i in range(n):\n",
    "    for j in range(len(train_acc[i])):\n",
    "        print(i,j)\n",
    "        plt.plot(x_axis,train_acc[i][j], color=color[i], alpha=0.2)\n",
    "    plt.plot(x_axis,np.mean(train_acc[i], axis=0), color=color[i])\n",
    "plt.plot(x_axis,benchmark_acc, color='black')\n",
    "plt.xticks([2.5,5.0,7.5,10.0],[2,5,7,10])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"f1 score\")\n",
    "plt.plot(x_axis, DP_1_f1, 'o', color='black')\n",
    "plt.plot(x_axis, DP_0_5_f1, '-.', color='black')\n",
    "plt.plot(x_axis, DP_0_1_f1, 'd', color='black')\n",
    "plt.legend(handles=leg, bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", borderaxespad=0, ncol=5)\n",
    "plt.savefig(\"fig/all_train_F1_train_20Epochs_1000.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0deb918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loss\n",
    "from random import randint\n",
    "color = []\n",
    "n = len(train_loss)\n",
    "color = ['red', 'yellow','blue', 'green','cyan']\n",
    "for i in range(n):\n",
    "    for j in range(len(train_loss[i])):\n",
    "        plt.plot(x_axis,train_loss[i][j], color=color[i], alpha=0.2)\n",
    "    plt.plot(x_axis,np.mean(train_loss[i], axis=0), color=color[i])\n",
    "plt.plot(x_axis,benchmark_loss, color='black')\n",
    "plt.xticks([2.5,5.0,7.5,10.0],[2,5,7,10])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(x_axis, DP_1_loss, 'o', color='black')\n",
    "plt.plot(x_axis, DP_0_5_loss, '-.', color='black')\n",
    "plt.plot(x_axis, DP_0_1_loss, 'd', color='black')\n",
    "plt.legend(handles=leg, bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", borderaxespad=0, ncol=5)\n",
    "plt.savefig(\"fig/all_train_loss_train_20Epochs_1000.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d93b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation loss loss\n",
    "from random import randint\n",
    "color = []\n",
    "n = len(val_acc)\n",
    "color = ['red', 'yellow','blue', 'green','cyan']\n",
    "for i in range(n):\n",
    "    for j in range(len(train_loss[i])):\n",
    "        plt.plot(x_axis,val_loss[i][j], color=color[i], alpha=0.2)\n",
    "    plt.plot(x_axis,np.mean(val_loss[i], axis=0), color=color[i])\n",
    "plt.plot(x_axis,benchmark_val_loss, color='black')\n",
    "plt.xticks([2.5,5.0,7.5,10.0],[2,5,7,10])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(handles=leg, bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", borderaxespad=0, ncol=5)\n",
    "plt.savefig(\"fig/all_train_loss_val_20Epochs_1000.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98248507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets compute the accuracy for bigger test set 30% benchmark test data\n",
    "high_test_loss=[[0.3551]*171]#already ran for most recurring model hence initialized with predefined value\n",
    "high_test_f1=[[0.8296]*171]\n",
    "high_weights=list(np.array(add_weights)[A])\n",
    "\n",
    "for i in range(1,len(high_weights)):\n",
    "    for j in range(len(high_weights[i])):\n",
    "        high_test_model=get_initial_model(X_test.shape[1], 2) #same intial weights\n",
    "        high_test_model.set_weights(Models[i][0])\n",
    "        high_test_metrics=high_test_model.evaluate(X_test,y_test)\n",
    "        if j == 0:\n",
    "            high_test_loss.append([high_test_metrics[0]])\n",
    "            high_test_f1.append([high_test_metrics[1]])\n",
    "        else:\n",
    "            high_test_loss[i].append(high_test_metrics[0])\n",
    "            high_test_f1[i].append(high_test_metrics[1])\n",
    "    print(i)\n",
    "print(high_test_loss, high_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178eef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_test_accs=list(np.array(high_test_f1)[A])\n",
    "print(high_test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588761b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for test accuracy plot\n",
    "#plot test accuracy of smaller test set\n",
    "benchmark_test_accs=[benchmark_test_accuracy]*n\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0.7, 1.0])\n",
    "for i in range(n):\n",
    "    plt.scatter([i]*len(test_acc[i]),test_acc[i], color=color[i], alpha=0.2)\n",
    "    plt.scatter(i,high_test_accs[i][0], color=color[i])\n",
    "plt.plot(benchmark_test_accs, color='black')\n",
    "plt.legend(handles=leg, bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", borderaxespad=0, ncol=5)\n",
    "plt.savefig(\"fig/all_train_test_acc_20Epochs_1000.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8407430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot for mean model accuracy and benchmark loss:\n",
    "n=5\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0.7, 0.9])\n",
    "for i in range(n):\n",
    "    plt.plot(i, mean_model_acc[i],'o',color=color[i])\n",
    "    plt.plot(i, mean_model_test_acc[i],'o', mfc='none',color=color[i])\n",
    "plt.plot([benchmark_acc[-1]]*n,color='black')\n",
    "plt.plot([benchmark_test_accuracy]*n,'-.',color='black')\n",
    "\"\"\"\n",
    "plt.plot([DP_0_1_f1_train]*n, 'o', color='black')\n",
    "plt.plot([DP_0_1_f1_test]*n, 'o', color='magenta')\n",
    "plt.plot([DP_0_5_f1_train]*n, '-.', color='black')\n",
    "plt.plot([DP_0_5_f1_test]*n, '-.', color='magenta')\n",
    "plt.plot([DP_1_f1_train]*n, 'd', color='black')\n",
    "plt.plot([DP_1_f1_test]*n, 'd', color='magenta')\n",
    "\"\"\"\n",
    "plt.legend(handles=leg, bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", borderaxespad=0, ncol=5)\n",
    "plt.savefig(\"fig/all_train_mean_model_results.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87f400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from here on ANN-2,3,4\n",
    "dataset = pd.read_csv(\"all_train.csv\",sep=',')\n",
    "dataset=dataset.round(3)\n",
    "#print(dataset['label'])\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "dataset = pd.DataFrame(scaler.fit_transform(dataset), columns=dataset.columns)\n",
    "target_variable=\"# label\"\n",
    "print(dataset.head)\n",
    "Positive=dataset[dataset[target_variable]==0.0]\n",
    "Negative=dataset[dataset[target_variable]==1.0]\n",
    "print(len(Positive))\n",
    "print(len(Negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d964917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the below code is for binary classification since MNSIT is a multi-class low-resolution image dataset. I am updating the code in the next cell.\n",
    "N=10000\n",
    "#Positive=Pos\n",
    "#Negative=Neg\n",
    "positiveN=int((Positive.shape[0]/dataset.shape[0])*N)\n",
    "negativeN=int(N-positiveN)\n",
    "print(positiveN, negativeN)\n",
    "#target variable\n",
    "#target_variable=\"default.payment.next.month\"\n",
    "df1=Positive.sample(positiveN)\n",
    "Positive.drop(df1.index, inplace=True)\n",
    "df2=Negative.sample(negativeN)\n",
    "Negative.drop(df2.index, inplace=True)\n",
    "test_data=df1.append(df2, ignore_index=True)\n",
    "test_data=test_data.sample(frac = 1) #This is to shuffel the training and testing data\n",
    "test_data=test_data.sample(frac = 1)\n",
    "test_data=test_data.sample(frac = 1)\n",
    "X_test=test_data.drop(columns=[target_variable])\n",
    "y_test=to_categorical(test_data[target_variable])\n",
    "\n",
    "# adding dense layer\n",
    "initial_model= get_initial_model_2(X_test.shape[1], 2)\n",
    "initial_model.set_weights(update_weights(initial_model.get_weights()))\n",
    "Models=[]\n",
    "val_acc=[]\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "val_loss=[]\n",
    "train_loss=[]\n",
    "add_weights=[]\n",
    "while Positive.empty==False and Negative.empty==False:\n",
    "  print(positiveN, negativeN)\n",
    "  df1=Positive.sample(min(positiveN, len(Positive)))\n",
    "  Positive.drop(df1.index, inplace=True)\n",
    "  df2=Negative.sample(min(negativeN, len(Negative)))\n",
    "  Negative.drop(df2.index, inplace=True)\n",
    "  train_data=df1.append(df2, ignore_index=True)\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "    \n",
    "  #all models have different initialization\n",
    "  # define the sequential model\n",
    "  \"\"\"initial_model = keras.Sequential()\n",
    "\n",
    "    # adding dense layer\n",
    "  initial_model.add(Dense(5, input_dim=X_test.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "  initial_model.add(Dense(10, activation='relu'))\n",
    "  initial_model.add(Dense(5, activation='relu'))\n",
    "\n",
    "    # adding dense layer with softmax activation/output layer\n",
    "  initial_model.add(Dense(2, activation='softmax'))\n",
    "  #initial_model.summary()\"\"\"\n",
    "  ann_model=get_initial_model_2(X_test.shape[1], 2) #same intial weights\n",
    "  ann_model.set_weights(initial_model.get_weights())\n",
    "  X_train=train_data.drop(columns=[target_variable])\n",
    "  #train_data[target_variable]=train_data[target_variable]-1 #only for skin_nonskin dataset\n",
    "  y_train=to_categorical(train_data[target_variable])\n",
    "  #print(y_train)\n",
    "  ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_m]) # metrics=['accuracy']\n",
    "  history = ann_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "  print(history.history['f1_m'][-1])\n",
    "  ann_model.set_weights(update_weights(ann_model.get_weights()))\n",
    "  pred_test=ann_model.predict(X_test)\n",
    "  present=False\n",
    "  for i in range(len(Models)):\n",
    "    if (check_models(Models[i][0], ann_model.get_weights())):\n",
    "      print(\"if any\")\n",
    "      Models[i][1]=Models[i][1]+1\n",
    "      add_weights[i].append(ann_model.get_weights())\n",
    "      val_acc[i].append(history.history['val_f1_m'])\n",
    "      train_acc[i].append(history.history['f1_m'])\n",
    "      test_acc[i].append(f1_m(y_test, pred_test))\n",
    "      val_loss[i].append(history.history['val_loss'])\n",
    "      train_loss[i].append(history.history['loss'])\n",
    "      present=True\n",
    "      break;\n",
    "  if present==False:\n",
    "    add_weights.append([ann_model.get_weights()])\n",
    "    Models.append([ann_model.get_weights(), 1])\n",
    "    val_acc.append([history.history['val_f1_m']])\n",
    "    train_acc.append([history.history['f1_m']])\n",
    "    test_acc.append([f1_m(y_test, pred_test)])\n",
    "    val_loss.append([history.history['val_loss']])\n",
    "    train_loss.append([history.history['loss']])\n",
    "for i in range(len(Models)):\n",
    "  print(Models[i][1])\n",
    "print(\"All Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d5386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_model_weights=get_avg_weights_2(add_weights[0],X_test.shape[1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd9a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7a8e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from here on ANN-2,3,4\n",
    "dataset = pd.read_csv(\"all_train.csv\",sep=',')\n",
    "dataset=dataset.round(3)\n",
    "#print(dataset['label'])\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "dataset = pd.DataFrame(scaler.fit_transform(dataset), columns=dataset.columns)\n",
    "target_variable=\"# label\"\n",
    "print(dataset.head)\n",
    "Positive=dataset[dataset[target_variable]==0.0]\n",
    "Negative=dataset[dataset[target_variable]==1.0]\n",
    "print(len(Positive))\n",
    "print(len(Negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1cc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the below code is for binary classification since MNSIT is a multi-class low-resolution image dataset. I am updating the code in the next cell.\n",
    "N=10000\n",
    "#Positive=Pos\n",
    "#Negative=Neg\n",
    "positiveN=int((Positive.shape[0]/dataset.shape[0])*N)\n",
    "negativeN=int(N-positiveN)\n",
    "print(positiveN, negativeN)\n",
    "#target variable\n",
    "#target_variable=\"default.payment.next.month\"\n",
    "df1=Positive.sample(positiveN)\n",
    "Positive.drop(df1.index, inplace=True)\n",
    "df2=Negative.sample(negativeN)\n",
    "Negative.drop(df2.index, inplace=True)\n",
    "test_data=df1.append(df2, ignore_index=True)\n",
    "test_data=test_data.sample(frac = 1) #This is to shuffel the training and testing data\n",
    "test_data=test_data.sample(frac = 1)\n",
    "test_data=test_data.sample(frac = 1)\n",
    "X_test=test_data.drop(columns=[target_variable])\n",
    "y_test=to_categorical(test_data[target_variable])\n",
    "\n",
    "# adding dense layer\n",
    "initial_model= get_initial_model_3(X_test.shape[1], 2)\n",
    "initial_model.set_weights(update_weights(initial_model.get_weights()))\n",
    "Models=[]\n",
    "val_acc=[]\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "val_loss=[]\n",
    "train_loss=[]\n",
    "add_weights=[]\n",
    "while Positive.empty==False and Negative.empty==False:\n",
    "  print(positiveN, negativeN)\n",
    "  df1=Positive.sample(min(positiveN, len(Positive)))\n",
    "  Positive.drop(df1.index, inplace=True)\n",
    "  df2=Negative.sample(min(negativeN, len(Negative)))\n",
    "  Negative.drop(df2.index, inplace=True)\n",
    "  train_data=df1.append(df2, ignore_index=True)\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "    \n",
    "  #all models have different initialization\n",
    "  # define the sequential model\n",
    "  \"\"\"initial_model = keras.Sequential()\n",
    "\n",
    "    # adding dense layer\n",
    "  initial_model.add(Dense(5, input_dim=X_test.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "  initial_model.add(Dense(10, activation='relu'))\n",
    "  initial_model.add(Dense(5, activation='relu'))\n",
    "\n",
    "    # adding dense layer with softmax activation/output layer\n",
    "  initial_model.add(Dense(2, activation='softmax'))\n",
    "  #initial_model.summary()\"\"\"\n",
    "  ann_model=get_initial_model_3(X_test.shape[1], 2) #same intial weights\n",
    "  ann_model.set_weights(initial_model.get_weights())\n",
    "  X_train=train_data.drop(columns=[target_variable])\n",
    "  #train_data[target_variable]=train_data[target_variable]-1 #only for skin_nonskin dataset\n",
    "  y_train=to_categorical(train_data[target_variable])\n",
    "  #print(y_train)\n",
    "  ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_m]) # metrics=['accuracy']\n",
    "  history = ann_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "  print(history.history['f1_m'][-1])\n",
    "  ann_model.set_weights(update_weights(ann_model.get_weights()))\n",
    "  pred_test=ann_model.predict(X_test)\n",
    "  present=False\n",
    "  for i in range(len(Models)):\n",
    "    if (check_models(Models[i][0], ann_model.get_weights())):\n",
    "      print(\"if any\")\n",
    "      Models[i][1]=Models[i][1]+1\n",
    "      add_weights[i].append(ann_model.get_weights())\n",
    "      val_acc[i].append(history.history['val_f1_m'])\n",
    "      train_acc[i].append(history.history['f1_m'])\n",
    "      test_acc[i].append(f1_m(y_test, pred_test))\n",
    "      val_loss[i].append(history.history['val_loss'])\n",
    "      train_loss[i].append(history.history['loss'])\n",
    "      present=True\n",
    "      break;\n",
    "  if present==False:\n",
    "    add_weights.append([ann_model.get_weights()])\n",
    "    Models.append([ann_model.get_weights(), 1])\n",
    "    val_acc.append([history.history['val_f1_m']])\n",
    "    train_acc.append([history.history['f1_m']])\n",
    "    test_acc.append([f1_m(y_test, pred_test)])\n",
    "    val_loss.append([history.history['val_loss']])\n",
    "    train_loss.append([history.history['loss']])\n",
    "for i in range(len(Models)):\n",
    "  print(Models[i][1])\n",
    "print(\"All Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bde855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(add_weights[7][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c8395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to average out all the models in the epsilon range\n",
    "#the problem is different here than compared with model comparison. Shape not important.\n",
    "def get_avg_weights_3(models_weights, inp_shape, out_shape):\n",
    "    avg_sum=get_initial_model_3(inp_shape, out_shape).get_weights()\n",
    "    #print(avg_sum.summary())\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                for k in range(len(avg_sum[i][j])):\n",
    "                    avg_sum[i][j][k]=0\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=0\n",
    "    #print(avg_sum)\n",
    "    print(models_weights[0])\n",
    "    for i in range(len(models_weights)):\n",
    "        for j in range(0, len(avg_sum),2):\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #if(isinstance(avg_sum[j][0], np.ndarray)):\n",
    "            if (j+2<len(avg_sum)):\n",
    "                for k in range(len(avg_sum[j])):\n",
    "                    avg_sum[j][k]=[avg_sum[j][k][l]+models_weights[i][j][k][l] for l in range(len(avg_sum[j][k]))]\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #else: gayab kr diya\n",
    "                print('andr aara h')\n",
    "                for k in range(len(avg_sum[j+1])):\n",
    "                    avg_sum[j+1][k]=avg_sum[j+1][k]+models_weights[i][j+1][k]\n",
    "    print(\"yhn tk\")\n",
    "    mean_size=len(models_weights)\n",
    "    print(mean_size)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                #print(\"yhn tk\")\n",
    "                avg_sum[i][j]=[avg_sum[i][j][k]/mean_size for k in range(len(avg_sum[i][j]))]\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=avg_sum[i+1][j]/mean_size\n",
    "    print(\"Done\")\n",
    "    return avg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a21e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_model_weights=get_avg_weights_3(add_weights[7],X_test.shape[1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c92534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = to_categorical(dataset[target_variable], num_classes=2)\n",
    "X = dataset.drop(columns=target_variable)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "init_model=get_initial_model_3(X_test.shape[1], 2)\n",
    "init_model.set_weights(add_weights[7][0])\n",
    "init_model.evaluate(X_test, y_test)\n",
    "print(\"Done for model selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad797d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from here on ANN-2,3,4\n",
    "dataset = pd.read_csv(\"all_train.csv\",sep=',')\n",
    "dataset=dataset.round(3)\n",
    "#print(dataset['label'])\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "dataset = pd.DataFrame(scaler.fit_transform(dataset), columns=dataset.columns)\n",
    "target_variable=\"# label\"\n",
    "print(dataset.head)\n",
    "Positive=dataset[dataset[target_variable]==0.0]\n",
    "Negative=dataset[dataset[target_variable]==1.0]\n",
    "print(len(Positive))\n",
    "print(len(Negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0f995e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the below code is for binary classification since MNSIT is a multi-class low-resolution image dataset. I am updating the code in the next cell.\n",
    "N=10000\n",
    "#Positive=Pos\n",
    "#Negative=Neg\n",
    "positiveN=int((Positive.shape[0]/dataset.shape[0])*N)\n",
    "negativeN=int(N-positiveN)\n",
    "print(positiveN, negativeN)\n",
    "#target variable\n",
    "#target_variable=\"default.payment.next.month\"\n",
    "df1=Positive.sample(positiveN)\n",
    "Positive.drop(df1.index, inplace=True)\n",
    "df2=Negative.sample(negativeN)\n",
    "Negative.drop(df2.index, inplace=True)\n",
    "test_data=df1.append(df2, ignore_index=True)\n",
    "test_data=test_data.sample(frac = 1) #This is to shuffel the training and testing data\n",
    "test_data=test_data.sample(frac = 1)\n",
    "test_data=test_data.sample(frac = 1)\n",
    "X_test=test_data.drop(columns=[target_variable])\n",
    "y_test=to_categorical(test_data[target_variable])\n",
    "\n",
    "# adding dense layer\n",
    "initial_model= get_initial_model_4(X_test.shape[1], 2)\n",
    "initial_model.set_weights(update_weights(initial_model.get_weights()))\n",
    "Models=[]\n",
    "val_acc=[]\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "val_loss=[]\n",
    "train_loss=[]\n",
    "add_weights=[]\n",
    "while Positive.empty==False and Negative.empty==False:\n",
    "  print(positiveN, negativeN)\n",
    "  df1=Positive.sample(min(positiveN, len(Positive)))\n",
    "  Positive.drop(df1.index, inplace=True)\n",
    "  df2=Negative.sample(min(negativeN, len(Negative)))\n",
    "  Negative.drop(df2.index, inplace=True)\n",
    "  train_data=df1.append(df2, ignore_index=True)\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "  train_data=train_data.sample(frac = 1) #shuffel train data 3 times\n",
    "    \n",
    "  #all models have different initialization\n",
    "  # define the sequential model\n",
    "  \"\"\"initial_model = keras.Sequential()\n",
    "\n",
    "    # adding dense layer\n",
    "  initial_model.add(Dense(5, input_dim=X_test.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "  initial_model.add(Dense(10, activation='relu'))\n",
    "  initial_model.add(Dense(5, activation='relu'))\n",
    "\n",
    "    # adding dense layer with softmax activation/output layer\n",
    "  initial_model.add(Dense(2, activation='softmax'))\n",
    "  #initial_model.summary()\"\"\"\n",
    "  ann_model=get_initial_model_4(X_test.shape[1], 2) #same intial weights\n",
    "  ann_model.set_weights(initial_model.get_weights())\n",
    "  X_train=train_data.drop(columns=[target_variable])\n",
    "  #train_data[target_variable]=train_data[target_variable]-1 #only for skin_nonskin dataset\n",
    "  y_train=to_categorical(train_data[target_variable])\n",
    "  #print(y_train)\n",
    "  ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1_m]) # metrics=['accuracy']\n",
    "  history = ann_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "  print(history.history['f1_m'][-1])\n",
    "  ann_model.set_weights(update_weights(ann_model.get_weights()))\n",
    "  pred_test=ann_model.predict(X_test)\n",
    "  present=False\n",
    "  for i in range(len(Models)):\n",
    "    if (check_models(Models[i][0], ann_model.get_weights())):\n",
    "      print(\"if any\")\n",
    "      Models[i][1]=Models[i][1]+1\n",
    "      add_weights[i].append(ann_model.get_weights())\n",
    "      val_acc[i].append(history.history['val_f1_m'])\n",
    "      train_acc[i].append(history.history['f1_m'])\n",
    "      test_acc[i].append(f1_m(y_test, pred_test))\n",
    "      val_loss[i].append(history.history['val_loss'])\n",
    "      train_loss[i].append(history.history['loss'])\n",
    "      present=True\n",
    "      break;\n",
    "  if present==False:\n",
    "    add_weights.append([ann_model.get_weights()])\n",
    "    Models.append([ann_model.get_weights(), 1])\n",
    "    val_acc.append([history.history['val_f1_m']])\n",
    "    train_acc.append([history.history['f1_m']])\n",
    "    test_acc.append([f1_m(y_test, pred_test)])\n",
    "    val_loss.append([history.history['val_loss']])\n",
    "    train_loss.append([history.history['loss']])\n",
    "for i in range(len(Models)):\n",
    "  print(Models[i][1])\n",
    "print(\"All Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to average out all the models in the epsilon range\n",
    "#the problem is different here than compared with model comparison. Shape not important.\n",
    "def get_avg_weights_4(models_weights, inp_shape, out_shape):\n",
    "    avg_sum=get_initial_model_4(inp_shape, out_shape).get_weights()\n",
    "    #print(avg_sum.summary())\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                for k in range(len(avg_sum[i][j])):\n",
    "                    avg_sum[i][j][k]=0\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=0\n",
    "    #print(avg_sum)\n",
    "    #print(models_weights[0])\n",
    "    for i in range(len(models_weights)):\n",
    "        for j in range(0, len(avg_sum),2):\n",
    "            #print(isinstance(avg_sum[j], np.ndarray))\n",
    "            #if(isinstance(avg_sum[j][0], np.ndarray)):\n",
    "            if (j+2<=len(avg_sum)):\n",
    "                for k in range(len(avg_sum[j])):\n",
    "                    print(models_weights[i][j][k])\n",
    "                    avg_sum[j][k]=[avg_sum[j][k][l]+models_weights[i][j][k][l] for l in range(len(avg_sum[j][k]))]\n",
    "                print('andr aara h')\n",
    "                for k in range(len(avg_sum[j+1])):\n",
    "                    avg_sum[j+1][k]=avg_sum[j+1][k]+models_weights[i][j+1][k]\n",
    "    print(\"yhn tk\")\n",
    "    mean_size=len(models_weights)\n",
    "    print(mean_size)\n",
    "    for i in range(0,len(avg_sum),2):\n",
    "        if (i+2<=len(avg_sum)):\n",
    "            for j in range(len(avg_sum[i])):\n",
    "                #print(\"yhn tk\")\n",
    "                avg_sum[i][j]=[avg_sum[i][j][k]/mean_size for k in range(len(avg_sum[i][j]))]\n",
    "            for j in range(len(avg_sum[i+1])):\n",
    "                avg_sum[i+1][j]=avg_sum[i+1][j]/mean_size\n",
    "    print(\"Done\")\n",
    "    return avg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d35b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(add_weights[0][0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_model_weights=get_avg_weights_4(add_weights[0],X_test.shape[1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16a5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b9a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = to_categorical(dataset[target_variable], num_classes=2)\n",
    "X = dataset.drop(columns=target_variable)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "init_model=get_initial_model_4(X_test.shape[1], 2)\n",
    "init_model.set_weights(mean_model_weights)\n",
    "init_model.evaluate(X_test, y_test)\n",
    "print(\"Done for model selection\")\n",
    "init_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f28e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
